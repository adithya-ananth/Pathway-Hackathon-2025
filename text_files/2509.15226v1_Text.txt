HEADINGS:
# Calibration-Aware Prompt Learning for Medical Vision-Language Models
CONTENT:
Abhishek Basu 1
abhishek.basu@mbzuai.ac.ae
Fahad Shamshad 1
fahad.shamshad@mbzuai.ac.ae
Ashshak Sharifdeen 1
ashshak.sharifdeen@mbzuai.ac.ae
Karthik Nandakumar 1,2
nandakum@msu.edu
Muhammad Haris Khan 1
muhammad.haris@mbzuai.ac.ae
1 Department of Computer Vision, Mohamed bin Zayed University of Artificial Intelligence (MBZUAI), Abu Dhabi, UAE
2 Department of Computer Science and Engineering, Michigan State University (MSU), East Lansing, USAHEADINGS:
# Abstract
CONTENT:
Medical Vision-Language Models (Med-VLMs) have demonstrated remarkable performance across diverse medical imaging tasks by leveraging large-scale image-text pretraining. However, their confidence calibration is largely unexplored, and so remains a significant challenge. As such, miscalibrated predictions can lead to overconfident errors, undermining clinical trust and decision-making reliability. To address this, we introduce CalibPrompt , the first framework to calibrate Med-VLMs during prompt tuning. CalibPrompt optimizes a small set of learnable prompts with carefully designed calibration objectives under scarce labeled data regime. First, we study a regularizer that attempts to align the smoothed accuracy with the predicted model confidences. Second, we introduce an angular separation loss to maximize textual feature proximity toward improving the reliability in confidence estimates of multimodal Med-VLMs. Extensive experiments on four publicly available Med-VLMs and five diverse medical imaging datasets reveal that CalibPrompt consistently improves calibration without drastically affecting clean accuracy. Our code is available at https: //github.com/iabh1shekbasu/CalibPrompt .HEADINGS:
# Introduction
CONTENT:
Medical Vision-Language models (Med-VLMs) have emerged as powerful tools for medical image analysis, leveraging large-scale image-text pretraining to enable zero-shot classification across diverse medical imaging tasks [1, 19, 36]. These models align medical images with textual descriptions, facilitating interpretation and diagnosis without requiring task-specific fine-tuning. However, despite their strong performance in recognizing medical concepts, Med-VLMs often suffer from poor calibration, where their confidence scores fail
©2025. The copyright of this document resides with its authors.
BASU ET.AL: CALIBPROMPT
to reliably indicate actual correctness [8, 24], which is particularly concerning in medical imaging, as miscalibrated model can lead to misdiagnoses and undermine clinical trust [16].HEADINGS:
# Introduction
CONTENT:
Model calibration techniques generally fall into two categories: post-hoc calibration and training-time calibration. Post-hoc methods, such as Platt scaling [26] and temperature scaling [4], adjust confidence scores after training via a transformation function. While computationally inexpensive, they have two key limitations: (1) reliance on a small validation set, which may not reflect real-world medical distributions [22, 31], and (2) failure to improve the model's internal representations, leaving calibration issues unresolved at the decisionmaking level [25]. In contrast, training-time calibration jointly optimizes accuracy and calibration, leading to more robust and generalizable confidence estimates [14]. By integrating calibration objectives into training, it ensures well-calibrated Med-VLM predictions across medical tasks, enhancing clinical trust. However, fine-tuning large-scale Med-VLMs with calibration objectives is often impractical due to high computational cost and requirement of massive labeled medical datasets.HEADINGS:
# Introduction
CONTENT:
Meanwhile, prompt tuning has emerged as an efficient alternative to full-model finetuning for adapting Med-VLMs to downstream tasks with limited data [37]. Unlike conventional fine-tuning, which updates the entire model, prompt tuning modifies only a small set of learnable parameters, significantly reducing computational costs while preserving generalization [5, 10]. This efficiency is particularly valuable in medical imaging, where labeled data is scarce and full fine-tuning is often impractical. Despite its strong performance in datalimited settings, prompt tuning primarily optimizes classification and does not inherently improve model calibration. This raises a key question: Can the efficiency of prompt tuning, with its low data requirements and minimal parameter updates, be leveraged to enhance calibration without compromising adaptability? Addressing this is crucial to ensure Med-VLMs produce both accurate and well-calibrated predictions for reliable clinical decision-making.
In this paper, we introduce CalibPrompt , the first approach to calibrate Medical Vision-Language Models during prompt learning. Specifically, we make following technical contributions.HEADINGS:
# Introduction
CONTENT:
- We investigate a simple regularizer that aligns softened accuracy with model confidences to effectively calibrate under class ambiguities inherent in medical imaging.
- Wepropose a novel angular separation loss that promotes angular gap between textual features during prompt tuning, specifically tailored to the multimodal architecture of Med-VLMs.
- We demonstrate the effectiveness of CalibPrompt through comprehensive experiments across four publicly available Med-VLMs and five downstream datasets spanning different imaging modalities, achieving superior calibration performance while tuning only 0.1% of the model parameters.HEADINGS:
# 2 Related Work
CONTENT:
Medical Vision-Language Models (Med-VLMs) . Med-VLMs inspired by Contrastive Language-Image Pretraining (CLIP), have advanced medical imaging by aligning image-text pairs across modalities such as X-ray, histopathology, and retinal imaging. These models enable zero-shot and few-shot classification, making them particularly valuable in data-scarce medical applications [1, 19, 36]. To adapt Med-VLMs efficiently, prompt learning (PL) has
emerged as a lightweight alternative to full-model fine-tuning [5, 10, 37]. By introducing learnable prompt tokens without modifying the backbone, PL enhances task performance while maintaining computational efficiency, making it well-suited for medical imaging with limited data. In this work, we investigate whether PL can simultaneously improve calibration and task adaptation, positioning it as a parameter-efficient alternative to computationally intensive calibration methods for accurate and trustworthy predictions in medical AI.HEADINGS:
# 2 Related Work
CONTENT:
Confidence Calibration . Confidence calibration assesses how well a model's predicted confidence aligns with its actual accuracy, a critical requirement in high-stakes domains like medical imaging. Well-calibrated models yield reliable uncertainty estimates, crucial for clinical decision-making, as overconfident misclassifications can lead to severe consequences [4, 16]. Post-hoc calibration methods, such as Temperature Scaling, adjust model logits via a learned temperature parameter optimized on a held-out validation set [4]. While computationally efficient, these methods heavily depend on labeled datasets closely matching the target distribution [22, 31].To overcome such limitations, train-time calibration methods incorporate calibration objectives directly into model training, typically through auxiliary loss functions alongside primary task-specific objectives, resulting in more robust and generalizable confidence estimates [7, 14, 25]. For instance, MACSO [13] aligns predicted confidences with softened target distributions derived from the model's internal knowledge, utilizing correlation-based distance measures. Other effective train-time approaches include Margin-based Label Smoothing (MbLS) [18], which imposes inequality constraints on logit distances to prevent overly confident predictions, and Logit Normalization (LogitNorm) [33], which enforces a constant norm on logits during training to mitigate overconfidence. Moreover, Murugesan et al. [20] identified expanded logit distributions in prompt-tuned models as a significant calibration issue, introducing Zero-Shot Normalization to restore alignment with pretrained distributions. Test-time calibration methods like C-TPT [34] addresses calibration via test-time prompt tuning by optimizing text feature dispersion using prototypes. Concurrent with our research, O-TPT [29] addresses calibration in vision-language models (VLMs) through test-time prompt tuning, enforcing strict orthogonality constraints on textual features without relying on labeled data. Similarly, Wang et al. [31] proposed DAC, which adjusts softmax temperatures based on semantic distances between embeddings, primarily targeting novel-class calibration in domains with numerous classes. However, medical imaging datasets typically involve fewer classes, limiting DAC's applicability in specialized medical contexts. Their findings underscore that post-hoc calibration methods alone cannot fully recover the pretrained calibration behaviour of VLMs. In contrast, we introduce a novel prompt-based calibration framework that operates effectively in few-shot scenarios by jointly optimizing regularization objectives in both probability and feature spaces. Unlike O-TPT, our method encourages (rather than strictly enforces) angular separation, providing greater flexibility to capture nuanced class relationships within specialized medical imaging domains.HEADINGS:
# 3 Method
CONTENT:
Our goal is to calibrate Med-VLMs in data-limited settings to ensure that their confidence scores accurately reflect prediction correctness. In medical imaging, miscalibrated models can lead to overconfident errors with serious clinical implications. To this end, we introduce a new approach CalibPrompt under prompt-learning setup which is built on two novel regularizers. The first regularizer matches the softened accuracy with predicted confidences,
while the second is an angular separation loss that explicitly maximizes the proximity between textual features. Below, we first describe zero-shot inference with Med-VLMs, then explain the prompt learning basics, and finally introduce our calibration-aware prompt tuning approach CalibPrompt .HEADINGS:
# 3.1 Preliminaries:
CONTENT:
Zero-Shot Inference for Med-VLMs: Med-VLMs learn a joint representation of images and text through contrastive pretraining, enabling zero-shot classification. These models consist of an image encoder E img : I → R d and a text encoder E txt : T → R d , where I and T denote the image and text spaces, respectively. Given an input image I ∈I ⊆ R H × W × C , the image encoder extracts a d -dimensional feature vector v = E img ( I ) . Similarly, the text encoder maps a textual prompt t ( y ) ∈ T associated with class label y ∈ Y into a text feature vector u = E txt ( t ( y )) . During zero-shot inference, class labels { y 1 , . . . , yK } are converted into text prompts using a predefined template, such as t ( yi ) = ' A H&E image of [CLASS yi ] ′′ , and are processed by the text encoder to obtain { u 1 , . . . , u K } , where u i = E txt ( t ( yi )) . For a test image I t , the similarity between the image and text features is computed as si = sim ( v t , u i ) , where v t = E img ( I t ) . The final classification probabilities are obtained using a softmax function as P ( yi | I t ) = exp ( τ s i ) ∑ K j = 1 exp ( τ s j ) , where τ is the softmax temperature parameter.HEADINGS:
# 3.1 Preliminaries:
CONTENT:
The predicted label is then given by ˆ yt = argmax y ∈ Y P ( y | I t ) . The corresponding confidence is given by ˆ pt = max y ∈ Y P ( y | I t ) . While this zero-shot framework enables flexible classification, Med-VLMs often produce overconfident predictions as shown in Table 1. A naive solution is to fine-tune Med-VLMs with explicit calibration objectives (i.e. train-time auxiliary losses); however, this is computationally expensive and requires extensive labeled data. Thus, an efficient alternative is needed to enhance calibration without full model retraining.HEADINGS:
# 3.1 Preliminaries:
CONTENT:
Prompt Learning: Prompt learning (PL) has emerged as an efficient alternative, enabling adaptation to new tasks without modifying the model backbone. Instead of updating the entire network, PL optimizes a small set of learnable prompt tokens, making it particularly useful for data-scarce medical applications. When a text prompt t ( yi ) ∈ T is passed to the text encoder, it is tokenized into a sequence of word embeddings. Typically, a class-specific text prompt is represented as [ w ] 1 [ w ] 2 · · · [ CLASS yi ] , where each [ ∗ ] denotes a word embedding. In PL, all fixed embeddings (except for the class token) are replaced with M learnable embeddings, transforming the prompt into p i 1 p i 2 · · · p iM [ CLASS yi ] , where each prompt embedding p has the same dimensionality as [ w ] . Let P = p im , where i ∈ [ 1 , K ] and m ∈ [ 1 , M ] , represent the set of all learnable prompts. The output text feature vector, incorporating these learned prompts, is denoted as u i ( P ) , and the modified zero-shot classifier is f P .HEADINGS:
# 3.1 Preliminaries:
CONTENT:
Limitation and Motivation: While prompt learning effectively adapts Med-VLMs to downstream tasks with limited data, our empirical analysis reveals a critical limitation for MedVLMs: it increases calibration error despite improving classification accuracy . We observe that prompt-tuned models consistently exhibit high Expected Calibration Error [21], indicating a mismatch between confidence scores and actual correctness. To understand this behavior, we analyze the geometric properties of learned textual prompts and find that prompt tuning significantly increases intra-class cosine similarity, causing class representations to become highly aligned (see Fig. 1 left). While this enhances classification separability, it also amplifies confidence scores, leading to overconfident predictions and calibration error, with approximately 22% of misclassifications occurring at high confidence levels of 0.9-1.0 (see Fig.1 middle). Further, our results reveal a strong correlation between high cosine sim-HEADINGS:
# 3.1 Preliminaries:
CONTENT:
Figure 1: Analysis of Prompt Learning Effects on Model Calibration . Left : Cross entropy (CE) shows higher text feature similarity between classes than CE MDCA. Middle : Cross entropy histogram demonstrating overconfident misclassifications with higher confidence levels. Right : Greater feature similarity (CE, CE MDCA) directly correlates with increased calibration error compared to regularized approaches (LS, FL MDCA).
ilarity and increased miscalibration (see Fig. 1 right), underscoring the need for explicit text feature space regularization.HEADINGS:
# 3.2 CalibPrompt : Calibration-Aware Prompt Learning
CONTENT:
We introduce CalibPrompt as shown in Fig. 2, a new approach to improve confidence calibration in zero-shot classifiers based on Med-VLMs. Motivated by our observations, CalibPrompt incorporates learnable prompts into the text encoder and optimizes them with our proposed calibration-aware auxiliary losses to enforce appropriate confidence calibration while keeping the model backbone frozen. Specifically, given a zero-shot classifier f based on a pre-trained Med-VLM ( E image , E text ) and a few labeled samples { ( I n , yn ) } N n = 1 from a downstream dataset D , where I n ∈ I and yn ∈ Y , CalibPrompt optimizes the learnable prompts P to jointly minimize classification loss and calibration error:
<!-- formula-not-decoded -->HEADINGS:
# 3.2 CalibPrompt : Calibration-Aware Prompt Learning
CONTENT:
where L CE is the cross-entropy loss, L calib is our overall calibration objective, and λ balances accuracy and calibration objectives. The prompts are updated via backpropagation while keeping the Med-VLM frozen, preserving its pre-trained knowledge while optimizing for calibrated predictions. To address miscalibration, we introduce two complementary objectives: conforming softened accuracy with predicted confidences ( L SMAC) and the Angular Separation Loss ( L AS), forming our calibration objective L calib = α L SMAC + β L AS.HEADINGS:
# 3.2 CalibPrompt : Calibration-Aware Prompt Learning
CONTENT:
Smoothed Accuracy and Confidence Matching ( SMAC ): Medical image classification often involves inherent class ambiguities, where diagnostic categories exhibit overlapping visual features. Traditional hard-label-based calibration methods [14] enforce overly rigid decision boundaries, leading to miscalibrated overconfidence. To address this, we propose aligning predicted confidences with smoothed empirical class frequencies in a class-wise manner, termed SMAC . This provides a nuanced training signal that captures inherent ambiguities in medical imaging. Let p n = f P ( I n ) denote the predicted probability distribution for image I n across K classes, and let yn ∈{ 1 , 2 , ..., K } be the ground truth label. The SMAC loss
Figure 2: Overview of CalibPrompt . Learnable prompts are optimized using classification and calibration lossesSMAC and AS -while keeping the image and text encoders frozen. The SMAC loss aligns confidence with smoothed accuracy, while AS improves feature separation in the text embedding space.
is formulated as:
<!-- formula-not-decoded -->HEADINGS:
# 3.2 CalibPrompt : Calibration-Aware Prompt Learning
CONTENT:
where fc = 1 N ∑ N n = 1 I [ yn = c ] , and α ∈ [ 0 , 1 ) controls smoothing intensity. Using smoothed frequencies for confidence estimation, SMAC allows relaxed matching between predicted and empirical class distributions, thus reducing the likelihood of overconfident predictions in ambiguous scenarios.HEADINGS:
# 3.2 CalibPrompt : Calibration-Aware Prompt Learning
CONTENT:
Angular Separation ( AS ) Loss : Building on our observation that prompt tuning increases text embedding similarity (see Fig. 1), we address a key challenge in medical image classification where high inter-class feature similarity leads to overconfident predictions and degraded calibration. We propose an Angular Separation Loss for the textual embeddings, which discourages excessive similarity between class embeddings by minimizing their average pairwise cosine similarity. This ensures well-separated textual feature representations, improving confidence calibration while preserving classification accuracy. Mathematically, let Z ∈ R K × D be the text feature matrix, where each row z i represents the feature embedding of class i in a D-dimensional space. We compute the cosine similarity matrix between all pairs of feature vectors as S = ZZ T where Sij measures the cosine similarity between class embeddings z i and z j . To focus only on inter-class relationships, we mask the diagonal elements (self-similarities) as S off-diag = S -diag ( S ) . The Angular Separation Loss is then defined as the mean similarity across all class pairs:
̸HEADINGS:
# 3.2 CalibPrompt : Calibration-Aware Prompt Learning
CONTENT:
<!-- formula-not-decoded -->HEADINGS:
# 3.2 CalibPrompt : Calibration-Aware Prompt Learning
CONTENT:
Table 1: Zero-shot accuracy (%), confidence (%), and ECE (%) of Med-VLMs on X-ray and histopathology datasets. Final column shows over/underconfidence. Prompts used are shown above rows.

Hard Prompt: A chest X-ray image of [class] patient, Model = Hard Prompt: A chest X-ray image of [class] patient. Hard Prompt: A chest X-ray image of [class] patient, Accuracy (%) ↑ = Hard Prompt: A chest X-ray image of [class] patient. Hard Prompt: A chest X-ray image of [class] patient, Confidence (%) = Hard Prompt: A chest X-ray image of [class] patient. Hard Prompt: A chest X-ray image of [class] patient, ECE (%) ↓ = Hard Prompt: A chest X-ray image of [class] patient. Hard Prompt: A chest X-ray image of [class] patient, Calibration = Hard Prompt: A chest X-ray image of [class] patient. BioMedCLIP MedCLIP, Model = 84.37. BioMedCLIP MedCLIP, Accuracy (%) ↑ = 95.0. BioMedCLIP MedCLIP, Confidence (%) = 10.70 28.67. BioMedCLIP MedCLIP, ECE (%) ↓ = Overconfident Underconfident. BioMedCLIP MedCLIP, Calibration = COVID. , Model = 78.77 49.71. , Accuracy (%) ↑ = 50.0 79.0. , Confidence (%) = . , ECE (%) ↓ = . , Calibration = . BioMedCLIP MedCLIP, Model = 47.60. BioMedCLIP MedCLIP, Accuracy (%) ↑ = 34.0. BioMedCLIP MedCLIP, Confidence (%) = 29.49 14.05. BioMedCLIP MedCLIP, ECE (%) ↓ = Overconfident Underconfident. BioMedCLIP MedCLIP, Calibration = RSNA18. Hard Prompt: An H&E image of [class], Model = Hard Prompt: An H&E image of [class]. Hard Prompt: An H&E image of [class], Accuracy (%) ↑ = Hard Prompt: An H&E image of [class]. Hard Prompt: An H&E image of [class], Confidence (%) = Hard Prompt: An H&E image of [class]. Hard Prompt: An H&E image of [class], ECE (%) ↓ = Hard Prompt: An H&E image of [class]. Hard Prompt: An H&E image of [class], Calibration = Hard Prompt: An H&E image of [class]. PLIP, Model = 57.80. PLIP, Accuracy (%) ↑ = 74.0. PLIP, Confidence (%) = 16.32. PLIP, ECE (%) ↓ = Overconfident. PLIP, Calibration = . QuiltNet, Model = 60.39. QuiltNet, Accuracy (%) ↑ = 58.0. QuiltNet, Confidence (%) = 4.20. QuiltNet, ECE (%) ↓ = Underconfident. QuiltNet, Calibration = Kather. Prompt: An H&E image patch of [class] skin tissue, Model = Prompt: An H&E image patch of [class] skin tissue. Prompt: An H&E image patch of [class] skin tissue, Accuracy (%) ↑ = Prompt: An H&E image patch of [class] skin tissue. Prompt: An H&E image patch of [class] skin tissue, Confidence (%) = Prompt: An H&E image patch of [class] skin tissue. Prompt: An H&E image patch of [class] skin tissue, ECE (%) ↓ = Prompt: An H&E image patch of [class] skin tissue. Prompt: An H&E image patch of [class] skin tissue, Calibration = Prompt: An H&E image patch of [class] skin tissue. PLIP, Model = 56.42. PLIP, Accuracy (%) ↑ = 76.0. PLIP, Confidence (%) = 19.33. PLIP, ECE (%) ↓ = Overconfident. PLIP, Calibration = PanNuke. QuiltNet, Model = 55.59. QuiltNet, Accuracy (%) ↑ = 79.0. QuiltNet, Confidence (%) = 23.89. QuiltNet, ECE (%) ↓ = Overconfident. QuiltNet, Calibration = PanNuke. Hard Prompt: An H&E image patch of [class] tissue, Model = Hard Prompt: An H&E image patch of [class] tissue. Hard Prompt: An H&E image patch of [class] tissue, Accuracy (%) ↑ = Hard Prompt: An H&E image patch of [class] tissue. Hard Prompt: An H&E image patch of [class] tissue, Confidence (%) = Hard Prompt: An H&E image patch of [class] tissue. Hard Prompt: An H&E image patch of [class] tissue, ECE (%) ↓ = Hard Prompt: An H&E image patch of [class] tissue. Hard Prompt: An H&E image patch of [class] tissue, Calibration = Hard Prompt: An H&E image patch of [class] tissue. PLIP QuiltNet, Model = 80.53 53.39. PLIP QuiltNet, Accuracy (%) ↑ = 74.0 73.0. PLIP QuiltNet, Confidence (%) = 6.14 19.90. PLIP QuiltNet, ECE (%) ↓ = Underconfident Overconfident. PLIP QuiltNet, Calibration = DigestPathHEADINGS:
# 3.2 CalibPrompt : Calibration-Aware Prompt Learning
CONTENT:
By minimizing this loss, class embeddings become more distinct, reducing confidence overestimation and improving calibration. While SMAC loss refines probability-space confidence calibration, L AS explicitly regularizes the text embedding space, ensuring both feature separability and confidence reliability.HEADINGS:
# 4 Experiments
CONTENT:
Datasets, baselines and implementation details: We hypothesize that the full fine-tuning approach can lead to overfitting in large networks when training data is limited, resulting in suboptimal feature representations. We evaluate our method with two regularizers on four Med-VLMs: PLIP [9], QuiltNet [11], MedCLIP [32], and BioMedCLIP [35], using five downstream datasets: COVIDX [28], RSNA18 [30], KatherColon [12], PanNuke [3], and DigestPath [2]. All experiments are conducted on an NVIDIA RTX A6000 GPU with 48GB memory. Our baselines include cross-entropy (CE), focal loss (FL) (given in supplementary), and label smoothing (LS), along with their combinations with established calibration regularization techniques such as DCA [17], MMCE [15] , MDCA [6], ZS-Norm[20], Penalty[20], MbLS [18], and LogitNorm [33]. We evaluated these against our proposed methods SMAC and SMAC with AS across both full model fine-tuning and prompt learning approaches. We used an 8-shot setting (8 images per class) for both variations, we used a learning rate of 2 × 10 -7 for full model fine-tuning, while for prompt learning we used a learning rate of 0.002. Detailed hyperparameters are discussed in the supplementary.HEADINGS:
# 4 Experiments
CONTENT:
Performance of the Med-VLMs is measured using accuracy (ACC). Similarly, for calibration, Expectation Calibration Error [21] (ECE), Adaptive Calibration Error [23] (ACE), Maximum Calibration Error [21] (MCE), and Expectation Calibration Error - Kernel Density Estimates[27] ECE KDE .HEADINGS:
# 4 Experiments
CONTENT:
Table 2: Comparison of proposed regularizers ( SMAC , SMAC+AS ) with baseline methods using Cross Entropy (CE), and Label Smoothing (LS). Accuracy (ACC, %) and Expected Calibration Error (ECE, %) are shown for PLIP and QuiltNet on histopathology datasets (Kather, PanNuke, DigestPath). Subscripts FT and PL denote Few-shot Fine-Tuning and Prompt Learning. Best results are in bold , second-best underlined.

Cross Entropy-based Losses, PLIP.Kather ACC ↑ ECE ↓ = Cross Entropy-based Losses. Cross Entropy-based Losses, PLIP.Kather ACC ↑ ECE ↓ = Cross Entropy-based Losses. Cross Entropy-based Losses, PLIP.PanNuke ACC ↑ ECE ↓ = Cross Entropy-based Losses. Cross Entropy-based Losses, PLIP.PanNuke ACC ↑ ECE ↓ = Cross Entropy-based Losses. Cross Entropy-based Losses, PLIP.DigestPath ACC ↑ ECE ↓ = Cross Entropy-based Losses. Cross Entropy-based Losses, PLIP.DigestPath ACC ↑ ECE ↓ = Cross Entropy-based Losses. Cross Entropy-based Losses, QuiltNet.Kather Acc ↑ ECE ↓ = Cross Entropy-based Losses. Cross Entropy-based Losses, QuiltNet.Kather Acc ↑ ECE ↓ = Cross Entropy-based Losses. Cross Entropy-based Losses, QuiltNet.PanNuke ACC ↑ ECE ↓ = Cross Entropy-based Losses. Cross Entropy-based Losses, QuiltNet.PanNuke ACC ↑ ECE ↓ = Cross Entropy-based Losses. Cross Entropy-based Losses, QuiltNet.DigestPath ACC ↑ ECE ↓ = Cross Entropy-based Losses. Cross Entropy-based Losses, QuiltNet.DigestPath ACC ↑ ECE ↓ = Cross Entropy-based Losses. Cross Entropy-based Losses, Average.All ACC ↑ ECE ↓ = Cross Entropy-based Losses. Cross Entropy-based Losses, Average.All ACC ↑ ECE ↓ = Cross Entropy-based Losses. Cross Entropy Loss FT, PLIP.Kather ACC ↑ ECE ↓ = 81.17. Cross Entropy Loss FT, PLIP.Kather ACC ↑ ECE ↓ = 8.46. Cross Entropy Loss FT, PLIP.PanNuke ACC ↑ ECE ↓ = 60.08. Cross Entropy Loss FT, PLIP.PanNuke ACC ↑ ECE ↓ = 15.79. Cross Entropy Loss FT, PLIP.DigestPath ACC ↑ ECE ↓ = 82.68. Cross Entropy Loss FT, PLIP.DigestPath ACC ↑ ECE ↓ = 5.50. Cross Entropy Loss FT, QuiltNet.Kather Acc ↑ ECE ↓ = 79.19. Cross Entropy Loss FT, QuiltNet.Kather Acc ↑ ECE ↓ = 17.20. Cross Entropy Loss FT, QuiltNet.PanNuke ACC ↑ ECE ↓ = 72.80. Cross Entropy Loss FT, QuiltNet.PanNuke ACC ↑ ECE ↓ = 7.67. Cross Entropy Loss FT, QuiltNet.DigestPath ACC ↑ ECE ↓ = 75.93. Cross Entropy Loss FT, QuiltNet.DigestPath ACC ↑ ECE ↓ = 4.62. Cross Entropy Loss FT, Average.All ACC ↑ ECE ↓ = 75.31. Cross Entropy Loss FT, Average.All ACC ↑ ECE ↓ = 9.87. Cross Entropy Loss PL, PLIP.Kather ACC ↑ ECE ↓ = 83.91. Cross Entropy Loss PL, PLIP.Kather ACC ↑ ECE ↓ = 5.92. Cross Entropy Loss PL, PLIP.PanNuke ACC ↑ ECE ↓ = 66.70. Cross Entropy Loss PL, PLIP.PanNuke ACC ↑ ECE ↓ = 17.82. Cross Entropy Loss PL, PLIP.DigestPath ACC ↑ ECE ↓ = 82.87. Cross Entropy Loss PL, PLIP.DigestPath ACC ↑ ECE ↓ = 9.50. Cross Entropy Loss PL, QuiltNet.Kather Acc ↑ ECE ↓ = 87.97. Cross Entropy Loss PL, QuiltNet.Kather Acc ↑ ECE ↓ = 2.49. Cross Entropy Loss PL, QuiltNet.PanNuke ACC ↑ ECE ↓ = 69.82. Cross Entropy Loss PL, QuiltNet.PanNuke ACC ↑ ECE ↓ = 19.70. Cross Entropy Loss PL, QuiltNet.DigestPath ACC ↑ ECE ↓ = 81.59. Cross Entropy Loss PL, QuiltNet.DigestPath ACC ↑ ECE ↓ = 11.27. Cross Entropy Loss PL, Average.All ACC ↑ ECE ↓ = 78.81. Cross Entropy Loss PL, Average.All ACC ↑ ECE ↓ = 11.12. CE + DCA FT, PLIP.Kather ACC ↑ ECE ↓ = 82.63. CE + DCA FT, PLIP.Kather ACC ↑ ECE ↓ = 8.20. CE + DCA FT, PLIP.PanNuke ACC ↑ ECE ↓ = 59.37. CE + DCA FT, PLIP.PanNuke ACC ↑ ECE ↓ = 17.57. CE + DCA FT, PLIP.DigestPath ACC ↑ ECE ↓ = 83.76. CE + DCA FT, PLIP.DigestPath ACC ↑ ECE ↓ = 5.53. CE + DCA FT, QuiltNet.Kather Acc ↑ ECE ↓ = 80.11. CE + DCA FT, QuiltNet.Kather Acc ↑ ECE ↓ = 17.90. CE + DCA FT, QuiltNet.PanNuke ACC ↑ ECE ↓ = 51.19. CE + DCA FT, QuiltNet.PanNuke ACC ↑ ECE ↓ = 13.07. CE + DCA FT, QuiltNet.DigestPath ACC ↑ ECE ↓ = 31.49. CE + DCA FT, QuiltNet.DigestPath ACC ↑ ECE ↓ = 33.14. CE + DCA FT, Average.All ACC ↑ ECE ↓ = 64.76. CE + DCA FT, Average.All ACC ↑ ECE ↓ = 15.90. CE + DCA PL, PLIP.Kather ACC ↑ ECE ↓ = 85.74. CE + DCA PL, PLIP.Kather ACC ↑ ECE ↓ = 3.60. CE + DCA PL, PLIP.PanNuke ACC ↑ ECE ↓ = 67.94. CE + DCA PL, PLIP.PanNuke ACC ↑ ECE ↓ = 11.48. CE + DCA PL, PLIP.DigestPath ACC ↑ ECE ↓ = 74.71. CE + DCA PL, PLIP.DigestPath ACC ↑ ECE ↓ = 13.91. CE + DCA PL, QuiltNet.Kather Acc ↑ ECE ↓ = 88.16. CE + DCA PL, QuiltNet.Kather Acc ↑ ECE ↓ = 1.50. CE + DCA PL, QuiltNet.PanNuke ACC ↑ ECE ↓ = 74.47. CE + DCA PL, QuiltNet.PanNuke ACC ↑ ECE ↓ = 4.29. CE + DCA PL, QuiltNet.DigestPath ACC ↑ ECE ↓ = 84.24. CE + DCA PL, QuiltNet.DigestPath ACC ↑ ECE ↓ = 1.71. CE + DCA PL, Average.All ACC ↑ ECE ↓ = 79.21. CE + DCA PL, Average.All ACC ↑ ECE ↓ = 6.08. CE+MMCE FT, PLIP.Kather ACC ↑ ECE ↓ = 81.16. CE+MMCE FT, PLIP.Kather ACC ↑ ECE ↓ = 8.46. CE+MMCE FT, PLIP.PanNuke ACC ↑ ECE ↓ = 60.15. CE+MMCE FT, PLIP.PanNuke ACC ↑ ECE ↓ = 15.68. CE+MMCE FT, PLIP.DigestPath ACC ↑ ECE ↓ = 82.86. CE+MMCE FT, PLIP.DigestPath ACC ↑ ECE ↓ = 5.52. CE+MMCE FT, QuiltNet.Kather Acc ↑ ECE ↓ = 79.07. CE+MMCE FT, QuiltNet.Kather Acc ↑ ECE ↓ = 17.06. CE+MMCE FT, QuiltNet.PanNuke ACC ↑ ECE ↓ = 73.05. CE+MMCE FT, QuiltNet.PanNuke ACC ↑ ECE ↓ = 7.26. CE+MMCE FT, QuiltNet.DigestPath ACC ↑ ECE ↓ = 80.50. CE+MMCE FT, QuiltNet.DigestPath ACC ↑ ECE ↓ = 7.60. CE+MMCE FT, Average.All ACC ↑ ECE ↓ = 76.13. CE+MMCE FT, Average.All ACC ↑ ECE ↓ = 10.26. CE+MMCE PL, PLIP.Kather ACC ↑ ECE ↓ = 83.52. CE+MMCE PL, PLIP.Kather ACC ↑ ECE ↓ = 3.07. CE+MMCE PL, PLIP.PanNuke ACC ↑ ECE ↓ = 67.05. CE+MMCE PL, PLIP.PanNuke ACC ↑ ECE ↓ = 12.16. CE+MMCE PL, PLIP.DigestPath ACC ↑ ECE ↓ = 78.81. CE+MMCE PL, PLIP.DigestPath ACC ↑ ECE ↓ = 9.68. CE+MMCE PL, QuiltNet.Kather Acc ↑ ECE ↓ = 90.97. CE+MMCE PL, QuiltNet.Kather Acc ↑ ECE ↓ = 2.11. CE+MMCE PL, QuiltNet.PanNuke ACC ↑ ECE ↓ = 68.08. CE+MMCE PL, QuiltNet.PanNuke ACC ↑ ECE ↓ = 17.95. CE+MMCE PL, QuiltNet.DigestPath ACC ↑ ECE ↓ = 85.13. CE+MMCE PL, QuiltNet.DigestPath ACC ↑ ECE ↓ = 4.49. CE+MMCE PL, Average.All ACC ↑ ECE ↓ = 78.93. CE+MMCE PL, Average.All ACC ↑ ECE ↓ = 8.24. CE + MDCA FT, PLIP.Kather ACC ↑ ECE ↓ = 81.14. CE + MDCA FT, PLIP.Kather ACC ↑ ECE ↓ = 8.41. CE + MDCA FT, PLIP.PanNuke ACC ↑ ECE ↓ = 59.90. CE + MDCA FT, PLIP.PanNuke ACC ↑ ECE ↓ = 16.08. CE + MDCA FT, PLIP.DigestPath ACC ↑ ECE ↓ = 82.54. CE + MDCA FT, PLIP.DigestPath ACC ↑ ECE ↓ = 5.48. CE + MDCA FT, QuiltNet.Kather Acc ↑ ECE ↓ = 79.14. CE + MDCA FT, QuiltNet.Kather Acc ↑ ECE ↓ = 17.17. CE + MDCA FT, QuiltNet.PanNuke ACC ↑ ECE ↓ = 72.80. CE + MDCA FT, QuiltNet.PanNuke ACC ↑ ECE ↓ = 7.67. CE + MDCA FT, QuiltNet.DigestPath ACC ↑ ECE ↓ = 31.07. CE + MDCA FT, QuiltNet.DigestPath ACC ↑ ECE ↓ = 32.53. CE + MDCA FT, Average.All ACC ↑ ECE ↓ = 67.77. CE + MDCA FT, Average.All ACC ↑ ECE ↓ = 14.56. CE + MDCA PL, PLIP.Kather ACC ↑ ECE ↓ = 83.91. CE + MDCA PL, PLIP.Kather ACC ↑ ECE ↓ = 5.79. CE + MDCA PL, PLIP.PanNuke ACC ↑ ECE ↓ = 70.67. CE + MDCA PL, PLIP.PanNuke ACC ↑ ECE ↓ = 9.69. CE + MDCA PL, PLIP.DigestPath ACC ↑ ECE ↓ = 88.92. CE + MDCA PL, PLIP.DigestPath ACC ↑ ECE ↓ = 4.08. CE + MDCA PL, QuiltNet.Kather Acc ↑ ECE ↓ = 89.99. CE + MDCA PL, QuiltNet.Kather Acc ↑ ECE ↓ = 1.61. CE + MDCA PL, QuiltNet.PanNuke ACC ↑ ECE ↓ = 69.29. CE + MDCA PL, QuiltNet.PanNuke ACC ↑ ECE ↓ = 13.46. CE + MDCA PL, QuiltNet.DigestPath ACC ↑ ECE ↓ = 84.89. CE + MDCA PL, QuiltNet.DigestPath ACC ↑ ECE ↓ = 6.55. CE + MDCA PL, Average.All ACC ↑ ECE ↓ = 81.28. CE + MDCA PL, Average.All ACC ↑ ECE ↓ = 6.86. MbLS PL, PLIP.Kather ACC ↑ ECE ↓ = 84.39. MbLS PL, PLIP.Kather ACC ↑ ECE ↓ = 3.57. MbLS PL, PLIP.PanNuke ACC ↑ ECE ↓ = 66.70. MbLS PL, PLIP.PanNuke ACC ↑ ECE ↓ = 17.82. MbLS PL, PLIP.DigestPath ACC ↑ ECE ↓ = 82.76. MbLS PL, PLIP.DigestPath ACC ↑ ECE ↓ = 9.53. MbLS PL, QuiltNet.Kather Acc ↑ ECE ↓ = 84.76. MbLS PL, QuiltNet.Kather Acc ↑ ECE ↓ = 3.48. MbLS PL, QuiltNet.PanNuke ACC ↑ ECE ↓ = 65.54. MbLS PL, QuiltNet.PanNuke ACC ↑ ECE ↓ = 23.05. MbLS PL, QuiltNet.DigestPath ACC ↑ ECE ↓ = 82.58. MbLS PL, QuiltNet.DigestPath ACC ↑ ECE ↓ = 10.90. MbLS PL, Average.All ACC ↑ ECE ↓ = 77.79. MbLS PL, Average.All ACC ↑ ECE ↓ = 11.39. LogitNorm PL, PLIP.Kather ACC ↑ ECE ↓ = 86.52. LogitNorm PL, PLIP.Kather ACC ↑ ECE ↓ = 5.12. LogitNorm PL, PLIP.PanNuke ACC ↑ ECE ↓ = 57.10. LogitNorm PL, PLIP.PanNuke ACC ↑ ECE ↓ = 31.72. LogitNorm PL, PLIP.DigestPath ACC ↑ ECE ↓ = 84.80. LogitNorm PL, PLIP.DigestPath ACC ↑ ECE ↓ = 9.04. LogitNorm PL, QuiltNet.Kather Acc ↑ ECE ↓ = 88.22. LogitNorm PL, QuiltNet.Kather Acc ↑ ECE ↓ = 3.42. LogitNorm PL, QuiltNet.PanNuke ACC ↑ ECE ↓ = 72.91. LogitNorm PL, QuiltNet.PanNuke ACC ↑ ECE ↓ = 13.88. LogitNorm PL, QuiltNet.DigestPath ACC ↑ ECE ↓ = 87.17. LogitNorm PL, QuiltNet.DigestPath ACC ↑ ECE ↓ = 5.26. LogitNorm PL, Average.All ACC ↑ ECE ↓ = 79.45. LogitNorm PL, Average.All ACC ↑ ECE ↓ = 11.41. ZS-Norm PL, PLIP.Kather ACC ↑ ECE ↓ = 85.63. ZS-Norm PL, PLIP.Kather ACC ↑ ECE ↓ = 3.07. ZS-Norm PL, PLIP.PanNuke ACC ↑ ECE ↓ = 71.52. ZS-Norm PL, PLIP.PanNuke ACC ↑ ECE ↓ = 17.22. ZS-Norm PL, PLIP.DigestPath ACC ↑ ECE ↓ = 82.84. ZS-Norm PL, PLIP.DigestPath ACC ↑ ECE ↓ = 7.31. ZS-Norm PL, QuiltNet.Kather Acc ↑ ECE ↓ = 91.91. ZS-Norm PL, QuiltNet.Kather Acc ↑ ECE ↓ = 0.87. ZS-Norm PL, QuiltNet.PanNuke ACC ↑ ECE ↓ = 69.55. ZS-Norm PL, QuiltNet.PanNuke ACC ↑ ECE ↓ = 19.18. ZS-Norm PL, QuiltNet.DigestPath ACC ↑ ECE ↓ = 84.78. ZS-Norm PL, QuiltNet.DigestPath ACC ↑ ECE ↓ = 6.37. ZS-Norm PL, Average.All ACC ↑ ECE ↓ = 81.04. ZS-Norm PL, Average.All ACC ↑ ECE ↓ = 9.00. Penalty PL, PLIP.Kather ACC ↑ ECE ↓ = 86.48. Penalty PL, PLIP.Kather ACC ↑ ECE ↓ = 3.90. Penalty PL, PLIP.PanNuke ACC ↑ ECE ↓ = 70.49. Penalty PL, PLIP.PanNuke ACC ↑ ECE ↓ = 3.11. Penalty PL, PLIP.DigestPath ACC ↑ ECE ↓ = 69.61. Penalty PL, PLIP.DigestPath ACC ↑ ECE ↓ = 2.40. Penalty PL, QuiltNet.Kather Acc ↑ ECE ↓ = 89.29. Penalty PL, QuiltNet.Kather Acc ↑ ECE ↓ = 12.28. Penalty PL, QuiltNet.PanNuke ACC ↑ ECE ↓ = 59.88. Penalty PL, QuiltNet.PanNuke ACC ↑ ECE ↓ = 3.41. Penalty PL, QuiltNet.DigestPath ACC ↑ ECE ↓ = 79.23. Penalty PL, QuiltNet.DigestPath ACC ↑ ECE ↓ = 17.53. Penalty PL, Average.All ACC ↑ ECE ↓ = 75.83. Penalty PL, Average.All ACC ↑ ECE ↓ = 7.11. CE + SMAC FT, PLIP.Kather ACC ↑ ECE ↓ = 81.39. CE + SMAC FT, PLIP.Kather ACC ↑ ECE ↓ = 8.52. CE + SMAC FT, PLIP.PanNuke ACC ↑ ECE ↓ = 64.37. CE + SMAC FT, PLIP.PanNuke ACC ↑ ECE ↓ = 9.20. CE + SMAC FT, PLIP.DigestPath ACC ↑ ECE ↓ = 82.94. CE + SMAC FT, PLIP.DigestPath ACC ↑ ECE ↓ = 5.75. CE + SMAC FT, QuiltNet.Kather Acc ↑ ECE ↓ = 79.75. CE + SMAC FT, QuiltNet.Kather Acc ↑ ECE ↓ = 17.85. CE + SMAC FT, QuiltNet.PanNuke ACC ↑ ECE ↓ = 72.78. CE + SMAC FT, QuiltNet.PanNuke ACC ↑ ECE ↓ = 5.87. CE + SMAC FT, QuiltNet.DigestPath ACC ↑ ECE ↓ = 80.96. CE + SMAC FT, QuiltNet.DigestPath ACC ↑ ECE ↓ = 8.64. CE + SMAC FT, Average.All ACC ↑ ECE ↓ = 77.03. CE + SMAC FT, Average.All ACC ↑ ECE ↓ = 9.31. CE + SMAC PL, PLIP.Kather ACC ↑ ECE ↓ = 84.11. CE + SMAC PL, PLIP.Kather ACC ↑ ECE ↓ = 5.42. CE + SMAC PL, PLIP.PanNuke ACC ↑ ECE ↓ = 70.67. CE + SMAC PL, PLIP.PanNuke ACC ↑ ECE ↓ = 9.69. CE + SMAC PL, PLIP.DigestPath ACC ↑ ECE ↓ = 88.92. CE + SMAC PL, PLIP.DigestPath ACC ↑ ECE ↓ = 4.08. CE + SMAC PL, QuiltNet.Kather Acc ↑ ECE ↓ = 89.99. CE + SMAC PL, QuiltNet.Kather Acc ↑ ECE ↓ = 1.57. CE + SMAC PL, QuiltNet.PanNuke ACC ↑ ECE ↓ = 69.29. CE + SMAC PL, QuiltNet.PanNuke ACC ↑ ECE ↓ = 13.46. CE + SMAC PL, QuiltNet.DigestPath ACC ↑ ECE ↓ = 84.89. CE + SMAC PL, QuiltNet.DigestPath ACC ↑ ECE ↓ = 6.55. CE + SMAC PL, Average.All ACC ↑ ECE ↓ = 81.31. CE + SMAC PL, Average.All ACC ↑ ECE ↓ = 6.80. CE + ( SMAC + AS ) FT, PLIP.Kather ACC ↑ ECE ↓ = 81.45. CE + ( SMAC + AS ) FT, PLIP.Kather ACC ↑ ECE ↓ = 8.55. CE + ( SMAC + AS ) FT, PLIP.PanNuke ACC ↑ ECE ↓ = 64.16. CE + ( SMAC + AS ) FT, PLIP.PanNuke ACC ↑ ECE ↓ = 9.56. CE + ( SMAC + AS ) FT, PLIP.DigestPath ACC ↑ ECE ↓ = 83.04. CE + ( SMAC + AS ) FT, PLIP.DigestPath ACC ↑ ECE ↓ = 5.78. CE + ( SMAC + AS ) FT, QuiltNet.Kather Acc ↑ ECE ↓ = 78.38. CE + ( SMAC + AS ) FT, QuiltNet.Kather Acc ↑ ECE ↓ = 13.13. CE + ( SMAC + AS ) FT, QuiltNet.PanNuke ACC ↑ ECE ↓ = 72.91. CE + ( SMAC + AS ) FT, QuiltNet.PanNuke ACC ↑ ECE ↓ = 6.02. CE + ( SMAC + AS ) FT, QuiltNet.DigestPath ACC ↑ ECE ↓ = 81.26. CE + ( SMAC + AS ) FT, QuiltNet.DigestPath ACC ↑ ECE ↓ = 8.81. CE + ( SMAC + AS ) FT, Average.All ACC ↑ ECE ↓ = 76.87. CE + ( SMAC + AS ) FT, Average.All ACC ↑ ECE ↓ = 8.64. CE + ( SMAC + AS ) PL, PLIP.Kather ACC ↑ ECE ↓ = 84.65. CE + ( SMAC + AS ) PL, PLIP.Kather ACC ↑ ECE ↓ = 5.05. CE + ( SMAC + AS ) PL, PLIP.PanNuke ACC ↑ ECE ↓ = 65.81. CE + ( SMAC + AS ) PL, PLIP.PanNuke ACC ↑ ECE ↓ = 9.30. CE + ( SMAC + AS ) PL, PLIP.DigestPath ACC ↑ ECE ↓ = 89.75. CE + ( SMAC + AS ) PL, PLIP.DigestPath ACC ↑ ECE ↓ = 2.47. CE + ( SMAC + AS ) PL, QuiltNet.Kather Acc ↑ ECE ↓ = 89.53. CE + ( SMAC + AS ) PL, QuiltNet.Kather Acc ↑ ECE ↓ = 2.82. CE + ( SMAC + AS ) PL, QuiltNet.PanNuke ACC ↑ ECE ↓ = 69.02. CE + ( SMAC + AS ) PL, QuiltNet.PanNuke ACC ↑ ECE ↓ = 16.51. CE + ( SMAC + AS ) PL, QuiltNet.DigestPath ACC ↑ ECE ↓ = 86.52. CE + ( SMAC + AS ) PL, QuiltNet.DigestPath ACC ↑ ECE ↓ = 6.03. CE + ( SMAC + AS ) PL, Average.All ACC ↑ ECE ↓ = 80.88. CE + ( SMAC + AS ) PL, Average.All ACC ↑ ECE ↓ = 7.03. Label Smoothing FT, PLIP.Kather ACC ↑ ECE ↓ = 80.64. Label Smoothing FT, PLIP.Kather ACC ↑ ECE ↓ = 9.32. Label Smoothing FT, PLIP.PanNuke ACC ↑ ECE ↓ = 59.83. Label Smoothing FT, PLIP.PanNuke ACC ↑ ECE ↓ = 15.66. Label Smoothing FT, PLIP.DigestPath ACC ↑ ECE ↓ = 82.24. Label Smoothing FT, PLIP.DigestPath ACC ↑ ECE ↓ = 5.81. Label Smoothing FT, QuiltNet.Kather Acc ↑ ECE ↓ = 77.99. Label Smoothing FT, QuiltNet.Kather Acc ↑ ECE ↓ = 17.34. Label Smoothing FT, QuiltNet.PanNuke ACC ↑ ECE ↓ = 72.73. Label Smoothing FT, QuiltNet.PanNuke ACC ↑ ECE ↓ = 7.73. Label Smoothing FT, QuiltNet.DigestPath ACC ↑ ECE ↓ = 76.58. Label Smoothing FT, QuiltNet.DigestPath ACC ↑ ECE ↓ = 7.11. Label Smoothing FT, Average.All ACC ↑ ECE ↓ = 75.00. Label Smoothing FT, Average.All ACC ↑ ECE ↓ = 10.50. Label Smoothing PL, PLIP.Kather ACC ↑ ECE ↓ = 85.28. Label Smoothing PL, PLIP.Kather ACC ↑ ECE ↓ = 2.23. Label Smoothing PL, PLIP.PanNuke ACC ↑ ECE ↓ = 67.85. Label Smoothing PL, PLIP.PanNuke ACC ↑ ECE ↓ = 16.01. Label Smoothing PL, PLIP.DigestPath ACC ↑ ECE ↓ = 80.87. Label Smoothing PL, PLIP.DigestPath ACC ↑ ECE ↓ = 5.50. Label Smoothing PL, QuiltNet.Kather Acc ↑ ECE ↓ = 89.43. Label Smoothing PL, QuiltNet.Kather Acc ↑ ECE ↓ = 3.26. Label Smoothing PL, QuiltNet.PanNuke ACC ↑ ECE ↓ = 67.55. Label Smoothing PL, QuiltNet.PanNuke ACC ↑ ECE ↓ = 16.93. Label Smoothing PL, QuiltNet.DigestPath ACC ↑ ECE ↓ = 76.69. Label Smoothing PL, QuiltNet.DigestPath ACC ↑ ECE ↓ = 7.03. Label Smoothing PL, Average.All ACC ↑ ECE ↓ = 77.95. Label Smoothing PL, Average.All ACC ↑ ECE ↓ = 8.49. LS + MDCA FT, PLIP.Kather ACC ↑ ECE ↓ = 80.72. LS + MDCA FT, PLIP.Kather ACC ↑ ECE ↓ = 9.44. LS + MDCA FT, PLIP.PanNuke ACC ↑ ECE ↓ = 60.24. LS + MDCA FT, PLIP.PanNuke ACC ↑ ECE ↓ = 14.92. LS + MDCA FT, PLIP.DigestPath ACC ↑ ECE ↓ = 82.55. LS + MDCA FT, PLIP.DigestPath ACC ↑ ECE ↓ = 5.88. LS + MDCA FT, QuiltNet.Kather Acc ↑ ECE ↓ = 78.04. LS + MDCA FT, QuiltNet.Kather Acc ↑ ECE ↓ = 17.36. LS + MDCA FT, QuiltNet.PanNuke ACC ↑ ECE ↓ = 72.73. LS + MDCA FT, QuiltNet.PanNuke ACC ↑ ECE ↓ = 7.73. LS + MDCA FT, QuiltNet.DigestPath ACC ↑ ECE ↓ = 77.45. LS + MDCA FT, QuiltNet.DigestPath ACC ↑ ECE ↓ = 12.38. LS + MDCA FT, Average.All ACC ↑ ECE ↓ = 75.29. LS + MDCA FT, Average.All ACC ↑ ECE ↓ = 11.29. LS + MDCA PL, PLIP.Kather ACC ↑ ECE ↓ = 83.58. LS + MDCA PL, PLIP.Kather ACC ↑ ECE ↓ = 1.06. LS + MDCA PL, PLIP.PanNuke ACC ↑ ECE ↓ = 68.93. LS + MDCA PL, PLIP.PanNuke ACC ↑ ECE ↓ = 9.39. LS + MDCA PL, PLIP.DigestPath ACC ↑ ECE ↓ = 83.29. LS + MDCA PL, PLIP.DigestPath ACC ↑ ECE ↓ = 3.49. LS + MDCA PL, QuiltNet.Kather Acc ↑ ECE ↓ = 91.53. LS + MDCA PL, QuiltNet.Kather Acc ↑ ECE ↓ = 4.37. LS + MDCA PL, QuiltNet.PanNuke ACC ↑ ECE ↓ = 75.32. LS + MDCA PL, QuiltNet.PanNuke ACC ↑ ECE ↓ = 3.91. LS + MDCA PL, QuiltNet.DigestPath ACC ↑ ECE ↓ = 88.05. LS + MDCA PL, QuiltNet.DigestPath ACC ↑ ECE ↓ = 0.82. LS + MDCA PL, Average.All ACC ↑ ECE ↓ = 81.78. LS + MDCA PL, Average.All ACC ↑ ECE ↓ = 3.84. LS + SMAC FT, PLIP.Kather ACC ↑ ECE ↓ = 80.91. LS + SMAC FT, PLIP.Kather ACC ↑ ECE ↓ = 9.47. LS + SMAC FT, PLIP.PanNuke ACC ↑ ECE ↓ = 65.10. LS + SMAC FT, PLIP.PanNuke ACC ↑ ECE ↓ = 6.13. LS + SMAC FT, PLIP.DigestPath ACC ↑ ECE ↓ = 83.15. LS + SMAC FT, PLIP.DigestPath ACC ↑ ECE ↓ = 6.10. LS + SMAC FT, QuiltNet.Kather Acc ↑ ECE ↓ = 79.48. LS + SMAC FT, QuiltNet.Kather Acc ↑ ECE ↓ = 17.83. LS + SMAC FT, QuiltNet.PanNuke ACC ↑ ECE ↓ = 72.20. LS + SMAC FT, QuiltNet.PanNuke ACC ↑ ECE ↓ = 5.29. LS + SMAC FT, QuiltNet.DigestPath ACC ↑ ECE ↓ = 76.88. LS + SMAC FT, QuiltNet.DigestPath ACC ↑ ECE ↓ = 6.25. LS + SMAC FT, Average.All ACC ↑ ECE ↓ = 76.29. LS + SMAC FT, Average.All ACC ↑ ECE ↓ = 8.51. LS + SMAC PL, PLIP.Kather ACC ↑ ECE ↓ = 83.59. LS + SMAC PL, PLIP.Kather ACC ↑ ECE ↓ = 1.30. LS + SMAC PL, PLIP.PanNuke ACC ↑ ECE ↓ = 59.49. LS + SMAC PL, PLIP.PanNuke ACC ↑ ECE ↓ = 2.38. LS + SMAC PL, PLIP.DigestPath ACC ↑ ECE ↓ = 85.48. LS + SMAC PL, PLIP.DigestPath ACC ↑ ECE ↓ = 3.11. LS + SMAC PL, QuiltNet.Kather Acc ↑ ECE ↓ = 90.57. LS + SMAC PL, QuiltNet.Kather Acc ↑ ECE ↓ = 0.89. LS + SMAC PL, QuiltNet.PanNuke ACC ↑ ECE ↓ = 69.64. LS + SMAC PL, QuiltNet.PanNuke ACC ↑ ECE ↓ = 2.58. LS + SMAC PL, QuiltNet.DigestPath ACC ↑ ECE ↓ = 88.05. LS + SMAC PL, QuiltNet.DigestPath ACC ↑ ECE ↓ = 0.82. LS + SMAC PL, Average.All ACC ↑ ECE ↓ = 79.47. LS + SMAC PL, Average.All ACC ↑ ECE ↓ = 1.85. LS + ( SMAC + AS ) FT, PLIP.Kather ACC ↑ ECE ↓ = 80.95. LS + ( SMAC + AS ) FT, PLIP.Kather ACC ↑ ECE ↓ = 9.48. LS + ( SMAC + AS ) FT, PLIP.PanNuke ACC ↑ ECE ↓ = 64.67. LS + ( SMAC + AS ) FT, PLIP.PanNuke ACC ↑ ECE ↓ = 6.78. LS + ( SMAC + AS ) FT, PLIP.DigestPath ACC ↑ ECE ↓ = 83.06. LS + ( SMAC + AS ) FT, PLIP.DigestPath ACC ↑ ECE ↓ = 6.04. LS + ( SMAC + AS ) FT, QuiltNet.Kather Acc ↑ ECE ↓ = 77.98. LS + ( SMAC + AS ) FT, QuiltNet.Kather Acc ↑ ECE ↓ = 12.99. LS + ( SMAC + AS ) FT, QuiltNet.PanNuke ACC ↑ ECE ↓ = 72.09. LS + ( SMAC + AS ) FT, QuiltNet.PanNuke ACC ↑ ECE ↓ = 5.15. LS + ( SMAC + AS ) FT, QuiltNet.DigestPath ACC ↑ ECE ↓ = 76.68. LS + ( SMAC + AS ) FT, QuiltNet.DigestPath ACC ↑ ECE ↓ = 6.12. LS + ( SMAC + AS ) FT, Average.All ACC ↑ ECE ↓ = 75.91. LS + ( SMAC + AS ) FT, Average.All ACC ↑ ECE ↓ = 7.76. LS + ( SMAC + AS ) PL, PLIP.Kather ACC ↑ ECE ↓ = 85.08. LS + ( SMAC + AS ) PL, PLIP.Kather ACC ↑ ECE ↓ = 3.11. LS + ( SMAC + AS ) PL, PLIP.PanNuke ACC ↑ ECE ↓ = 58.68. LS + ( SMAC + AS ) PL, PLIP.PanNuke ACC ↑ ECE ↓ = 2.19. LS + ( SMAC + AS ) PL, PLIP.DigestPath ACC ↑ ECE ↓ = 86.30. LS + ( SMAC + AS ) PL, PLIP.DigestPath ACC ↑ ECE ↓ = 3.08. LS + ( SMAC + AS ) PL, QuiltNet.Kather Acc ↑ ECE ↓ = 87.52. LS + ( SMAC + AS ) PL, QuiltNet.Kather Acc ↑ ECE ↓ = 3.58. LS + ( SMAC + AS ) PL, QuiltNet.PanNuke ACC ↑ ECE ↓ = 71.93. LS + ( SMAC + AS ) PL, QuiltNet.PanNuke ACC ↑ ECE ↓ = 2.47. LS + ( SMAC + AS ) PL, QuiltNet.DigestPath ACC ↑ ECE ↓ = 85.52. LS + ( SMAC + AS ) PL, QuiltNet.DigestPath ACC ↑ ECE ↓ = 0.77. LS + ( SMAC + AS ) PL, Average.All ACC ↑ ECE ↓ = 79.17. LS + ( SMAC + AS ) PL, Average.All ACC ↑ ECE ↓ = 2.53HEADINGS:
# 4.1 Results
CONTENT:
As shown in Table 1, Med-VLMs demonstrate encouraging zero-shot capability on downstream medical tasks, confirming their potential utility in real-world clinical pipelines. However, these models consistently suffer from severe calibration errors, often assigning high confidence to incorrect predictions. This miscalibration is particularly concerning in the medical domain, where overconfident false predictions can compromise trust and safety. Table 2 provides a detailed comparison, showing that our method yield substantially better calibration while maintaining competitive or improved classification accuracy. In particular, LS-based methods achieve an average ECE of only 1.85%, an improvement over the 8.49% baseline. Importantly, the accuracy remains stable or slightly better across most evaluation settings, confirming that calibration-aware objectives do not trade off predictive performance. Similarly, Table 3 demonstrates that our regularizer attains state-of-the-art (SOTA) or second-best calibration across both BioMedCLIP and MedCLIP on two independent datasets. These results highlight the generalizability of our approach across different medical VLM architectures and data distributions. Finally, Table 4 reports results across multiple calibration metrics (ECE, MCE, Brier score), showing consistent improvements for both PLIP and BioMedCLIP. The improvements are not restricted to one metric but hold across the board, which underlines the robustness of our method.HEADINGS:
# 4.1 Results
CONTENT:
Table 3: Comparison of our proposed calibration regularizers ( SMAC , SMAC+AS ) with baseline methods using Cross Entropy (CE), and Label Smoothing (LS). Results show Accuracy (ACC, %) and Expected Calibration Error (ECE, %) for BioMedCLIP and MedCLIP on COVID and RSNA datasets. Best results are in bold , second-best are underlined.

Cross Entropy-based Losses, BioMedCLIP.COVID ACC ↑ ECE ↓ = Cross Entropy-based Losses. Cross Entropy-based Losses, BioMedCLIP.COVID ACC ↑ ECE ↓ = Cross Entropy-based Losses. Cross Entropy-based Losses, BioMedCLIP.RSNA ACC ↑ ECE ↓ = Cross Entropy-based Losses. Cross Entropy-based Losses, BioMedCLIP.RSNA ACC ↑ ECE ↓ = Cross Entropy-based Losses. Cross Entropy-based Losses, MedCLIP.COVID ACC ↑ ECE ↓ = Cross Entropy-based Losses. Cross Entropy-based Losses, MedCLIP.COVID ACC ↑ ECE ↓ = Cross Entropy-based Losses. Cross Entropy-based Losses, MedCLIP.RSNA ACC ↑ ECE ↓ = Cross Entropy-based Losses. Cross Entropy-based Losses, MedCLIP.RSNA ACC ↑ ECE ↓ = Cross Entropy-based Losses. Cross Entropy-based Losses, Average.All ACC ↑ ECE ↓ = Cross Entropy-based Losses. Cross Entropy-based Losses, Average.All ACC ↑ ECE ↓ = Cross Entropy-based Losses. Cross Entropy Loss PL, BioMedCLIP.COVID ACC ↑ ECE ↓ = 80.10. Cross Entropy Loss PL, BioMedCLIP.COVID ACC ↑ ECE ↓ = 6.61. Cross Entropy Loss PL, BioMedCLIP.RSNA ACC ↑ ECE ↓ = 62.98. Cross Entropy Loss PL, BioMedCLIP.RSNA ACC ↑ ECE ↓ = 7.02. Cross Entropy Loss PL, MedCLIP.COVID ACC ↑ ECE ↓ = 77.61. Cross Entropy Loss PL, MedCLIP.COVID ACC ↑ ECE ↓ = 27.51. Cross Entropy Loss PL, MedCLIP.RSNA ACC ↑ ECE ↓ = 50.68. Cross Entropy Loss PL, MedCLIP.RSNA ACC ↑ ECE ↓ = 17.21. Cross Entropy Loss PL, Average.All ACC ↑ ECE ↓ = 67.84. Cross Entropy Loss PL, Average.All ACC ↑ ECE ↓ = 14.59. CE + DCA PL, BioMedCLIP.COVID ACC ↑ ECE ↓ = 59.79. CE + DCA PL, BioMedCLIP.COVID ACC ↑ ECE ↓ = 3.75. CE + DCA PL, BioMedCLIP.RSNA ACC ↑ ECE ↓ = 51.71. CE + DCA PL, BioMedCLIP.RSNA ACC ↑ ECE ↓ = 5.79. CE + DCA PL, MedCLIP.COVID ACC ↑ ECE ↓ = 78.26. CE + DCA PL, MedCLIP.COVID ACC ↑ ECE ↓ = 28.15. CE + DCA PL, MedCLIP.RSNA ACC ↑ ECE ↓ = 50.66. CE + DCA PL, MedCLIP.RSNA ACC ↑ ECE ↓ = 17.20. CE + DCA PL, Average.All ACC ↑ ECE ↓ = 60.11. CE + DCA PL, Average.All ACC ↑ ECE ↓ = 13.72. CE+MMCE PL, BioMedCLIP.COVID ACC ↑ ECE ↓ = 81.24. CE+MMCE PL, BioMedCLIP.COVID ACC ↑ ECE ↓ = 5.38. CE+MMCE PL, BioMedCLIP.RSNA ACC ↑ ECE ↓ = 63.40. CE+MMCE PL, BioMedCLIP.RSNA ACC ↑ ECE ↓ = 11.72. CE+MMCE PL, MedCLIP.COVID ACC ↑ ECE ↓ = 77.59. CE+MMCE PL, MedCLIP.COVID ACC ↑ ECE ↓ = 27.49. CE+MMCE PL, MedCLIP.RSNA ACC ↑ ECE ↓ = 50.64. CE+MMCE PL, MedCLIP.RSNA ACC ↑ ECE ↓ = 17.17. CE+MMCE PL, Average.All ACC ↑ ECE ↓ = 68.22. CE+MMCE PL, Average.All ACC ↑ ECE ↓ = 15.44. CE + MDCA PL, BioMedCLIP.COVID ACC ↑ ECE ↓ = 78.39. CE + MDCA PL, BioMedCLIP.COVID ACC ↑ ECE ↓ = 1.13. CE + MDCA PL, BioMedCLIP.RSNA ACC ↑ ECE ↓ = 62.12. CE + MDCA PL, BioMedCLIP.RSNA ACC ↑ ECE ↓ = 8.63. CE + MDCA PL, MedCLIP.COVID ACC ↑ ECE ↓ = 77.61. CE + MDCA PL, MedCLIP.COVID ACC ↑ ECE ↓ = 27.51. CE + MDCA PL, MedCLIP.RSNA ACC ↑ ECE ↓ = 50.69. CE + MDCA PL, MedCLIP.RSNA ACC ↑ ECE ↓ = 17.22. CE + MDCA PL, Average.All ACC ↑ ECE ↓ = 67.20. CE + MDCA PL, Average.All ACC ↑ ECE ↓ = 13.62. MbLS PL, BioMedCLIP.COVID ACC ↑ ECE ↓ = 80.10. MbLS PL, BioMedCLIP.COVID ACC ↑ ECE ↓ = 6.61. MbLS PL, BioMedCLIP.RSNA ACC ↑ ECE ↓ = 62.22. MbLS PL, BioMedCLIP.RSNA ACC ↑ ECE ↓ = 6.97. MbLS PL, MedCLIP.COVID ACC ↑ ECE ↓ = 77.61. MbLS PL, MedCLIP.COVID ACC ↑ ECE ↓ = 27.51. MbLS PL, MedCLIP.RSNA ACC ↑ ECE ↓ = 50.68. MbLS PL, MedCLIP.RSNA ACC ↑ ECE ↓ = 17.21. MbLS PL, Average.All ACC ↑ ECE ↓ = 67.65. MbLS PL, Average.All ACC ↑ ECE ↓ = 14.58. LogitNorm PL, BioMedCLIP.COVID ACC ↑ ECE ↓ = 72.45. LogitNorm PL, BioMedCLIP.COVID ACC ↑ ECE ↓ = 14.39. LogitNorm PL, BioMedCLIP.RSNA ACC ↑ ECE ↓ = 46.23. LogitNorm PL, BioMedCLIP.RSNA ACC ↑ ECE ↓ = 30.37. LogitNorm PL, MedCLIP.COVID ACC ↑ ECE ↓ = 77.82. LogitNorm PL, MedCLIP.COVID ACC ↑ ECE ↓ = 27.72. LogitNorm PL, MedCLIP.RSNA ACC ↑ ECE ↓ = 50.90. LogitNorm PL, MedCLIP.RSNA ACC ↑ ECE ↓ = 17.42. LogitNorm PL, Average.All ACC ↑ ECE ↓ = 61.85. LogitNorm PL, Average.All ACC ↑ ECE ↓ = 22.48. ZS-Norm PL, BioMedCLIP.COVID ACC ↑ ECE ↓ = 79.85. ZS-Norm PL, BioMedCLIP.COVID ACC ↑ ECE ↓ = 4.16. ZS-Norm PL, BioMedCLIP.RSNA ACC ↑ ECE ↓ = 62.93. ZS-Norm PL, BioMedCLIP.RSNA ACC ↑ ECE ↓ = 8.09. ZS-Norm PL, MedCLIP.COVID ACC ↑ ECE ↓ = 77.62. ZS-Norm PL, MedCLIP.COVID ACC ↑ ECE ↓ = 27.52. ZS-Norm PL, MedCLIP.RSNA ACC ↑ ECE ↓ = 50.80. ZS-Norm PL, MedCLIP.RSNA ACC ↑ ECE ↓ = 17.32. ZS-Norm PL, Average.All ACC ↑ ECE ↓ = 67.80. ZS-Norm PL, Average.All ACC ↑ ECE ↓ = 14.27. Penalty PL, BioMedCLIP.COVID ACC ↑ ECE ↓ = 77.85. Penalty PL, BioMedCLIP.COVID ACC ↑ ECE ↓ = 9.35. Penalty PL, BioMedCLIP.RSNA ACC ↑ ECE ↓ = 50.54. Penalty PL, BioMedCLIP.RSNA ACC ↑ ECE ↓ = 2.40. Penalty PL, MedCLIP.COVID ACC ↑ ECE ↓ = 77.55. Penalty PL, MedCLIP.COVID ACC ↑ ECE ↓ = 27.45. Penalty PL, MedCLIP.RSNA ACC ↑ ECE ↓ = 50.69. Penalty PL, MedCLIP.RSNA ACC ↑ ECE ↓ = 17.22. Penalty PL, Average.All ACC ↑ ECE ↓ = 64.16. Penalty PL, Average.All ACC ↑ ECE ↓ = 14.11. CE + SMAC PL, BioMedCLIP.COVID ACC ↑ ECE ↓ = 78.39. CE + SMAC PL, BioMedCLIP.COVID ACC ↑ ECE ↓ = 1.13. CE + SMAC PL, BioMedCLIP.RSNA ACC ↑ ECE ↓ = 61.90. CE + SMAC PL, BioMedCLIP.RSNA ACC ↑ ECE ↓ = 8.44. CE + SMAC PL, MedCLIP.COVID ACC ↑ ECE ↓ = 77.61. CE + SMAC PL, MedCLIP.COVID ACC ↑ ECE ↓ = 27.51. CE + SMAC PL, MedCLIP.RSNA ACC ↑ ECE ↓ = 50.69. CE + SMAC PL, MedCLIP.RSNA ACC ↑ ECE ↓ = 17.22. CE + SMAC PL, Average.All ACC ↑ ECE ↓ = 67.15. CE + SMAC PL, Average.All ACC ↑ ECE ↓ = 13.58. CE + ( SMAC + AS ) PL, BioMedCLIP.COVID ACC ↑ ECE ↓ = 74.88. CE + ( SMAC + AS ) PL, BioMedCLIP.COVID ACC ↑ ECE ↓ = 4.04. CE + ( SMAC + AS ) PL, BioMedCLIP.RSNA ACC ↑ ECE ↓ = 58.75. CE + ( SMAC + AS ) PL, BioMedCLIP.RSNA ACC ↑ ECE ↓ = 4.23. CE + ( SMAC + AS ) PL, MedCLIP.COVID ACC ↑ ECE ↓ = 77.58. CE + ( SMAC + AS ) PL, MedCLIP.COVID ACC ↑ ECE ↓ = 27.48. CE + ( SMAC + AS ) PL, MedCLIP.RSNA ACC ↑ ECE ↓ = 50.68. CE + ( SMAC + AS ) PL, MedCLIP.RSNA ACC ↑ ECE ↓ = 17.21. CE + ( SMAC + AS ) PL, Average.All ACC ↑ ECE ↓ = 65.47. CE + ( SMAC + AS ) PL, Average.All ACC ↑ ECE ↓ = 13.24. Label Smoothing-based Losses, BioMedCLIP.COVID ACC ↑ ECE ↓ = Label Smoothing-based Losses. Label Smoothing-based Losses, BioMedCLIP.COVID ACC ↑ ECE ↓ = Label Smoothing-based Losses. Label Smoothing-based Losses, BioMedCLIP.RSNA ACC ↑ ECE ↓ = Label Smoothing-based Losses. Label Smoothing-based Losses, BioMedCLIP.RSNA ACC ↑ ECE ↓ = Label Smoothing-based Losses. Label Smoothing-based Losses, MedCLIP.COVID ACC ↑ ECE ↓ = Label Smoothing-based Losses. Label Smoothing-based Losses, MedCLIP.COVID ACC ↑ ECE ↓ = Label Smoothing-based Losses. Label Smoothing-based Losses, MedCLIP.RSNA ACC ↑ ECE ↓ = Label Smoothing-based Losses. Label Smoothing-based Losses, MedCLIP.RSNA ACC ↑ ECE ↓ = Label Smoothing-based Losses. Label Smoothing-based Losses, Average.All ACC ↑ ECE ↓ = Label Smoothing-based Losses. Label Smoothing-based Losses, Average.All ACC ↑ ECE ↓ = Label Smoothing-based Losses. Label Smoothing PL, BioMedCLIP.COVID ACC ↑ ECE ↓ = 78.64. Label Smoothing PL, BioMedCLIP.COVID ACC ↑ ECE ↓ = 12.65. Label Smoothing PL, BioMedCLIP.RSNA ACC ↑ ECE ↓ = 62.13. Label Smoothing PL, BioMedCLIP.RSNA ACC ↑ ECE ↓ = 16.93. Label Smoothing PL, MedCLIP.COVID ACC ↑ ECE ↓ = 77.61. Label Smoothing PL, MedCLIP.COVID ACC ↑ ECE ↓ = 27.51. Label Smoothing PL, MedCLIP.RSNA ACC ↑ ECE ↓ = 50.63. Label Smoothing PL, MedCLIP.RSNA ACC ↑ ECE ↓ = 17.16. Label Smoothing PL, Average.All ACC ↑ ECE ↓ = 67.25. Label Smoothing PL, Average.All ACC ↑ ECE ↓ = 18.56. LS + MDCA PL, BioMedCLIP.COVID ACC ↑ ECE ↓ = 74.77. LS + MDCA PL, BioMedCLIP.COVID ACC ↑ ECE ↓ = 13.33. LS + MDCA PL, BioMedCLIP.RSNA ACC ↑ ECE ↓ = 63.13. LS + MDCA PL, BioMedCLIP.RSNA ACC ↑ ECE ↓ = 18.89. LS + MDCA PL, MedCLIP.COVID ACC ↑ ECE ↓ = 77.51. LS + MDCA PL, MedCLIP.COVID ACC ↑ ECE ↓ = 27.41. LS + MDCA PL, MedCLIP.RSNA ACC ↑ ECE ↓ = 50.63. LS + MDCA PL, MedCLIP.RSNA ACC ↑ ECE ↓ = 17.16. LS + MDCA PL, Average.All ACC ↑ ECE ↓ = 66.51. LS + MDCA PL, Average.All ACC ↑ ECE ↓ = 19.20. LS + SMAC PL, BioMedCLIP.COVID ACC ↑ ECE ↓ = 78.29. LS + SMAC PL, BioMedCLIP.COVID ACC ↑ ECE ↓ = 9.83. LS + SMAC PL, BioMedCLIP.RSNA ACC ↑ ECE ↓ = 63.67. LS + SMAC PL, BioMedCLIP.RSNA ACC ↑ ECE ↓ = 15.82. LS + SMAC PL, MedCLIP.COVID ACC ↑ ECE ↓ = 77.51. LS + SMAC PL, MedCLIP.COVID ACC ↑ ECE ↓ = 27.41. LS + SMAC PL, MedCLIP.RSNA ACC ↑ ECE ↓ = 50.63. LS + SMAC PL, MedCLIP.RSNA ACC ↑ ECE ↓ = 17.16. LS + SMAC PL, Average.All ACC ↑ ECE ↓ = 67.53. LS + SMAC PL, Average.All ACC ↑ ECE ↓ = 17.56. LS + ( SMAC + AS ) PL, BioMedCLIP.COVID ACC ↑ ECE ↓ = 74.84. LS + ( SMAC + AS ) PL, BioMedCLIP.COVID ACC ↑ ECE ↓ = 4.69. LS + ( SMAC + AS ) PL, BioMedCLIP.RSNA ACC ↑ ECE ↓ = 59.51. LS + ( SMAC + AS ) PL, BioMedCLIP.RSNA ACC ↑ ECE ↓ = 5.74. LS + ( SMAC + AS ) PL, MedCLIP.COVID ACC ↑ ECE ↓ = 77.51. LS + ( SMAC + AS ) PL, MedCLIP.COVID ACC ↑ ECE ↓ = 27.41. LS + ( SMAC + AS ) PL, MedCLIP.RSNA ACC ↑ ECE ↓ = 50.61. LS + ( SMAC + AS ) PL, MedCLIP.RSNA ACC ↑ ECE ↓ = 17.14. LS + ( SMAC + AS ) PL, Average.All ACC ↑ ECE ↓ = 65.62. LS + ( SMAC + AS ) PL, Average.All ACC ↑ ECE ↓ = 13.75HEADINGS:
# 4.2 Ablations
CONTENT:
Number of Few-shots: Figure 3 illustrates the effect of varying the number of shots per class. As expected, increasing the number of training examples consistently improves accuracy (ACC) while also reducing calibration error (ECE). This suggests that additional supervision not only strengthens discriminative ability but also stabilizes confidence estimation. Context Length: Figure 3 also evaluates the influence of prompt token length. Our approach achieves optimal performance at 16 tokens, balancing expressivity and stability. Beyond this point, increasing the token count introduces instability and variance in both ACC and ECE. Med-VLMs for Application-Specific Tasks: We further examine the integration of our calibration-aware design with application-specific objectives. In particular, adding Angular Loss to PromptSmooth [10] yields measurable improvements in calibration while maintaining high accuracy. Specifically, PromptSmooth (few-shot + zero-shot) achieves 76.6% ACC with 15.54% ECE, while the addition of Angular Loss slightly decreases ACC to 76.2% but significantly reduces ECE to 13.62%. This reduction in calibration error, despite marginal accuracy changes, demonstrates that reliability can be substantially improved without compromising predictive utility.HEADINGS:
# 5 Conclusion
CONTENT:
We introduced CalibPrompt , a calibration-aware prompt tuning framework for MedVLMs that improves confidence reliability while keeping the backbone frozen, making it efficient and deployment-friendly. By combining SMCA loss for probability-space cali-HEADINGS:
# 5 Conclusion
CONTENT:
Table 4: Average calibration results of PLIP (PanNuke, DigestPath) and BioMedCLIP (COVID, RSNA) using ECE, ACE, MCE, and ECE KDE . Bold and underline denote best and second-best scores, respectively.

Cross Entropy-based Losses, PLIP.ECE ↓ = Cross Entropy-based Losses. Cross Entropy-based Losses, PLIP.ACE ↓ = Cross Entropy-based Losses. Cross Entropy-based Losses, PLIP.MCE ↓ = Cross Entropy-based Losses. Cross Entropy-based Losses, PLIP.KDE ↓ = Cross Entropy-based Losses. Cross Entropy-based Losses, BioMedCLIP.ECE ↓ = Cross Entropy-based Losses. Cross Entropy-based Losses, BioMedCLIP.ACE ↓ = Cross Entropy-based Losses. Cross Entropy-based Losses, BioMedCLIP.MCE ↓ = Cross Entropy-based Losses. Cross Entropy-based Losses, BioMedCLIP.KDE ↓ = Cross Entropy-based Losses. Cross Entropy Loss PL, PLIP.ECE ↓ = 13.66. Cross Entropy Loss PL, PLIP.ACE ↓ = 13.66. Cross Entropy Loss PL, PLIP.MCE ↓ = 8.53. Cross Entropy Loss PL, PLIP.KDE ↓ = 13.25. Cross Entropy Loss PL, BioMedCLIP.ECE ↓ = 6.82. Cross Entropy Loss PL, BioMedCLIP.ACE ↓ = 6.78. Cross Entropy Loss PL, BioMedCLIP.MCE ↓ = 2.65. Cross Entropy Loss PL, BioMedCLIP.KDE ↓ = 6.54. CE + DCA PL, PLIP.ECE ↓ = 12.70. CE + DCA PL, PLIP.ACE ↓ = 12.90. CE + DCA PL, PLIP.MCE ↓ = 7.40. CE + DCA PL, PLIP.KDE ↓ = 12.76. CE + DCA PL, BioMedCLIP.ECE ↓ = 4.77. CE + DCA PL, BioMedCLIP.ACE ↓ = 7.03. CE + DCA PL, BioMedCLIP.MCE ↓ = 2.26. CE + DCA PL, BioMedCLIP.KDE ↓ = 4.43. CE+MMCE PL, PLIP.ECE ↓ = 10.92. CE+MMCE PL, PLIP.ACE ↓ = 11.06. CE+MMCE PL, PLIP.MCE ↓ = 6.33. CE+MMCE PL, PLIP.KDE ↓ = 10.77. CE+MMCE PL, BioMedCLIP.ECE ↓ = 8.55. CE+MMCE PL, BioMedCLIP.ACE ↓ = 8.55. CE+MMCE PL, BioMedCLIP.MCE ↓ = 3.57. CE+MMCE PL, BioMedCLIP.KDE ↓ = 8.49. CE + MDCA PL, PLIP.ECE ↓ = 6.89. CE + MDCA PL, PLIP.ACE ↓ = 6.88. CE + MDCA PL, PLIP.MCE ↓ = 4.09. CE + MDCA PL, PLIP.KDE ↓ = 6.60. CE + MDCA PL, BioMedCLIP.ECE ↓ = 4.88. CE + MDCA PL, BioMedCLIP.ACE ↓ = 4.87. CE + MDCA PL, BioMedCLIP.MCE ↓ = 2.08. CE + MDCA PL, BioMedCLIP.KDE ↓ = 5.10. MbLS PL, PLIP.ECE ↓ = 13.68. MbLS PL, PLIP.ACE ↓ = 13.68. MbLS PL, PLIP.MCE ↓ = 8.53. MbLS PL, PLIP.KDE ↓ = 13.26. MbLS PL, BioMedCLIP.ECE ↓ = 6.79. MbLS PL, BioMedCLIP.ACE ↓ = 6.77. MbLS PL, BioMedCLIP.MCE ↓ = 2.65. MbLS PL, BioMedCLIP.KDE ↓ = 6.56. LogitNorm PL, PLIP.ECE ↓ = 20.38. LogitNorm PL, PLIP.ACE ↓ = 20.38. LogitNorm PL, PLIP.MCE ↓ = 14.46. LogitNorm PL, PLIP.KDE ↓ = 19.73. LogitNorm PL, BioMedCLIP.ECE ↓ = 22.38. LogitNorm PL, BioMedCLIP.ACE ↓ = 22.38. LogitNorm PL, BioMedCLIP.MCE ↓ = 10.47. LogitNorm PL, BioMedCLIP.KDE ↓ = 22.18. ZS-Norm PL, PLIP.ECE ↓ = 12.27. ZS-Norm PL, PLIP.ACE ↓ = 12.27. ZS-Norm PL, PLIP.MCE ↓ = 7.99. ZS-Norm PL, PLIP.KDE ↓ = 11.82. ZS-Norm PL, BioMedCLIP.ECE ↓ = 6.13. ZS-Norm PL, BioMedCLIP.ACE ↓ = 6.11. ZS-Norm PL, BioMedCLIP.MCE ↓ = 2.11. ZS-Norm PL, BioMedCLIP.KDE ↓ = 5.91. Penalty PL, PLIP.ECE ↓ = 2.76. Penalty PL, PLIP.ACE ↓ = 3.05. Penalty PL, PLIP.MCE ↓ = 1.16. Penalty PL, PLIP.KDE ↓ = 3.27. Penalty PL, BioMedCLIP.ECE ↓ = 5.88. Penalty PL, BioMedCLIP.ACE ↓ = 6.06. Penalty PL, BioMedCLIP.MCE ↓ = 2.26. Penalty PL, BioMedCLIP.KDE ↓ = 6.08. CE + SMAC PL, PLIP.ECE ↓ = 6.89. CE + SMAC PL, PLIP.ACE ↓ = 6.88. CE + SMAC PL, PLIP.MCE ↓ = 4.09. CE + SMAC PL, PLIP.KDE ↓ = 6.60. CE + SMAC PL, BioMedCLIP.ECE ↓ = 4.79. CE + SMAC PL, BioMedCLIP.ACE ↓ = 4.78. CE + SMAC PL, BioMedCLIP.MCE ↓ = 2.09. CE + SMAC PL, BioMedCLIP.KDE ↓ = 5.02. CE + ( SMAC + AS ) PL, PLIP.ECE ↓ = 5.89. CE + ( SMAC + AS ) PL, PLIP.ACE ↓ = 5.77. CE + ( SMAC + AS ) PL, PLIP.MCE ↓ = 2.90. CE + ( SMAC + AS ) PL, PLIP.KDE ↓ = 5.82. CE + ( SMAC + AS ) PL, BioMedCLIP.ECE ↓ = 4.14. CE + ( SMAC + AS ) PL, BioMedCLIP.ACE ↓ = 4.28. CE + ( SMAC + AS ) PL, BioMedCLIP.MCE ↓ = 1.70. CE + ( SMAC + AS ) PL, BioMedCLIP.KDE ↓ = 4.60. Label Smoothing-based Losses, PLIP.ECE ↓ = Label Smoothing-based Losses. Label Smoothing-based Losses, PLIP.ACE ↓ = Label Smoothing-based Losses. Label Smoothing-based Losses, PLIP.MCE ↓ = Label Smoothing-based Losses. Label Smoothing-based Losses, PLIP.KDE ↓ = Label Smoothing-based Losses. Label Smoothing-based Losses, BioMedCLIP.ECE ↓ = Label Smoothing-based Losses. Label Smoothing-based Losses, BioMedCLIP.ACE ↓ = Label Smoothing-based Losses. Label Smoothing-based Losses, BioMedCLIP.MCE ↓ = Label Smoothing-based Losses. Label Smoothing-based Losses, BioMedCLIP.KDE ↓ = Label Smoothing-based Losses. Label Smoothing PL, PLIP.ECE ↓ = 10.76. Label Smoothing PL, PLIP.ACE ↓ = 10.76. Label Smoothing PL, PLIP.MCE ↓ = 6.32. Label Smoothing PL, PLIP.KDE ↓ = 10.71. Label Smoothing PL, BioMedCLIP.ECE ↓ = 14.79. Label Smoothing PL, BioMedCLIP.ACE ↓ = 14.79. Label Smoothing PL, BioMedCLIP.MCE ↓ = 7.08. Label Smoothing PL, BioMedCLIP.KDE ↓ = 14.64. LS + MDCA PL, PLIP.ECE ↓ = 6.44. LS + MDCA PL, PLIP.ACE ↓ = 6.43. LS + MDCA PL, PLIP.MCE ↓ = 2.87. LS + MDCA PL, PLIP.KDE ↓ = 6.37. LS + MDCA PL, BioMedCLIP.ECE ↓ = 16.09. LS + MDCA PL, BioMedCLIP.ACE ↓ = 16.09. LS + MDCA PL, BioMedCLIP.MCE ↓ = 9.19. LS + MDCA PL, BioMedCLIP.KDE ↓ = 15.87. LS + SMAC PL, PLIP.ECE ↓ = 2.75. LS + SMAC PL, PLIP.ACE ↓ = 4.05. LS + SMAC PL, PLIP.MCE ↓ = 1.90. LS + SMAC PL, PLIP.KDE ↓ = 2.09. LS + SMAC PL, BioMedCLIP.ECE ↓ = 12.82. LS + SMAC PL, BioMedCLIP.ACE ↓ = 12.82. LS + SMAC PL, BioMedCLIP.MCE ↓ = 5.49. LS + SMAC PL, BioMedCLIP.KDE ↓ = 12.69. LS + ( SMAC + AS ) PL, PLIP.ECE ↓ = 2.64. LS + ( SMAC + AS ) PL, PLIP.ACE ↓ = 3.51. LS + ( SMAC + AS ) PL, PLIP.MCE ↓ = 1.73. LS + ( SMAC + AS ) PL, PLIP.KDE ↓ = 2.25. LS + ( SMAC + AS ) PL, BioMedCLIP.ECE ↓ = 5.22. LS + ( SMAC + AS ) PL, BioMedCLIP.ACE ↓ = 5.14. LS + ( SMAC + AS ) PL, BioMedCLIP.MCE ↓ = 2.15. LS + ( SMAC + AS ) PL, BioMedCLIP.KDE ↓ = 5.27HEADINGS:
# 5 Conclusion
CONTENT:
Figure 3: Comparative analysis of accuracy (ACC) and calibration error (ECE) for different loss functions across varying few-shot counts and context token lengths.
bration with our proposed Angular Separation ( AS ) loss for feature regularization, CalibPrompt mitigates overconfidence and enhances uncertainty estimation, yielding consistently lower calibration errors without sacrificing accuracy across multiple Med-VLMs and datasets. Extensive experiments and ablations confirm the complementary benefits of few-shot supervision and prompt design choices, while broader results highlight that calibration should be treated as an integral part of training rather than a post-hoc fix. In the future, we envision extending CalibPrompt beyond classification to more challenging tasks such as medical report generation, cross-modal retrieval, and multimodal reasoning, thereby advancing the development of calibration-aware Med-VLMs that are both reliable and clinically actionable.HEADINGS:
# References
CONTENT:
- [1] Mark A Chia, Fares Antaki, Yukun Zhou, Angus W Turner, Aaron Y Lee, and Pearse A Keane. Foundation models in ophthalmology. British Journal of Ophthalmology , 108 (10):1341-1348, 2024.
- [2] Qian Da, Xiaodi Huang, Zhongyu Li, Yanfei Zuo, Chenbin Zhang, Jingxin Liu, Wen Chen, Jiahui Li, Dou Xu, Zhiqiang Hu, et al. Digestpath: A benchmark dataset with challenge review for the pathological detection and segmentation of digestive-system. Medical Image Analysis , 80:102485, 2022.
- [3] Jevgenij Gamper, Navid Alemi Koohbanani, Ksenija Benet, Ali Khuram, and Nasir Rajpoot. Pannuke: an open pan-cancer histology dataset for nuclei instance segmentation and classification. In Digital Pathology: 15th European Congress, ECDP 2019, Warwick, UK, April 10-13, 2019, Proceedings 15 , pages 11-19. Springer, 2019.
- [4] Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural networks. In ICML , pages 1321-1330. PMLR, 2017.
- [5] Asif Hanif, Fahad Shamshad, Muhammad Awais, Muzammal Naseer, Fahad Shahbaz Khan, Karthik Nandakumar, Salman Khan, and Rao Muhammad Anwer. Baple: Backdoor attacks on medical foundational models using prompt learning. In International Conference on Medical Image Computing and Computer-Assisted Intervention , pages 443-453. Springer, 2024.
- [6] Ramya Hebbalaguppe, Jatin Prakash, Neelabh Madan, and Chetan Arora. A stitch in time saves nine: A train-time regularizing loss for improved neural network calibration. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 16081-16090, 2022.
- [7] Ramya Hebbalaguppe, Jatin Prakash, Neelabh Madan, and Chetan Arora. A stitch in time saves nine: A train-time regularizing loss for improved neural network calibration. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 16081-16090, 2022.
- [8] Yingxiang Huang, Wentao Li, Fima Macheret, Rodney A Gabriel, and Lucila OhnoMachado. A tutorial on calibration measurements and calibration models for clinical prediction models. Journal of the American Medical Informatics Association , 27(4): 621-633, 2020.
- [9] Zhi Huang, Federico Bianchi, Mert Yuksekgonul, Thomas J Montine, and James Zou. A visual-language foundation model for pathology image analysis using medical twitter. Nature medicine , 29(9):2307-2316, 2023.
- [10] Noor Hussein, Fahad Shamshad, Muzammal Naseer, and Karthik Nandakumar. Promptsmooth: Certifying robustness of medical vision-language models via prompt learning. In International Conference on Medical Image Computing and ComputerAssisted Intervention , pages 698-708. Springer, 2024.
- [11] Wisdom Ikezogwo, Saygin Seyfioglu, Fatemeh Ghezloo, Dylan Geva, Fatwir Sheikh Mohammed, Pavan Kumar Anand, Ranjay Krishna, and Linda Shapiro. Quilt1m: One million image-text pairs for histopathology. Advances in neural information processing systems , 36:37995-38017, 2023.HEADINGS:
# References
CONTENT:
- [12] Jakob Nikolas Kather, Johannes Krisam, Pornpimol Charoentong, Tom Luedde, Esther Herpel, Cleo-Aron Weis, Timo Gaiser, Alexander Marx, Nektarios A Valous, Dyke Ferber, et al. Predicting survival from colorectal cancer histology slides using deep learning: A retrospective multicenter study. PLoS medicine , 16(1):e1002730, 2019.
- [13] Vinith Kugathasan, Honglu Zhou, Zachary Izzo, Gayal Kuruppu, Sanoojan Baliah, and Muhammad Haris Khan. Matching confidences and softened target occurrences for calibration. In 2024 International Conference on Digital Image Computing: Techniques and Applications (DICTA) , pages 109-116. IEEE, 2024.
- [14] Aviral Kumar, Sunita Sarawagi, and Ujjwal Jain. Trainable calibration measures for neural networks from kernel mean embeddings. In International Conference on Machine Learning , pages 2805-2814. PMLR, 2018.
- [15] Aviral Kumar, Sunita Sarawagi, and Ujjwal Jain. Trainable calibration measures for neural networks from kernel mean embeddings. In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th International Conference on Machine Learning , volume 80 of Proceedings of Machine Learning Research , pages 2805-2814. PMLR, 10-15 Jul 2018. URL https://proceedings.mlr.press/v80/kumar18a. html .
- [16] Benjamin Lambert, Florence Forbes, Senan Doyle, Harmonie Dehaene, and Michel Dojat. Trustworthy clinical ai solutions: a unified review of uncertainty quantification in deep learning models for medical image analysis. Artificial Intelligence in Medicine , page 102830, 2024.
- [17] Gongbo Liang, Yu Zhang, Xiaoqin Wang, and Nathan Jacobs. Improved trainable calibration method for neural networks on medical imaging classification. arXiv preprint arXiv:2009.04057 , 2020.
- [18] Bingyuan Liu, Ismail Ben Ayed, Adrian Galdran, and Jose Dolz. The devil is in the margin: Margin-based label smoothing for network calibration. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 80-88, 2022.
- [19] Michael Moor, Oishi Banerjee, Zahra Shakeri Hossein Abad, Harlan M Krumholz, Jure Leskovec, Eric J Topol, and Pranav Rajpurkar. Foundation models for generalist medical artificial intelligence. Nature , 616(7956):259-265, 2023.
- [20] Balamurali Murugesan, Julio Silva-Rodríguez, Ismail Ben Ayed, and Jose Dolz. Robust calibration of large vision-language adapters. In European Conference on Computer Vision , pages 147-165. Springer, 2024.
- [21] Mahdi Pakdaman Naeini, Gregory Cooper, and Milos Hauskrecht. Obtaining well calibrated probabilities using bayesian binning. In Proceedings of the AAAI conference on artificial intelligence , volume 29, 2015.
- [22] Alexandru Niculescu-Mizil and Rich Caruana. Predicting good probabilities with supervised learning. In Proceedings of the 22nd international conference on Machine learning , pages 625-632, 2005.HEADINGS:
# References
CONTENT:
- [23] Jeremy Nixon, Michael W Dusenberry, Linchuan Zhang, Ghassen Jerfel, and Dustin Tran. Measuring calibration in deep learning. In CVPR workshops , volume 2, 2019.
- [24] Harsha Nori, Yin Tat Lee, Sheng Zhang, Dean Carignan, Richard Edgar, Nicolo Fusi, Nicholas King, Jonathan Larson, Yuanzhi Li, Weishung Liu, et al. Can generalist foundation models outcompete special-purpose tuning? case study in medicine. arXiv preprint arXiv:2311.16452 , 2023.
- [25] Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, David Sculley, Sebastian Nowozin, Joshua Dillon, Balaji Lakshminarayanan, and Jasper Snoek. Can you trust your model's uncertainty? evaluating predictive uncertainty under dataset shift. Advances in neural information processing systems , 32, 2019.
- [26] John Platt et al. Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods. Advances in large margin classifiers , 10(3):61-74, 1999.
- [27] Teodora Popordanoska, Raphael Sayer, and Matthew Blaschko. A consistent and differentiable lp canonical calibration error estimator. Advances in Neural Information Processing Systems , 35:7933-7946, 2022.
- [28] Tawsifur Rahman, Amith Khandakar, Yazan Qiblawey, Anas Tahir, Serkan Kiranyaz, Saad Bin Abul Kashem, Mohammad Tariqul Islam, Somaya Al Maadeed, Susu M Zughaier, Muhammad Salman Khan, et al. Exploring the effect of image enhancement techniques on covid-19 detection using chest x-ray images. Computers in biology and medicine , 132:104319, 2021.
- [29] Ashshak Sharifdeen, Muhammad Akhtar Munir, Sanoojan Baliah, Salman Khan, and Muhammad Haris Khan. O-tpt: Orthogonality constraints for calibrating test-time prompt tuning in vision-language models. arXiv preprint arXiv:2503.12096 , 2025.
- [30] Anouk Stein, Carol Wu, Chris Carr, George Shih, Jamie Dulkowski, J KalpathyCramer, et al. Rsna pneumonia detection challenge. Mountain View: Kaggle , 2018.
- [31] Shuoyuan Wang, Jindong Wang, Guoqing Wang, Bob Zhang, Kaiyang Zhou, and Hongxin Wei. Open-vocabulary calibration for fine-tuned clip. In International Conference on Machine Learning , pages 51734-51754. PMLR, 2024.
- [32] Zifeng Wang, Zhenbang Wu, Dinesh Agarwal, and Jimeng Sun. Medclip: Contrastive learning from unpaired medical images and text. In Proceedings of the Conference on Empirical Methods in Natural Language Processing. Conference on Empirical Methods in Natural Language Processing , volume 2022, page 3876, 2022.
- [33] Hongxin Wei, Renchunzi Xie, Hao Cheng, Lei Feng, Bo An, and Yixuan Li. Mitigating neural network overconfidence with logit normalization. In International conference on machine learning , pages 23631-23644. PMLR, 2022.
- [34] Hee Suk Yoon, Eunseop Yoon, Joshua Tian Jin Tee, Mark Hasegawa-Johnson, Yingzhen Li, and Chang D Yoo. C-tpt: Calibrated test-time prompt tuning for visionlanguage models via text feature dispersion. arXiv preprint arXiv:2403.14119 , 2024.HEADINGS:
# References
CONTENT:
- [35] Sheng Zhang, Yanbo Xu, Naoto Usuyama, Hanwen Xu, Jaspreet Bagga, Robert Tinn, Sam Preston, Rajesh Rao, Mu Wei, Naveen Valluri, et al. Biomedclip: a multimodal biomedical foundation model pretrained from fifteen million scientific image-text pairs. arXiv preprint arXiv:2303.00915 , 2023.
- [36] Zihao Zhao, Yuxiao Liu, Han Wu, Mei Wang, Yonghao Li, Sheng Wang, Lin Teng, Disheng Liu, Zhiming Cui, Qian Wang, et al. Clip in medical imaging: A comprehensive survey. arXiv preprint arXiv:2312.07353 , 2023.
- [37] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for vision-language models. International Journal of Computer Vision , 130(9):23372348, 2022.HEADINGS:
# 6 Hyperparameter Settings
CONTENT:
Our hyperparameter selection revealed distinct patterns across imaging modalities and model architectures. Tables 5 and 6 summarize our key hyperparameter configurations.
For histopathology datasets, we observed that effective calibration required more conservative regularization, with smaller SMAC values ( α ≈ 0 . 03-0 . 07) and minimal AS weights (0.001-0.01) for most models. This suggests that histopathology models are more sensitive to excessive regularization, likely due to the fine-grained nature of cellular features that require preserving subtle discriminative patterns. Interestingly, the QuiltNet architecture demonstrated greater robustness to stronger regularization, particularly on the Kather dataset where an AS weight of 1.0 proved optimal.
In contrast, radiological datasets consistently benefited from stronger regularization, with higher SMAC values ( α = 0 . 1-0 . 2) and substantially larger AS weights (1.0-3.0). This difference suggests that models trained on radiological images are more prone to overconfidence, potentially due to the lower spatial resolution of diagnostic features, necessitating stronger calibration signals during training.HEADINGS:
# 6 Hyperparameter Settings
CONTENT:
We further extended our analysis to include advanced calibration techniques such as Margin-based Logit Suppression (MBLS) and LogitNorm. For MBLS, we found that a consistent margin threshold of 10.0 with an α value of 0.1 worked effectively across both imaging modalities. The consistency of these parameters, unlike the variability observed with SMAC and AS , suggests that MBLS provides a more universally applicable calibration mechanism that is less sensitive to dataset-specific characteristics. LogitNorm similarly demonstrated robust performance with a standard temperature parameter of 1.0 across all datasets, indicating its adaptability across different medical imaging contexts.
For zero-shot calibration techniques (ECCV_PENALTY and ECCV_ZS), we maintained uniform weighting factors of 1.0 across all datasets. While these methods offer the advantage of leveraging pre-trained knowledge, their consistent parameterization reflects their primary reliance on the underlying zero-shot model rather than dataset-specific hyperparameter tuning.HEADINGS:
# 6 Hyperparameter Settings
CONTENT:
For combined loss functions, we maintained consistency in hyperparameter values. When using SMAC with Label Smoothing (LS), we ensured matching α values (e.g., LS α =0.05 with SMAC α =0.05 for Kather). Similarly, when combining with Focal Loss (FL), we maintained consistent hyperparameters across loss components (e.g., FL γ =3.0 with SMAC α =0.1 for radiological datasets). This consistency principle was extended to our newer calibration techniques as well, particularly when combining MBLS with other losses, where we maintained its standard parameterization to preserve its inherent calibration capabilities.HEADINGS:
# 6 Hyperparameter Settings
CONTENT:
For combined loss functions, we maintained consistency in hyperparameter values. When using SMAC with Label Smoothing (LS), we ensured matching α values (e.g., LS α =0.05 with SMAC α =0.05 for Kather). Similarly, when combining with Focal Loss (FL), we maintained consistent hyperparameters across loss components (e.g., FL γ =3.0 with SMAC α =0.1 for radiological datasets).HEADINGS:
# 6 Hyperparameter Settings
CONTENT:
Table 5: Hyperparameter settings for histopathology datasets.

DCA weight, Kather = 9.0. DCA weight, Pannuke = 9.0. DCA weight, DigestPath = 9.0. Focal Loss ( γ ), Kather = 3.0. Focal Loss ( γ ), Pannuke = 3.0. Focal Loss ( γ ), DigestPath = 3.0. Label Smoothing ( α ), Kather = 0.05. Label Smoothing ( α ), Pannuke = 0.05. Label Smoothing ( α ), DigestPath = 0.05. MMCEweight, Kather = 1.0. MMCEweight, Pannuke = 1.0. MMCEweight, DigestPath = 1.0. MDCA weight, Kather = 1.0. MDCA weight, Pannuke = 1.0. MDCA weight, DigestPath = 1.0. PLIP Model, Kather = PLIP Model. PLIP Model, Pannuke = PLIP Model. PLIP Model, DigestPath = PLIP Model. SMAC ( α ), Kather = 0.03-0.07. SMAC ( α ), Pannuke = 0.2. SMAC ( α ), DigestPath = 0.03. AS weight, Kather = 0.01. AS weight, Pannuke = 0.1. AS weight, DigestPath = 0.001. QuiltNet Model, Kather = QuiltNet Model. QuiltNet Model, Pannuke = QuiltNet Model. QuiltNet Model, DigestPath = QuiltNet Model. SMAC ( α ), Kather = 0.01-0.07. SMAC ( α ), Pannuke = 0.1. SMAC ( α ), DigestPath = 0.05. AS weight, Kather = 1.0. AS weight, Pannuke = 0.001. AS weight, DigestPath = 0.001HEADINGS:
# 6 Hyperparameter Settings
CONTENT:
Table 6: Hyperparameter settings for radiological datasets.

DCA weight, BioMedCLIP = 9.0. DCA weight, MedCLIP = 9.0. Focal Loss ( γ ), BioMedCLIP = 3.0. Focal Loss ( γ ), MedCLIP = 3.0. Label Smoothing ( α ), BioMedCLIP = 0.2. Label Smoothing ( α ), MedCLIP = 0.2. MMCEweight, BioMedCLIP = 2.0. MMCEweight, MedCLIP = 2.0. MDCA weight, BioMedCLIP = 1.0. MDCA weight, MedCLIP = 1.0. SMAC ( α ), BioMedCLIP = 0.1. SMAC ( α ), MedCLIP = 0.2. AS weight, BioMedCLIP = 3.0. AS weight, MedCLIP = 1.0HEADINGS:
# 7 Evaluation Metrics
CONTENT:
We evaluated model performance using both accuracy and calibration metrics:
- -Accuracy (ACC) : Proportion of correctly classified samples, indicating overall predictive performance.
- -Expected Calibration Error (ECE) [21]: Measures the weighted average difference between predicted confidence and actual accuracy across 10 fixed-width bins.
- -Adaptive Calibration Error (ACE) [23]: Similar to ECE but uses adaptive binning to ensure equal sample counts per bin, improving reliability on skewed confidence distributions.
- -Maximum Calibration Error (MCE) [21]: Reports the largest absolute difference between confidence and accuracy across all bins, highlighting worst-case miscalibration.
- -ECE with Kernel Density Estimation (ECE KDE ) [27]: A binning-free metric that uses kernel density estimation to compute a continuous calibration error estimate. Our implementation uses a simplified Gaussian kernel rather than the Dirichlet/Beta kernels from the original paper, offering computational efficiency while maintainingHEADINGS:
# 7 Evaluation Metrics
CONTENT:
Table 7: Comparison of proposed regularizers ( SMAC , SMAC+AS ) with baseline methods using Focal Loss (FL). Accuracy (ACC, %) and Expected Calibration Error (ECE, %) are shown for PLIP and QuiltNet on histopathology datasets (Kather, PanNuke, DigestPath). Subscripts FT and PL denote Few-shot Fine-Tuning and Prompt Learning. Best results are in bold , second-best underlined.

Focal Loss-based Losses, PLIP.Kather.ACC ↑ ECE = Focal Loss-based Losses. Focal Loss-based Losses, PLIP.Kather.↓ = Focal Loss-based Losses. Focal Loss-based Losses, PLIP.PanNuke.ACC = Focal Loss-based Losses. Focal Loss-based Losses, PLIP.PanNuke.↑ ECE ↓ = Focal Loss-based Losses. Focal Loss-based Losses, PLIP.DigestPath.ACC ↑ = Focal Loss-based Losses. Focal Loss-based Losses, PLIP.DigestPath.ECE ↓ = Focal Loss-based Losses. Focal Loss-based Losses, QuiltNet.Kather ↑.Acc = Focal Loss-based Losses. Focal Loss-based Losses, QuiltNet.Kather ↑.ECE ↓ = Focal Loss-based Losses. Focal Loss-based Losses, QuiltNet.PanNuke ↓.ACC ↑ = Focal Loss-based Losses. Focal Loss-based Losses, QuiltNet.PanNuke ↓.ECE = Focal Loss-based Losses. Focal Loss-based Losses, QuiltNet.DigestPath ↑ ↓.ACC = Focal Loss-based Losses. Focal Loss-based Losses, QuiltNet.DigestPath ↑ ↓.ECE = Focal Loss-based Losses. Focal Loss-based Losses, Average.All ↑.ACC = Focal Loss-based Losses. Focal Loss-based Losses, Average.All ↑.ECE ↓ = Focal Loss-based Losses. Focal Loss FT, PLIP.Kather.ACC ↑ ECE = 80.91. Focal Loss FT, PLIP.Kather.↓ = 12.06. Focal Loss FT, PLIP.PanNuke.ACC = 58.23. Focal Loss FT, PLIP.PanNuke.↑ ECE ↓ = 17.00. Focal Loss FT, PLIP.DigestPath.ACC ↑ = 80.56. Focal Loss FT, PLIP.DigestPath.ECE ↓ = 6.22. Focal Loss FT, QuiltNet.Kather ↑.Acc = 79.23. Focal Loss FT, QuiltNet.Kather ↑.ECE ↓ = 19.84. Focal Loss FT, QuiltNet.PanNuke ↓.ACC ↑ = 72.89. Focal Loss FT, QuiltNet.PanNuke ↓.ECE = 7.93. Focal Loss FT, QuiltNet.DigestPath ↑ ↓.ACC = 77.81. Focal Loss FT, QuiltNet.DigestPath ↑ ↓.ECE = 7.73. Focal Loss FT, Average.All ↑.ACC = 74.94. Focal Loss FT, Average.All ↑.ECE ↓ = 11.80. Focal Loss PL, PLIP.Kather.ACC ↑ ECE = 84.07. Focal Loss PL, PLIP.Kather.↓ = 2.82. Focal Loss PL, PLIP.PanNuke.ACC = 62.65. Focal Loss PL, PLIP.PanNuke.↑ ECE ↓ = 3.27. Focal Loss PL, PLIP.DigestPath.ACC ↑ = 87.30. Focal Loss PL, PLIP.DigestPath.ECE ↓ = 7.94. Focal Loss PL, QuiltNet.Kather ↑.Acc = 86.36. Focal Loss PL, QuiltNet.Kather ↑.ECE ↓ = 2.02. Focal Loss PL, QuiltNet.PanNuke ↓.ACC ↑ = 68.93. Focal Loss PL, QuiltNet.PanNuke ↓.ECE = 5.68. Focal Loss PL, QuiltNet.DigestPath ↑ ↓.ACC = 84.94. Focal Loss PL, QuiltNet.DigestPath ↑ ↓.ECE = 3.33. Focal Loss PL, Average.All ↑.ACC = 79.04. Focal Loss PL, Average.All ↑.ECE ↓ = 4.18. FL + MDCA FT, PLIP.Kather.ACC ↑ ECE = 80.91. FL + MDCA FT, PLIP.Kather.↓ = 12.03. FL + MDCA FT, PLIP.PanNuke.ACC = 58.59. FL + MDCA FT, PLIP.PanNuke.↑ ECE ↓ = 16.38. FL + MDCA FT, PLIP.DigestPath.ACC ↑ = 81.07. FL + MDCA FT, PLIP.DigestPath.ECE ↓ = 6.50. FL + MDCA FT, QuiltNet.Kather ↑.Acc = 79.35. FL + MDCA FT, QuiltNet.Kather ↑.ECE ↓ = 19.94. FL + MDCA FT, QuiltNet.PanNuke ↓.ACC ↑ = 73.01. FL + MDCA FT, QuiltNet.PanNuke ↓.ECE = 8.02. FL + MDCA FT, QuiltNet.DigestPath ↑ ↓.ACC = 77.50. FL + MDCA FT, QuiltNet.DigestPath ↑ ↓.ECE = 11.46. FL + MDCA FT, Average.All ↑.ACC = 75.07. FL + MDCA FT, Average.All ↑.ECE ↓ = 12.39. FL + MDCA PL, PLIP.Kather.ACC ↑ ECE = 85.96. FL + MDCA PL, PLIP.Kather.↓ = 2.60. FL + MDCA PL, PLIP.PanNuke.ACC = 63.82. FL + MDCA PL, PLIP.PanNuke.↑ ECE ↓ = 10.30. FL + MDCA PL, PLIP.DigestPath.ACC ↑ = 89.22. FL + MDCA PL, PLIP.DigestPath.ECE ↓ = 30.71. FL + MDCA PL, QuiltNet.Kather ↑.Acc = 89.81. FL + MDCA PL, QuiltNet.Kather ↑.ECE ↓ = 3.72. FL + MDCA PL, QuiltNet.PanNuke ↓.ACC ↑ = 72.89. FL + MDCA PL, QuiltNet.PanNuke ↓.ECE = 19.64. FL + MDCA PL, QuiltNet.DigestPath ↑ ↓.ACC = 90.20. FL + MDCA PL, QuiltNet.DigestPath ↑ ↓.ECE = 36.04. FL + MDCA PL, Average.All ↑.ACC = 82.00. FL + MDCA PL, Average.All ↑.ECE ↓ = 17.17. FL + SMAC FT, PLIP.Kather.ACC ↑ ECE = 81.32. FL + SMAC FT, PLIP.Kather.↓ = 12.18. FL + SMAC FT, PLIP.PanNuke.ACC = 62.72. FL + SMAC FT, PLIP.PanNuke.↑ ECE ↓ = 9.74. FL + SMAC FT, PLIP.DigestPath.ACC ↑ = 82.82. FL + SMAC FT, PLIP.DigestPath.ECE ↓ = 7.58. FL + SMAC FT, QuiltNet.Kather ↑.Acc = 79.78. FL + SMAC FT, QuiltNet.Kather ↑.ECE ↓ = 20.37. FL + SMAC FT, QuiltNet.PanNuke ↓.ACC ↑ = 72.85. FL + SMAC FT, QuiltNet.PanNuke ↓.ECE = 6.52. FL + SMAC FT, QuiltNet.DigestPath ↑ ↓.ACC = 80.12. FL + SMAC FT, QuiltNet.DigestPath ↑ ↓.ECE = 10.20. FL + SMAC FT, Average.All ↑.ACC = 76.60. FL + SMAC FT, Average.All ↑.ECE ↓ = 11.10. FL + SMAC PL, PLIP.Kather.ACC ↑ ECE = 85.99. FL + SMAC PL, PLIP.Kather.↓ = 2.59. FL + SMAC PL, PLIP.PanNuke.ACC = 63.82. FL + SMAC PL, PLIP.PanNuke.↑ ECE ↓ = 10.30. FL + SMAC PL, PLIP.DigestPath.ACC ↑ = 89.22. FL + SMAC PL, PLIP.DigestPath.ECE ↓ = 30.71. FL + SMAC PL, QuiltNet.Kather ↑.Acc = 87.05. FL + SMAC PL, QuiltNet.Kather ↑.ECE ↓ = 2.43. FL + SMAC PL, QuiltNet.PanNuke ↓.ACC ↑ = 72.89. FL + SMAC PL, QuiltNet.PanNuke ↓.ECE = 19.64. FL + SMAC PL, QuiltNet.DigestPath ↑ ↓.ACC = 90.20. FL + SMAC PL, QuiltNet.DigestPath ↑ ↓.ECE = 36.04. FL + SMAC PL, Average.All ↑.ACC = 81.53. FL + SMAC PL, Average.All ↑.ECE ↓ = 16.95. FL + ( SMAC + AS ) FT, PLIP.Kather.ACC ↑ ECE = 81.32. FL + ( SMAC + AS ) FT, PLIP.Kather.↓ = 12.18. FL + ( SMAC + AS ) FT, PLIP.PanNuke.ACC = 62.47. FL + ( SMAC + AS ) FT, PLIP.PanNuke.↑ ECE ↓ = 10.14. FL + ( SMAC + AS ) FT, PLIP.DigestPath.ACC ↑ = 82.65. FL + ( SMAC + AS ) FT, PLIP.DigestPath.ECE ↓ = 7.50. FL + ( SMAC + AS ) FT, QuiltNet.Kather ↑.Acc = 78.61. FL + ( SMAC + AS ) FT, QuiltNet.Kather ↑.ECE ↓ = 16.23. FL + ( SMAC + AS ) FT, QuiltNet.PanNuke ↓.ACC ↑ = 72.64. FL + ( SMAC + AS ) FT, QuiltNet.PanNuke ↓.ECE = 6.32. FL + ( SMAC + AS ) FT, QuiltNet.DigestPath ↑ ↓.ACC = 74.24. FL + ( SMAC + AS ) FT, QuiltNet.DigestPath ↑ ↓.ECE = 5.92. FL + ( SMAC + AS ) FT, Average.All ↑.ACC = 75.32. FL + ( SMAC + AS ) FT, Average.All ↑.ECE ↓ = 9.72. FL + ( SMAC + AS ) PL, PLIP.Kather.ACC ↑ ECE = 85.21. FL + ( SMAC + AS ) PL, PLIP.Kather.↓ = 2.58. FL + ( SMAC + AS ) PL, PLIP.PanNuke.ACC = 62.44. FL + ( SMAC + AS ) PL, PLIP.PanNuke.↑ ECE ↓ = 8.95. FL + ( SMAC + AS ) PL, PLIP.DigestPath.ACC ↑ = 89.02. FL + ( SMAC + AS ) PL, PLIP.DigestPath.ECE ↓ = 32.15. FL + ( SMAC + AS ) PL, QuiltNet.Kather ↑.Acc = 88.97. FL + ( SMAC + AS ) PL, QuiltNet.Kather ↑.ECE ↓ = 1.27. FL + ( SMAC + AS ) PL, QuiltNet.PanNuke ↓.ACC ↑ = 66.09. FL + ( SMAC + AS ) PL, QuiltNet.PanNuke ↓.ECE = 13.64. FL + ( SMAC + AS ) PL, QuiltNet.DigestPath ↑ ↓.ACC = 82.13. FL + ( SMAC + AS ) PL, QuiltNet.DigestPath ↑ ↓.ECE = 30.31. FL + ( SMAC + AS ) PL, Average.All ↑.ACC = 78.98. FL + ( SMAC + AS ) PL, Average.All ↑.ECE ↓ = 14.82HEADINGS:
# 7 Evaluation Metrics
CONTENT:
effectiveness for medical imaging tasks. We employ adaptive bandwidth selection (ranging from 0.05 to 0.3) based on dataset size, which is particularly beneficial for few-shot learning scenarios where test sets can vary significantly in size. This approach avoids binning artifacts and provides a smoother, more continuous estimate of the confidence-accuracy relationship.HEADINGS:
# 8 Focal Loss Experiments
CONTENT:
In this supplementary material, we present additional experiments using focal loss that were not included in the main paper. These experiments complement our main findings and provide further insights into calibration techniques for vision-language models.HEADINGS:
# 8.1 Experimental Setup
CONTENT:
We conducted experiments applying focal loss to our context optimization approach across various datasets. The focal loss function is defined as:
<!-- formula-not-decoded -->
where pt represents the model's predicted probability for the true class and γ is the focusing parameter that determines how much to down-weight well-classified examples.HEADINGS:
# 8.2 Results
CONTENT:
The results of our focal loss experiments are presented in the following tables:
Table 7 shows the performance on histopathology datasets, while Table 8 presents results on X-ray datasets. Table 9 provides a comprehensive analysis using various calibration metrics beyond ECE.HEADINGS:
# 8.2 Results
CONTENT:
Table 8: Comparison of our proposed calibration regularizers ( SMAC , SMAC+AS ) with baseline methods using Focal Loss (FL). Results show Accuracy (ACC, %) and Expected Calibration Error (ECE, %) for BioMedCLIP and MedCLIP on COVID and RSNA datasets. Best results are in bold , second-best are underlined.

Loss ↓, BioMedCLIP.COVID = ACC ↑. Loss ↓, BioMedCLIP. = ECE ↓. Loss ↓, BioMedCLIP.RSNA = ACC ↑. Loss ↓, BioMedCLIP. = ECE ↓. Loss ↓, MedCLIP.COVID = ACC ↑. Loss ↓, MedCLIP. = ECE ↓. Loss ↓, MedCLIP.RSNA = ACC ↑. Loss ↓, MedCLIP.RSNA = ECE ↓. Loss ↓, Average.All = ACC ↑. Loss ↓, Average. = ECE ↓. , BioMedCLIP.COVID = Focal Loss-based Losses. , BioMedCLIP. = Focal Loss-based Losses. , BioMedCLIP.RSNA = Focal Loss-based Losses. , BioMedCLIP. = Focal Loss-based Losses. , MedCLIP.COVID = Focal Loss-based Losses. , MedCLIP. = Focal Loss-based Losses. , MedCLIP.RSNA = Focal Loss-based Losses. , MedCLIP.RSNA = Focal Loss-based Losses. , Average.All = Focal Loss-based Losses. , Average. = Focal Loss-based Losses. Focal Loss PL, BioMedCLIP.COVID = 78.42. Focal Loss PL, BioMedCLIP. = 16.47. Focal Loss PL, BioMedCLIP.RSNA = 61.13. Focal Loss PL, BioMedCLIP. = 17.45. Focal Loss PL, MedCLIP.COVID = 77.55. Focal Loss PL, MedCLIP. = 27.45. Focal Loss PL, MedCLIP.RSNA = 50.66. Focal Loss PL, MedCLIP.RSNA = 17.19. Focal Loss PL, Average.All = 66.94. Focal Loss PL, Average. = 19.64. FL + MDCA PL, BioMedCLIP.COVID = 71.18. FL + MDCA PL, BioMedCLIP. = 12.17. FL + MDCA PL, BioMedCLIP.RSNA = 61.74. FL + MDCA PL, BioMedCLIP. = 18.58. FL + MDCA PL, MedCLIP.COVID = 77.49. FL + MDCA PL, MedCLIP. = 27.39. FL + MDCA PL, MedCLIP.RSNA = 50.64. FL + MDCA PL, MedCLIP.RSNA = 17.17. FL + MDCA PL, Average.All = 65.26. FL + MDCA PL, Average. = 18.83. FL + SMAC PL, BioMedCLIP.COVID = 71.18. FL + SMAC PL, BioMedCLIP. = 12.17. FL + SMAC PL, BioMedCLIP.RSNA = 61.88. FL + SMAC PL, BioMedCLIP. = 18.70. FL + SMAC PL, MedCLIP.COVID = 77.49. FL + SMAC PL, MedCLIP. = 27.39. FL + SMAC PL, MedCLIP.RSNA = 50.64. FL + SMAC PL, MedCLIP.RSNA = 17.17. FL + SMAC PL, Average.All = 65.30. FL + SMAC PL, Average. = 18.86. FL + ( SMAC + AS ) PL, BioMedCLIP.COVID = 72.86. FL + ( SMAC + AS ) PL, BioMedCLIP. = 7.14. FL + ( SMAC + AS ) PL, BioMedCLIP.RSNA = 60.86. FL + ( SMAC + AS ) PL, BioMedCLIP. = 15.20. FL + ( SMAC + AS ) PL, MedCLIP.COVID = 77.49. FL + ( SMAC + AS ) PL, MedCLIP. = 27.39. FL + ( SMAC + AS ) PL, MedCLIP.RSNA = 50.64. FL + ( SMAC + AS ) PL, MedCLIP.RSNA = 17.17. FL + ( SMAC + AS ) PL, Average.All = 65.46. FL + ( SMAC + AS ) PL, Average. = 16.73HEADINGS:
# 8.2 Results
CONTENT:
Table 9: Average calibration results of PLIP (PanNuke, DigestPath) and BioMedCLIP (COVID, RSNA) using ECE, ACE, MCE, and ECE KDE . Bold and underline denote best and second-best scores, respectively.

Focal Loss PL, PLIP.ECE ↓ ACE.Focal Loss-based Losses = 5.61. Focal Loss PL, PLIP.↓ MCE ↓.Focal Loss-based Losses = 5.53. Focal Loss PL, PLIP.KDE ↓.Focal Loss-based Losses = 2.09. Focal Loss PL, PLIP.ECE ↓.Focal Loss-based Losses = 5.83. Focal Loss PL, BioMedCLIP..Focal Loss-based Losses = 16.96. Focal Loss PL, BioMedCLIP.ACE.Focal Loss-based Losses = 16.95. Focal Loss PL, BioMedCLIP.↓ MCE ↓.Focal Loss-based Losses = 10.63. Focal Loss PL, BioMedCLIP.KDE ↓.Focal Loss-based Losses = 16.87. FL + MDCA PL, PLIP.ECE ↓ ACE.Focal Loss-based Losses = 20.51. FL + MDCA PL, PLIP.↓ MCE ↓.Focal Loss-based Losses = 20.51. FL + MDCA PL, PLIP.KDE ↓.Focal Loss-based Losses = 12.98. FL + MDCA PL, PLIP.ECE ↓.Focal Loss-based Losses = 20.65. FL + MDCA PL, BioMedCLIP..Focal Loss-based Losses = 15.38. FL + MDCA PL, BioMedCLIP.ACE.Focal Loss-based Losses = 15.38. FL + MDCA PL, BioMedCLIP.↓ MCE ↓.Focal Loss-based Losses = 9.52. FL + MDCA PL, BioMedCLIP.KDE ↓.Focal Loss-based Losses = 15.01. FL + SMAC PL, PLIP.ECE ↓ ACE.Focal Loss-based Losses = 20.51. FL + SMAC PL, PLIP.↓ MCE ↓.Focal Loss-based Losses = 20.51. FL + SMAC PL, PLIP.KDE ↓.Focal Loss-based Losses = 12.98. FL + SMAC PL, PLIP.ECE ↓.Focal Loss-based Losses = 20.65. FL + SMAC PL, BioMedCLIP..Focal Loss-based Losses = 15.44. FL + SMAC PL, BioMedCLIP.ACE.Focal Loss-based Losses = 15.43. FL + SMAC PL, BioMedCLIP.↓ MCE ↓.Focal Loss-based Losses = 9.80. FL + SMAC PL, BioMedCLIP.KDE ↓.Focal Loss-based Losses = 15.09. FL + ( SMAC + AS ) PL, PLIP.ECE ↓ ACE.Focal Loss-based Losses = 20.55. FL + ( SMAC + AS ) PL, PLIP.↓ MCE ↓.Focal Loss-based Losses = 20.77. FL + ( SMAC + AS ) PL, PLIP.KDE ↓.Focal Loss-based Losses = 15.71. FL + ( SMAC + AS ) PL, PLIP.ECE ↓.Focal Loss-based Losses = 20.60. FL + ( SMAC + AS ) PL, BioMedCLIP..Focal Loss-based Losses = 11.17. FL + ( SMAC + AS ) PL, BioMedCLIP.ACE.Focal Loss-based Losses = 11.17. FL + ( SMAC + AS ) PL, BioMedCLIP.↓ MCE ↓.Focal Loss-based Losses = 6.78. FL + ( SMAC + AS ) PL, BioMedCLIP.KDE ↓.Focal Loss-based Losses = 11.05