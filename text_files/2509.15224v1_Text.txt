HEADINGS:
# Depth AnyEvent: A Cross-Modal Distillation Paradigm for Event-Based Monocular Depth Estimation
CONTENT:
Luca Bartolomei ∗ , †
Enrico Mannocci † Fabio Tosi † Matteo Poggi ∗ , † ∗ Advanced Research Center on Electronic System (ARCES)
Stefano Mattoccia ∗ , †
† Department of Computer Science and Engineering (DISI) University of Bologna, Italy
{ luca.bartolomei5, fabio.tosi5, m.poggi, stefano.mattoccia } @unibo.it https://bartn8.github.io/depthanyevent
Figure 1. DepthAnyEvent-R in action. The first column shows the input frame (used only for distillation) and the corresponding event visualization. The other three columns present depth estimation results from different approaches: E2Depth [15], our DepthAnyEvent-R, and our DepthAnyEvent-R trained with our distillation approach. The top row shows the estimated depth maps while the bottom row depicts their corresponding RMSE visualizations.HEADINGS:
# Abstract
CONTENT:
Event cameras capture sparse, high-temporal-resolution visual information, making them particularly suitable for challenging environments with high-speed motion and strongly varying lighting conditions. However, the lack of large datasets with dense ground-truth depth annotations hinders learning-based monocular depth estimation from event data. To address this limitation, we propose a crossmodal distillation paradigm to generate dense proxy labels leveraging a Vision Foundation Model (VFM). Our strategy requires an event stream spatially aligned with RGB frames, a simple setup even available off-the-shelf, and exploits the robustness of large-scale VFMs. Additionally, we propose to adapt VFMs, either a vanilla one like Depth Anything v2 (DAv2), or deriving from it a novel recurrent architecture to infer depth from monocular event cameras. We evaluate our approach with synthetic and real-world datasets, demonstrating that i) our cross-modal paradigm achieves competitive performance compared to fully supervised methods without requiring expensive depth annotations, and ii) our
VFM-based models achieve state-of-the-art performance.HEADINGS:
# 1. Introduction
CONTENT:
Depth perception from cameras is paramount for many application fields, such as those concerning the autonomous navigation of agents in complex scenarios or robotic tasks. In these fields, learning-based methods using conventional cameras have obtained compelling results in the last decade. Moreover, this paradigm enabled inferring depth from a single camera, which brings significant advantages compared to multicamera setups in terms of cost, calibration complexity, and physical constraints. Nonetheless, conventional camera systems struggle to provide a prompt and reliable perception of the sensed environment when dealing with highly dynamic scenes resulting from the fast movement of vehicles, drones, robots or in the presence of challenging illumination conditions such as high contrast scenarios, low light, or rapid lighting changes. These limitations are intrinsic to the conventional camera acquisition technology occurring at discrete periodic intervals and with a lim-HEADINGS:
# 1. Introduction
CONTENT:
ited dynamic range, causing motion blur, over/under exposure, and potentially missing critical information between frames. In contrast, the intrinsic ability to capture scene changes as soon as they appear - with microsecond temporal resolution - and the much higher dynamic range made event cameras [7] ideal for coping with the challenging application fields mentioned above. Event cameras only register brightness changes at each pixel independently, offering exceptional temporal resolution and robustness to lighting variations. However, these features come at the cost of meager information content compared to conventional cameras. Event cameras provide meaningful cues only for a small subset of the framed image with sufficient texture to trigger events, making depth perception from these devices extremely challenging. Moreover, the lack of large datasets with dense ground truth annotations further exacerbates this inherent difficulty, as collecting precise depth ground truth for event data remains costly and technically demanding.HEADINGS:
# 1. Introduction
CONTENT:
To tackle these issues in a monocular event camera setup, we propose to leverage the effectiveness of image-based Vision Foundation Models (VFMs) for monocular depth estimation. They have demonstrated remarkable capabilities through extensive pretraining on vast image collections, enabling robust depth prediction even in challenging scenario. As the first contribution, given sequences of aligned images and events, we propose a cross-modal distillation strategy that allows us to obtain dense proxy labels from a VFM to train event-based networks. This approach effectively transfers knowledge from the data-rich image domain to the data-sparse event domain. For our purposes, an offthe-shelf device like a DAVIS Camera [29, 32] that incorporates a conventional global shutter camera and an eventbased sensor in the same pixel array would suffice to gather spatially aligned event streams and RGB frames.HEADINGS:
# 1. Introduction
CONTENT:
Additionally, as the second contribution, we propose to adapt VFMs for event-based monocular depth estimation, either using a vanilla model like Depth Anything v2 (DAv2) or a novel recurrent architecture derived from it. To prove the effectiveness of our proposals, we assess the performance with synthetic and real-world datasets, showing that our cross-modal distillation paradigm allows for achieving competitive performance compared to fully supervised approaches, disregarding the need for expensive depth annotation. Moreover, adapting VFMs for monocular depth estimation according to our two proposals is state-of-the-art, setting new benchmarks for event-based depth estimation.
Figure 1 shows the compelling performance of our proposals, and our contributions can be summarized as follows:HEADINGS:
# 1. Introduction
CONTENT:
- A novel cross-modal distillation paradigm that leverages the robust proxy labels obtained from image-based VFMs for monocular depth estimation.
- An adapting strategy to cast existing image-based VFMs into the event domain effortlessly.
- A novel recurrent architecture based on an adapted image-based VFM.
- Adapting VFMs to the event domain yields state-of-theart performance, and our distillation paradigm is competitive against the supervision from depth sensors.HEADINGS:
# 2. Related Work
CONTENT:
Image-Based Monocular Depth Estimation. Monocular depth estimation has evolved from traditional approaches [27] to deep learning methods [6, 18]. Self-supervised techniques[12, 13, 38] have emerged to address this challenge of limited ground truth data by recasting depth estimation as an image reconstruction task using stereo images or videos. These approaches have been particularly valuable where dense depth annotations are expensive to obtain. A significant step came with affine-invariant models [25, 26] that estimate depth up to an unknown scale and shift, allowing impressive cross-domain generalization capabilities. MiDaS [26] pioneered this direction by training on diverse large-scale datasets, followed by DPT [25] and more recently, the Depth Anything series [33, 34]. These latter models represent the first generation of Visual Foundation Models for monocular depth estimation, leveraging large-scale pretraining and diverse data sources to achieve unprecedented robustness. The effectiveness of these models lies in their ability to combine knowledge from various domains, including internet photo collections [20, 35], LiDAR from autonomous driving scenarios [10], and RGBD sensors [23]. Recent advances in VFMs have focused on improving metric accuracy through camera parameter integration [14, 36], leveraging generative approaches like diffusion models [5, 17, 28], and addressing temporal consistency[30]. Furthermore, attention-based architectures and transformer models [37] have shown significant improvements in capturing long-range dependencies crucial for accurate depth. Despite recent advances, applying these methods to event-based cameras is still limited by the lack of large-scale annotated datasets. We tackle this by distilling knowledge from frame-based VFMs, enabling accurate depth estimation without costly event data annotations.HEADINGS:
# 2. Related Work
CONTENT:
Event-based Monocular Depth Estimation. Eventbased depth estimation began with supervised approach using recurrent architectures [8, 15, 21] designed to process the temporal information contained in event streams. Advanced models like [8] further expanded this concept by fusing event and RGB data to exploit their complementary charactetistics. Multimodal fusion techniques have also been explored, combining events with LiDAR to generate dense depth maps [3]. To address the scarcity of labeled event data, self-supervised methods have emerged as promising alternatives. Zhu et al. [40] developed a framework that jointly estimates depth, optical flow, and camera poses using stereo consistency and motion blur minimiza-
tion as training signals. Subsequent work [41] eliminated the need for stereo setups by leveraging pose information from consecutive RGB frames aligned with the event camera, enabling dense depth estimation. Despite these advances, event-based depth estimation still falls short compared to frame-based methods.HEADINGS:
# 3. Preliminaries: Event Depth Estimation
CONTENT:
Event cameras measure the logarithmic change in brightness over time, and when it changes over a threshold ± C , the associate pixel at position ( x k , y k ) emits at time t k an asynchronous signal e k = ( x k , y k , p k , t k ) called event . Depending on the sign of this change, the event will have polarity p k ∈ {-1 , 1 } . Each pixel of the W × H sensor grid of the event camera can independently emit events at any time, producing an asynchronous stream of events E = { e k } N k =1 , where N is the total number of fired events.HEADINGS:
# 3. Preliminaries: Event Depth Estimation
CONTENT:
Given the event history E , previous event-based dense monocular depth estimation models [8, 15, 21] convert the flow of events into a E ∈ R W × H × C structured representation - such as Voxel Grids [40] - since the sparse structure of E is not suitable for standard CNNs. Intentionally, to estimate a depth map D ∈ R W × H at a given timestamp t d , events are retrospectively sampled from the stream E , either within a fixed time window (SBT) -i.e ., E ∆ T t d = { e k ∈ E | t d -∆ T ≤ t k ≤ t d } - or up to a predefined number K of events (SBN) i.e ., E t K d = { e k ∈ E | d -K ≤ k ≤ d } - and subsequently stacked using different strategies, including:
Voxel Grid [40]: The time interval used for sampling events is divided into B uniform bins, where event polarities are accumulated using linear interpolation within each bin of a E ∈ R W × H × B stack.HEADINGS:
# 3. Preliminaries: Event Depth Estimation
CONTENT:
Image-like [21]: A color-based representation where the R and B channels encode positive and negative polarities, respectively, resulting in an RGB image, i.e . a E ∈ R W × H × 3 stack. Unlike the Voxel Grid representation, it does not retain temporal information.
Tencode [16]. A color image representation in which R and B channels encode positive and negative polarities, with Gencoding the timestamp relative to the total time-lapse. It produces an RGB image, i.e . a E ∈ R W × H × 3 stack.
For the sake of space, we report only the event representations relevant to our work, but additional details regarding event representations can be found in [1, 11].HEADINGS:
# 4. Proposed Method
CONTENT:
Our first goal is to leverage the knowledge of frame-based monocular depth models like DAv2 extracting pseudo labels to train any event-based student depth model e.g ., E2Depth - given aligned intensity frames and event stacks. Figure 2 outlines our cross-modal distillation paradigm.
Figure 2. Proposed Cross-Modal Distillation Strategy. During training, a VFM teacher processes RGB input frames I to generate proxy depth labels D ∗ , which supervise an event-based student model. The student takes aligned event stacks E as input and predicts the final depth map D .
Moreover, we propose to cast a frame-based model - DAv2 in our experiments - either in its original version or enriching it to exploit temporal cues, to the event domain taking advantage of the massive pre-train performed in the image domain.HEADINGS:
# 4.1. VFMs for Cross-Modal Distillation
CONTENT:
Visual Foundation Models have achieved astonishing results mainly due to their peculiar large-scale training procedures. For instance, DAv2 relies on a DINOv2 backbone that was pre-trained with hundreds of millions of images in an unsupervised manner. Furthermore, DAv2 uses tens of millions of pseudo-labeled and millions of labeled images for training. Unfortunately, event data lacks equivalent large-scale datasets [2, 9, 39], substantially precluding comparable training in the event domain. To bridge this gap, we propose leveraging a pre-trained VFM - DAv2 ViT-Large in our experiments- to provide dense supervision for any event-based depth estimation networks, as outlined in Figure 2. During training, a teacher VFM processes a frame, producing the proxy label D ∗ (Fig. 3 shows an example) and the student model predicts a depth map D from the spatially and temporally aligned events. The student model is supervised using a loss L = L si + λ L reg composed of a scale-invariant loss L si and a gradient regularization term L reg [19]:HEADINGS:
# 4.1. VFMs for Cross-Modal Distillation
CONTENT:
<!-- formula-not-decoded -->
where M is the set of valid pixels, ˆ D = s D + t and ˆ D ∗ = D ∗ are respectively the scaled and shifted versions of the student prediction D and the proxy label D ∗ , and ( s, t ) are the scaling factors obtained using the least-square approach:
<!-- formula-not-decoded -->
Figure 3. Labels Distillation from Frame-Based Vision Foundation Model. Given the availability of aligned color and event modalities, e.g., collected by a DAVIS346B sensor, we can exploit a VFM to extract proxy labels from the color images, resulting in much dense supervision compared to the one provided by semi-dense LiDAR annotations.HEADINGS:
# 4.1. VFMs for Cross-Modal Distillation
CONTENT:
Figure 4. Proposed Recurrent VFM. Our DepthAnyEvent-R model processes image patches with positional encoding through multiple transformer stages that produce multi-scale feature maps F s . These features are combined with hidden states H i s in ConvLSTM modules R s to incorporate temporal information from previous event stacks, generating enhanced feature maps ˆ F s and updated hidden states H i +1 s . A hierarchical fusion process integrates features from different scales to predict the final depth prediction ˆ F ∗ .
The regularization term L reg is defined as follows:
<!-- formula-not-decoded -->
where R k = ˆ D k -ˆ D ∗ k is the difference of maps at scale k and M k is the set of valid pixels at scale k .
To ensure alignment, frame and event cameras must be calibrated - intrinsically done in the DA VIS camera - and events are sliced from the frame's acquiring timestamp.HEADINGS:
# 4.2. Casting VFMs to the Event Domain
CONTENT:
Frame-based monocular depth models cannot be used directly on events, given the diverse nature of the latter. Hence, to adapt their capabilities to the event domain, we choose an appropriate event representation that can reduce the gap between frames and events encoding. Furthermore, we exploit the sequential nature of temporal events, proposing a novel recurrent architecture of DAv2.
Choosing the Right Event Representation. The events stream contains spatial and temporal information; hence, a good event representation should capture both to ensure limited loss of information. Since monocular models naturally process RGB frames i.e ., they produce a depth map given an image I ∈ R W × H × 3 as input - we have to choose an event representation that encodes both spatial and temporal requirements within an RGB frame to pursue minimal modifications of the pre-trained VFM.
Purposely, the Tencode [16] representation fits with our aim. Consequently, starting from a sliced event history E t d , either using SBT or SBN [22], Tencode encodes E t d into a stack E as follows:
<!-- formula-not-decoded -->HEADINGS:
# 4.2. Casting VFMs to the Event Domain
CONTENT:
where e k = ( x k , y k , p k , t k ) ∈ E t d is the k -th event of E t d and ∆ T is the time interval of event slice E t d .
VFM for Events. Although the Tencode representation significantly differs from a conventional RGB image of the same scene, we propose to adapt a pre-trained VFM to deal with the event domain through fine-tuning with event data using the Tencode representation. For this purpose, we use as the VFM a vanilla DAv2 ViT-S for our experiments. We dubbed the model as DepthAnyEvent .
Recurrent VFM for Events. Additionally, given the sequence nature of the event stream, Recurrent Neural Networks (RNNs) could encode previous features extracted from past event stacks into a hidden state [15, 21]. At each iteration, the recurrent module can update the hidden stateHEADINGS:
# 4.2. Casting VFMs to the Event Domain
CONTENT:
Table 1. Quantitative Results - Zero-Shot Generalization on MVSEC and DSEC. All networks are trained on the EventScape synthetic dataset only, and tested without any fine-tuning.

E2Depth [15], Dataset = . E2Depth [15], Abs Rel ↓ = 0.527. E2Depth [15], Sq Rel ↓ = 1.122. E2Depth [15], RMSE ↓ = 7.894. E2Depth [15], RMSE log ↓ = 0.512. E2Depth [15], SI log ↓ = 0.244. E2Depth [15], δ < 1 . 25 ↑ = 0.363. E2Depth [15], δ < 1 . 25 2 ↑ = 0.637. E2Depth [15], δ < 1 . 25 3 ↑ = 0.811. EReFormer [21], Dataset = MVSEC. EReFormer [21], Abs Rel ↓ = 0.518. EReFormer [21], Sq Rel ↓ = 1.012. EReFormer [21], RMSE ↓ = 8.423. EReFormer [21], RMSE log ↓ = 0.559. EReFormer [21], SI log ↓ = 0.316. EReFormer [21], δ < 1 . 25 ↑ = 0.361. EReFormer [21], δ < 1 . 25 2 ↑ = 0.630. EReFormer [21], δ < 1 . 25 3 ↑ = 0.800. DepthAnyEvent, Dataset = . DepthAnyEvent, Abs Rel ↓ = 0.466. DepthAnyEvent, Sq Rel ↓ = 0.976. DepthAnyEvent, RMSE ↓ = 7.824. DepthAnyEvent, RMSE log ↓ = 0.480. DepthAnyEvent, SI log ↓ = 0.229. DepthAnyEvent, δ < 1 . 25 ↑ = 0.408. DepthAnyEvent, δ < 1 . 25 2 ↑ = 0.689. DepthAnyEvent, δ < 1 . 25 3 ↑ = 0.847. DepthAnyEvent-R, Dataset = . DepthAnyEvent-R, Abs Rel ↓ = 0.469. DepthAnyEvent-R, Sq Rel ↓ = 0.946. DepthAnyEvent-R, RMSE ↓ = 8.064. DepthAnyEvent-R, RMSE log ↓ = 0.508. DepthAnyEvent-R, SI log ↓ = 0.272. DepthAnyEvent-R, δ < 1 . 25 ↑ = 0.428. DepthAnyEvent-R, δ < 1 . 25 2 ↑ = 0.690. DepthAnyEvent-R, δ < 1 . 25 3 ↑ = 0.832. E2Depth [15], Dataset = DSEC. E2Depth [15], Abs Rel ↓ = 0.395. E2Depth [15], Sq Rel ↓ = 0.334. E2Depth [15], RMSE ↓ = 13.258. E2Depth [15], RMSE log ↓ = 0.412. E2Depth [15], SI log ↓ = 0.167. E2Depth [15], δ < 1 . 25 ↑ = 0.409. E2Depth [15], δ < 1 . 25 2 ↑ = 0.719. E2Depth [15], δ < 1 . 25 3 ↑ = 0.891. EReFormer [21], Dataset = DSEC. EReFormer [21], Abs Rel ↓ = 0.297. EReFormer [21], Sq Rel ↓ = 0.195. EReFormer [21], RMSE ↓ = 11.608. EReFormer [21], RMSE log ↓ = 0.334. EReFormer [21], SI log ↓ = 0.113. EReFormer [21], δ < 1 . 25 ↑ = 0.524. EReFormer [21], δ < 1 . 25 2 ↑ = 0.824. EReFormer [21], δ < 1 . 25 3 ↑ = 0.945. DepthAnyEvent, Dataset = DSEC. DepthAnyEvent, Abs Rel ↓ = 0.297. DepthAnyEvent, Sq Rel ↓ = 0.186. DepthAnyEvent, RMSE ↓ = 11.072. DepthAnyEvent, RMSE log ↓ = 0.330. DepthAnyEvent, SI log ↓ = 0.108. DepthAnyEvent, δ < 1 . 25 ↑ = 0.519. DepthAnyEvent, δ < 1 . 25 2 ↑ = 0.827. DepthAnyEvent, δ < 1 . 25 3 ↑ = 0.948. DepthAnyEvent-R, Dataset = DSEC. DepthAnyEvent-R, Abs Rel ↓ = 0.276. DepthAnyEvent-R, Sq Rel ↓ = 0.165. DepthAnyEvent-R, RMSE ↓ = 10.942. DepthAnyEvent-R, RMSE log ↓ = 0.314. DepthAnyEvent-R, SI log ↓ = 0.101. DepthAnyEvent-R, δ < 1 . 25 ↑ = 0.555. DepthAnyEvent-R, δ < 1 . 25 2 ↑ = 0.843. DepthAnyEvent-R, δ < 1 . 25 3 ↑ = 0.954HEADINGS:
# 4.2. Casting VFMs to the Event Domain
CONTENT:
Figure 5. Qualitative Results on DSEC dataset - Zero-Shot Generalization. From left to right: event image, predictions by E2Depth, EReFormer, DepthAnyEvent and DepthAnyEvent-R, trained on EventScape only.
with the features extracted from the current stack, generating a new hidden state for the next iteration.HEADINGS:
# 4.2. Casting VFMs to the Event Domain
CONTENT:
However, monocular depth models typically lack a recurrent module since they are designed to work with singleframe instances. Hence, for our purposes, this could hinder the quality of predictions, especially during static scenes where events are not triggered. To effectively adapt them to the event domain, we introduce a recurrent extension of DAv2 ViT-Small, dubbed as DepthAnyEvent-R , that integrates cues from previous event stacks, as outlined in Figure 4. The DAv2 architecture is composed of two main modules: a DINOv2 [24] Encoder G based on Visual Transformer (ViT), and a Dense Depth Decoder D . Given an image I encoded with the Tencode representation, the encoder G first splits the image into patches and adds positional encoding to them. Next, patches are passed through multiple transformer stages and then reassembled from different stages into multi-scale feature maps F s ∈ R W s × H s × C s . For each scale s , we feed the feature maps F s and the hidden state H i s ∈ R W s × H s × C s with H 0 s = 0 to a ConvLSTM [31] module R s obtaining a new hidden state H i +1 s and temporally enhanced feature maps ˆ F s . Starting from the lowest scale, a series of fusion modules sequentially upsample and fuse the feature maps to obtain the final feature map ˆ F ∗ fed to the decoder D to obtain the final predicted depth map.HEADINGS:
# 5. Experiments
CONTENT:
We describe our implementation details, datasets, and evaluation protocols, followed by experiments.HEADINGS:
# 5.1. Implementation and Experimental Settings
CONTENT:
Hyperparameters Settings. We set the slicing window ∆ T , the number of Voxel Grid bins B , and the loss factor λ respectively to 50ms, 5, and 0.25. We implement eventbased student networks E2Depth [15] and EReFormer [21] starting from their codebase. For DepthAnyEvent and DepthAnyEvent-R, we start from the DAv2 ViT-Small codebase [34]. We use PyTorch, and a single A100 GPU with 64GB of RAM. Following the original papers, we fix the learning rate to 10 -4 and 3 . 2 · 10 -5 respectively for E2Depth and EReFormer, while we set a learning rate of 5 · 10 -6 for all DepthAnyEvent variants. We adjust the training steps to 75k, using the AdamW optimizer with the OneCycle scheduler, and apply data augmentations including horizontal flips and random crops at 224 × 224 . We set the batch size to 10, except for EReFormer: given the higher memory requirements, we change it to 2. We unroll all recurrent networks i.e ., E2Depth, EReFormer, and DepthAnyEvent-R - for 20 steps. We choose as the event representation Tencode [16] for DepthAnyEvent and DepthAnyEvent-R, while we maintained the original representation for E2Depth and EReFormer i.e ., respectively, Voxel Grid [40] and Image-like [21]. Finally, we use the scale-invariant L for all networks. The settings reported are used for all experiments unless otherwise specified.HEADINGS:
# 5.1. Implementation and Experimental Settings
CONTENT:
Proxy Labels Factory. We generate proxy labels from frames using the DAv2 ViT-Large trained for metric depth estimation: starting from the Large vanilla weights provided by the authors, we perform a fine-tuning on EventScape [8] for 10k steps with a learning rate of 10 -6 .
Synthetic Training Setup. We obtain the syntheticHEADINGS:
# 5.1. Implementation and Experimental Settings
CONTENT:
Table 2. Quantitative Results - In-Domain Evaluation on MVSEC and DSEC. All networks are trained on the EventScape synthetic dataset and then further fine-tuned on MVSEC and DSEC datasets separately.

E2Depth [15], Dataset = . E2Depth [15], Abs Rel ↓ = 0.420. E2Depth [15], Sq Rel ↓ = 0.806. E2Depth [15], RMSE ↓ = 7.268. E2Depth [15], RMSE log ↓ = 0.455. E2Depth [15], SI log ↓ = 0.213. E2Depth [15], δ < 1 . 25 ↑ = 0.432. E2Depth [15], δ < 1 . 25 2 ↑ = 0.717. E2Depth [15], δ < 1 . 25 3 ↑ = 0.868. EReFormer [21], Dataset = MVSEC. EReFormer [21], Abs Rel ↓ = 0.511. EReFormer [21], Sq Rel ↓ = 1.057. EReFormer [21], RMSE ↓ = 8.373. EReFormer [21], RMSE log ↓ = 0.523. EReFormer [21], SI log ↓ = 0.274. EReFormer [21], δ < 1 . 25 ↑ = 0.391. EReFormer [21], δ < 1 . 25 2 ↑ = 0.652. EReFormer [21], δ < 1 . 25 3 ↑ = 0.810. DepthAnyEvent, Dataset = . DepthAnyEvent, Abs Rel ↓ = 0.373. DepthAnyEvent, Sq Rel ↓ = 0.715. DepthAnyEvent, RMSE ↓ = 6.627. DepthAnyEvent, RMSE log ↓ = 0.449. DepthAnyEvent, SI log ↓ = 0.222. DepthAnyEvent, δ < 1 . 25 ↑ = 0.471. DepthAnyEvent, δ < 1 . 25 2 ↑ = 0.747. DepthAnyEvent, δ < 1 . 25 3 ↑ = 0.884. DepthAnyEvent-R, Dataset = . DepthAnyEvent-R, Abs Rel ↓ = 0.365. DepthAnyEvent-R, Sq Rel ↓ = 0.691. DepthAnyEvent-R, RMSE ↓ = 6.465. DepthAnyEvent-R, RMSE log ↓ = 0.483. DepthAnyEvent-R, SI log ↓ = 0.258. DepthAnyEvent-R, δ < 1 . 25 ↑ = 0.489. DepthAnyEvent-R, δ < 1 . 25 2 ↑ = 0.751. DepthAnyEvent-R, δ < 1 . 25 3 ↑ = 0.878. E2Depth [15], Dataset = . E2Depth [15], Abs Rel ↓ = 0.253. E2Depth [15], Sq Rel ↓ = 0.130. E2Depth [15], RMSE ↓ = 10.119. E2Depth [15], RMSE log ↓ = 0.315. E2Depth [15], SI log ↓ = 0.107. E2Depth [15], δ < 1 . 25 ↑ = 0.574. E2Depth [15], δ < 1 . 25 2 ↑ = 0.861. E2Depth [15], δ < 1 . 25 3 ↑ = 0.956. EReFormer [21], Dataset = DSEC. EReFormer [21], Abs Rel ↓ = 0.286. EReFormer [21], Sq Rel ↓ = 0.208. EReFormer [21], RMSE ↓ = 11.369. EReFormer [21], RMSE log ↓ = 0.325. EReFormer [21], SI log ↓ = 0.109. EReFormer [21], δ < 1 . 25 ↑ = 0.569. EReFormer [21], δ < 1 . 25 2 ↑ = 0.839. EReFormer [21], δ < 1 . 25 3 ↑ = 0.944. DepthAnyEvent, Dataset = . DepthAnyEvent, Abs Rel ↓ = 0.201. DepthAnyEvent, Sq Rel ↓ = 0.079. DepthAnyEvent, RMSE ↓ = 8.880. DepthAnyEvent, RMSE log ↓ = 0.266. DepthAnyEvent, SI log ↓ = 0.077. DepthAnyEvent, δ < 1 . 25 ↑ = 0.664. DepthAnyEvent, δ < 1 . 25 2 ↑ = 0.917. DepthAnyEvent, δ < 1 . 25 3 ↑ = 0.975. DepthAnyEvent-R, Dataset = . DepthAnyEvent-R, Abs Rel ↓ = 0.191. DepthAnyEvent-R, Sq Rel ↓ = 0.070. DepthAnyEvent-R, RMSE ↓ = 8.618. DepthAnyEvent-R, RMSE log ↓ = 0.244. DepthAnyEvent-R, SI log ↓ = 0.064. DepthAnyEvent-R, δ < 1 . 25 ↑ = 0.691. DepthAnyEvent-R, δ < 1 . 25 2 ↑ = 0.930. DepthAnyEvent-R, δ < 1 . 25 3 ↑ = 0.981HEADINGS:
# 5.1. Implementation and Experimental Settings
CONTENT:
DepthAnyEvent DepthAnyEvent (Distillation) DepthAnyEvent-R DepthAnyEvent-R (Distillation) Figure 6. Qualitative Results on MVSEC - Fine-tuned Models. From left to right: event image, predictions by E2Depth, EReFormer, DepthAnyEvent and DepthAnyEvent-R, trained on EventScape and fine-tuned on MVSEC.
checkpoints for all networks training on the synthetic EventScape [8] dataset. While E2Depth was trained from scratch, we followed EReFormer's original paper and set Swin-T pre-trained on ImageNet as the backbone. For DepthAnyEvent and DepthAnyEvent-R, we started from the Small weights provided by the authors.
Fine-tuning Setup. We follow [15], fine-tuning the models to the target domain using both real and synthetic data i.e ., MVSEC [39] + EventScape [8], and DSEC[9] + EventScape [8] - starting from the synthetic checkpoints obtained in the previous point.HEADINGS:
# 5.1. Implementation and Experimental Settings
CONTENT:
Distillation Training Setup. We use the proxy labels previously generated with DAv2 ViT-L instead of the original sparse ground-truth. Differently from the previous point, we trained the models on the dense proxy labels only instead of a synthetic+proxy mixture.HEADINGS:
# 5.2. Evaluation Datasets & Protocol
CONTENT:
Datasets. We utilize EventScape [8] as the synthetic training set, comprising about 120k groundtruth depth maps at resolution of 512 × 256 , captured from CARLA [4] simulator. For evaluation and domain fine-tunings we used two main benchmarks: MVSEC [39] and DSEC [9]. The dataset provides events at a resolution of 346 × 260 pixels from a stereo event camera consisting of two DAVIS346B sensors, which also capture spatially aligned images. ground-truth is obtained by processing data from a 16-line LiDAR using Lidar Odometry and Mapping (LOAM), yielding a total of 10k training samples and 20k testing samples. The test set is divided into a 5k-sample daytime subset and three nighttime subsets, each containing 5k samples. DSEC [9] employs two 640 × 480 Prophesee Gen3.1 event cameras in a stereo configuration. Ground-truth disparity is obtained using a 32-line LiDAR, processed with a Lidar Inertial Odometry algorithm, and further filtered to remove outliers. We convert the disparity ground-truth to depth based on the stereo setup parameters. Unlike MVSEC, RGB frames are captured using a pair of FLIR Blackfly S cameras. To align frames and events, we warp the RGB frames using the calibration parameters. We also apply a 640 × 320 center crop to mitigate misalignment artifacts in nearby objects. The dataset counts 26k training samples, divided as in [1] into 19k for training and 7k for testing.HEADINGS:
# 5.2. Evaluation Datasets & Protocol
CONTENT:
Evaluation Metrics. We evaluate the networks using different metrics: absolute relative error (Abs Rel), square Abs Rel (Sq Rel), root mean squared error (RMSE), logarithmic RMSE (RMSE log), logarithmic scale invariant error (SI log), and accuracy with different thresholds ( δ < 1 . 25 , δ < 1 . 25 2 , and δ < 1 . 25 3 ). We apply scale and shift to align predictions with the ground-truth before computing the metrics. We highlight using bold and underline the best and second best scores.HEADINGS:
# 5.2. Evaluation Datasets & Protocol
CONTENT:
Table 3. Quantitative Results - Supervised vs Distilled Models on MVSEC and DSEC. All networks are trained on the EventScape synthetic dataset and then fine-tuned on MVSEC and DSEC datasets separately, either through distillation or on ground-truth depth labels.

E2Depth Synth, Dataset = MVSEC. E2Depth Synth, Abs Rel ↓ = 0.527. E2Depth Synth, Sq Rel ↓ = 1.122. E2Depth Synth, RMSE ↓ = 7.894. E2Depth Synth, RMSE log ↓ = 0.512. E2Depth Synth, SI log ↓ = 0.244. E2Depth Synth, δ < 1 . 25 ↑ = 0.363. E2Depth Synth, δ < 1 . 25 2 ↑ = 0.637. E2Depth Synth, δ < 1 . 25 3 ↑ = 0.811. E2Depth Distilled, Dataset = . E2Depth Distilled, Abs Rel ↓ = 0.400. E2Depth Distilled, Sq Rel ↓ = 0.817. E2Depth Distilled, RMSE ↓ = 6.786. E2Depth Distilled, RMSE log ↓ = 0.538. E2Depth Distilled, SI log ↓ = 0.304. E2Depth Distilled, δ < 1 . 25 ↑ = 0.479. E2Depth Distilled, δ < 1 . 25 2 ↑ = 0.740. E2Depth Distilled, δ < 1 . 25 3 ↑ = 0.865. E2Depth Supervised, Dataset = . E2Depth Supervised, Abs Rel ↓ = 0.420. E2Depth Supervised, Sq Rel ↓ = 0.806. E2Depth Supervised, RMSE ↓ = 7.268. E2Depth Supervised, RMSE log ↓ = 0.455. E2Depth Supervised, SI log ↓ = 0.213. E2Depth Supervised, δ < 1 . 25 ↑ = 0.432. E2Depth Supervised, δ < 1 . 25 2 ↑ = 0.717. E2Depth Supervised, δ < 1 . 25 3 ↑ = 0.868. EReFormer Synth, Dataset = MVSEC. EReFormer Synth, Abs Rel ↓ = 0.518. EReFormer Synth, Sq Rel ↓ = 1.012. EReFormer Synth, RMSE ↓ = 8.423. EReFormer Synth, RMSE log ↓ = 0.559. EReFormer Synth, SI log ↓ = 0.316. EReFormer Synth, δ < 1 . 25 ↑ = 0.361. EReFormer Synth, δ < 1 . 25 2 ↑ = 0.630. EReFormer Synth, δ < 1 . 25 3 ↑ = 0.800. EReFormer Distilled, Dataset = . EReFormer Distilled, Abs Rel ↓ = 0.448. EReFormer Distilled, Sq Rel ↓ = 0.817. EReFormer Distilled, RMSE ↓ = 7.867. EReFormer Distilled, RMSE log ↓ = 0.498. EReFormer Distilled, SI log ↓ = 0.253. EReFormer Distilled, δ < 1 . 25 ↑ = 0.434. EReFormer Distilled, δ < 1 . 25 2 ↑ = 0.700. EReFormer Distilled, δ < 1 . 25 3 ↑ = 0.842. EReFormer Supervised, Dataset = . EReFormer Supervised, Abs Rel ↓ = 0.511. EReFormer Supervised, Sq Rel ↓ = 1.057. EReFormer Supervised, RMSE ↓ = 8.373. EReFormer Supervised, RMSE log ↓ = 0.523. EReFormer Supervised, SI log ↓ = 0.274. EReFormer Supervised, δ < 1 . 25 ↑ = 0.391. EReFormer Supervised, δ < 1 . 25 2 ↑ = 0.652. EReFormer Supervised, δ < 1 . 25 3 ↑ = 0.810. DepthAnyEvent Synth, Dataset = MVSEC. DepthAnyEvent Synth, Abs Rel ↓ = 0.466. DepthAnyEvent Synth, Sq Rel ↓ = 0.976. DepthAnyEvent Synth, RMSE ↓ = 7.824. DepthAnyEvent Synth, RMSE log ↓ = 0.480. DepthAnyEvent Synth, SI log ↓ = 0.229. DepthAnyEvent Synth, δ < 1 . 25 ↑ = 0.408. DepthAnyEvent Synth, δ < 1 . 25 2 ↑ = 0.689. DepthAnyEvent Synth, δ < 1 . 25 3 ↑ = 0.847. DepthAnyEvent Distilled, Dataset = . DepthAnyEvent Distilled, Abs Rel ↓ = 0.397. DepthAnyEvent Distilled, Sq Rel ↓ = 0.771. DepthAnyEvent Distilled, RMSE ↓ = 6.910. DepthAnyEvent Distilled, RMSE log ↓ = 0.495. DepthAnyEvent Distilled, SI log ↓ = 0.260. DepthAnyEvent Distilled, δ < 1 . 25 ↑ = 0.461. DepthAnyEvent Distilled, δ < 1 . 25 2 ↑ = 0.735. DepthAnyEvent Distilled, δ < 1 . 25 3 ↑ = 0.870. DepthAnyEvent Supervised, Dataset = . DepthAnyEvent Supervised, Abs Rel ↓ = 0.373. DepthAnyEvent Supervised, Sq Rel ↓ = 0.715. DepthAnyEvent Supervised, RMSE ↓ = 6.627. DepthAnyEvent Supervised, RMSE log ↓ = 0.449. DepthAnyEvent Supervised, SI log ↓ = 0.222. DepthAnyEvent Supervised, δ < 1 . 25 ↑ = 0.471. DepthAnyEvent Supervised, δ < 1 . 25 2 ↑ = 0.747. DepthAnyEvent Supervised, δ < 1 . 25 3 ↑ = 0.884. DepthAnyEvent-R Synth, Dataset = MVSEC. DepthAnyEvent-R Synth, Abs Rel ↓ = 0.469. DepthAnyEvent-R Synth, Sq Rel ↓ = 0.946. DepthAnyEvent-R Synth, RMSE ↓ = 8.064. DepthAnyEvent-R Synth, RMSE log ↓ = 0.508. DepthAnyEvent-R Synth, SI log ↓ = 0.272. DepthAnyEvent-R Synth, δ < 1 . 25 ↑ = 0.428. DepthAnyEvent-R Synth, δ < 1 . 25 2 ↑ = 0.690. DepthAnyEvent-R Synth, δ < 1 . 25 3 ↑ = 0.832. DepthAnyEvent-R Distilled, Dataset = . DepthAnyEvent-R Distilled, Abs Rel ↓ = 0.399. DepthAnyEvent-R Distilled, Sq Rel ↓ = 0.781. DepthAnyEvent-R Distilled, RMSE ↓ = 6.830. DepthAnyEvent-R Distilled, RMSE log ↓ = 0.509. DepthAnyEvent-R Distilled, SI log ↓ = 0.281. DepthAnyEvent-R Distilled, δ < 1 . 25 ↑ = 0.462. DepthAnyEvent-R Distilled, δ < 1 . 25 2 ↑ = 0.735. DepthAnyEvent-R Distilled, δ < 1 . 25 3 ↑ = 0.866. DepthAnyEvent-R Supervised, Dataset = . DepthAnyEvent-R Supervised, Abs Rel ↓ = 0.365. DepthAnyEvent-R Supervised, Sq Rel ↓ = 0.691. DepthAnyEvent-R Supervised, RMSE ↓ = 6.465. DepthAnyEvent-R Supervised, RMSE log ↓ = 0.483. DepthAnyEvent-R Supervised, SI log ↓ = 0.258. DepthAnyEvent-R Supervised, δ < 1 . 25 ↑ = 0.489. DepthAnyEvent-R Supervised, δ < 1 . 25 2 ↑ = 0.751. DepthAnyEvent-R Supervised, δ < 1 . 25 3 ↑ = 0.878. E2Depth Synth, Dataset = DSEC. E2Depth Synth, Abs Rel ↓ = 0.395. E2Depth Synth, Sq Rel ↓ = 0.334. E2Depth Synth, RMSE ↓ = 13.258. E2Depth Synth, RMSE log ↓ = 0.412. E2Depth Synth, SI log ↓ = 0.167. E2Depth Synth, δ < 1 . 25 ↑ = 0.409. E2Depth Synth, δ < 1 . 25 2 ↑ = 0.719. E2Depth Synth, δ < 1 . 25 3 ↑ = 0.891. E2Depth Distilled, Dataset = . E2Depth Distilled, Abs Rel ↓ = 0.272. E2Depth Distilled, Sq Rel ↓ = 0.153. E2Depth Distilled, RMSE ↓ = 10.579. E2Depth Distilled, RMSE log ↓ = 0.309. E2Depth Distilled, SI log ↓ = 0.096. E2Depth Distilled, δ < 1 . 25 ↑ = 0.551. E2Depth Distilled, δ < 1 . 25 2 ↑ = 0.851. E2Depth Distilled, δ < 1 . 25 3 ↑ = 0.959. E2Depth Supervised, Dataset = . E2Depth Supervised, Abs Rel ↓ = 0.253. E2Depth Supervised, Sq Rel ↓ = 0.130. E2Depth Supervised, RMSE ↓ = 10.119. E2Depth Supervised, RMSE log ↓ = 0.315. E2Depth Supervised, SI log ↓ = 0.107. E2Depth Supervised, δ < 1 . 25 ↑ = 0.574. E2Depth Supervised, δ < 1 . 25 2 ↑ = 0.861. E2Depth Supervised, δ < 1 . 25 3 ↑ = 0.956. EReFormer Synth, Dataset = DSEC. EReFormer Synth, Abs Rel ↓ = 0.297. EReFormer Synth, Sq Rel ↓ = 0.195. EReFormer Synth, RMSE ↓ = 11.608. EReFormer Synth, RMSE log ↓ = 0.334. EReFormer Synth, SI log ↓ = 0.113. EReFormer Synth, δ < 1 . 25 ↑ = 0.524. EReFormer Synth, δ < 1 . 25 2 ↑ = 0.824. EReFormer Synth, δ < 1 . 25 3 ↑ = 0.945. EReFormer Distilled, Dataset = . EReFormer Distilled, Abs Rel ↓ = 0.285. EReFormer Distilled, Sq Rel ↓ = 0.198. EReFormer Distilled, RMSE ↓ = 11.407. EReFormer Distilled, RMSE log ↓ = 0.327. EReFormer Distilled, SI log ↓ = 0.111. EReFormer Distilled, δ < 1 . 25 ↑ = 0.563. EReFormer Distilled, δ < 1 . 25 2 ↑ = 0.839. EReFormer Distilled, δ < 1 . 25 3 ↑ = 0.944. EReFormer Supervised, Dataset = . EReFormer Supervised, Abs Rel ↓ = 0.286. EReFormer Supervised, Sq Rel ↓ = 0.208. EReFormer Supervised, RMSE ↓ = 11.369. EReFormer Supervised, RMSE log ↓ = 0.325. EReFormer Supervised, SI log ↓ = 0.109. EReFormer Supervised, δ < 1 . 25 ↑ = 0.569. EReFormer Supervised, δ < 1 . 25 2 ↑ = 0.839. EReFormer Supervised, δ < 1 . 25 3 ↑ = 0.944. DepthAnyEvent Synth, Dataset = DSEC. DepthAnyEvent Synth, Abs Rel ↓ = 0.297. DepthAnyEvent Synth, Sq Rel ↓ = 0.186. DepthAnyEvent Synth, RMSE ↓ = 11.072. DepthAnyEvent Synth, RMSE log ↓ = 0.330. DepthAnyEvent Synth, SI log ↓ = 0.108. DepthAnyEvent Synth, δ < 1 . 25 ↑ = 0.519. DepthAnyEvent Synth, δ < 1 . 25 2 ↑ = 0.827. DepthAnyEvent Synth, δ < 1 . 25 3 ↑ = 0.948. DepthAnyEvent Distilled, Dataset = . DepthAnyEvent Distilled, Abs Rel ↓ = 0.213. DepthAnyEvent Distilled, Sq Rel ↓ = 0.095. DepthAnyEvent Distilled, RMSE ↓ = 8.930. DepthAnyEvent Distilled, RMSE log ↓ = 0.253. DepthAnyEvent Distilled, SI log ↓ = 0.065. DepthAnyEvent Distilled, δ < 1 . 25 ↑ = 0.662. DepthAnyEvent Distilled, δ < 1 . 25 2 ↑ = 0.915. DepthAnyEvent Distilled, δ < 1 . 25 3 ↑ = 0.980. DepthAnyEvent Supervised, Dataset = . DepthAnyEvent Supervised, Abs Rel ↓ = 0.201. DepthAnyEvent Supervised, Sq Rel ↓ = 0.079. DepthAnyEvent Supervised, RMSE ↓ = 8.880. DepthAnyEvent Supervised, RMSE log ↓ = 0.266. DepthAnyEvent Supervised, SI log ↓ = 0.077. DepthAnyEvent Supervised, δ < 1 . 25 ↑ = 0.664. DepthAnyEvent Supervised, δ < 1 . 25 2 ↑ = 0.917. DepthAnyEvent Supervised, δ < 1 . 25 3 ↑ = 0.975. DepthAnyEvent-R Synth DepthAnyEvent-R Distilled, Dataset = DSEC. DepthAnyEvent-R Synth DepthAnyEvent-R Distilled, Abs Rel ↓ = 0.276 0.226. DepthAnyEvent-R Synth DepthAnyEvent-R Distilled, Sq Rel ↓ = 0.165. DepthAnyEvent-R Synth DepthAnyEvent-R Distilled, RMSE ↓ = 10.942. DepthAnyEvent-R Synth DepthAnyEvent-R Distilled, RMSE log ↓ = 0.314. DepthAnyEvent-R Synth DepthAnyEvent-R Distilled, SI log ↓ = 0.101. DepthAnyEvent-R Synth DepthAnyEvent-R Distilled, δ < 1 . 25 ↑ = 0.555. DepthAnyEvent-R Synth DepthAnyEvent-R Distilled, δ < 1 . 25 2 ↑ = 0.843. DepthAnyEvent-R Synth DepthAnyEvent-R Distilled, δ < 1 . 25 3 ↑ = 0.954. , Dataset = . , Abs Rel ↓ = . , Sq Rel ↓ = 0.111. , RMSE ↓ = 9.310. , RMSE log ↓ = 0.266. , SI log ↓ = 0.072. , δ < 1 . 25 ↑ = 0.638. , δ < 1 . 25 2 ↑ = 0.906. , δ < 1 . 25 3 ↑ = 0.977. DepthAnyEvent-R Supervised, Dataset = . DepthAnyEvent-R Supervised, Abs Rel ↓ = 0.191. DepthAnyEvent-R Supervised, Sq Rel ↓ = 0.070. DepthAnyEvent-R Supervised, RMSE ↓ = 8.618. DepthAnyEvent-R Supervised, RMSE log ↓ = 0.244. DepthAnyEvent-R Supervised, SI log ↓ = 0.064. DepthAnyEvent-R Supervised, δ < 1 . 25 ↑ = 0.691. DepthAnyEvent-R Supervised, δ < 1 . 25 2 ↑ = 0.930. DepthAnyEvent-R Supervised, δ < 1 . 25 3 ↑ = 0.981HEADINGS:
# 5.2. Evaluation Datasets & Protocol
CONTENT:
Figure 7. Qualitative Results on DSEC - Supervised vs Distilled Models. From left to right: event image, predictions by DepthAnyEvent and its distilled counterpart, and by DepthAnyEvent-R and its distilled counterpart.HEADINGS:
# 5.3. Synthetic-to-Real Generalization
CONTENT:
We start by evaluating the capability of the different depth estimation models to generalize from synthetic data to real event streams. Purposely, we train E2Depth, EReFormer, DepthAnyEvent, and DepthAnyEvent-R on EventScape and measure their accuracy on both MVSEC and DSEC datasets. Table 1 collects the outcome of this experiment. DepthAnyEvent and DepthAnyEvent-R achieve the best results on almost any metric, hinting how the web-scale training infused in the weights we used to initialize these models represents a solid prior for depth estimation, although coming from images, i.e., a completely different modality with respect to event streams. The two models achieve mixed results one against the other on MVSEC, while DepthAnyEvent-R consistently achieves the best generalization results over DSEC, giving a first intuition about the effectiveness of our design choice to deal with streamed event data. Figure 5 presents a qualitative comparison of predictions from different models, showcasing the superior zero-shot capabilities of our DepthAnyEvent and
DepthAnyEvent-R models.HEADINGS:
# 5.4. Supervised Fine-tuning
CONTENT:
We now evaluate the accuracy of each model when trained on real event data annotated with semi-dense ground-truth depth. To this aim, we take the weights obtained after training on EventScape and perform further fine-tuning on MVSEC and DSEC separately, then evaluating on the corresponding validation sets. Table 2 reports the results of this evaluation. We can notice, once again, the notable gap in performance between DepthAnyEvent and DepthAnyEvent-R against existing methods EReFormer and E2Depth, confirming again the strong advantage that our models can exploit from the cross-modal training being conducted for image-based depth estimation. Specifically, this time we can notice how DepthAnyEvent-R consistently outperforms the vanilla DepthAnyEvent model on both MVSEC and DSEC datasets, validating our proposed design tailored to event-based depth estimation.
Figure 6 shows a qualitative comparison between theHEADINGS:
# 5.4. Supervised Fine-tuning
CONTENT:
E2Depth [15], Abs Rel ↓ = 0.344. E2Depth [15], Sq Rel ↓ = 0.253. E2Depth [15], RMSE ↓ = 13.467. E2Depth [15], RMSE log ↓ = 0.376. E2Depth [15], SI log ↓ = 0.098. E2Depth [15], δ < 1 . 25 ↑ = 0.447. E2Depth [15], δ < 1 . 25 2 ↑ = 0.755. E2Depth [15], δ < 1 . 25 3 ↑ = 0.915. EReFormer [21], Abs Rel ↓ = 0.387. EReFormer [21], Sq Rel ↓ = 0.401. EReFormer [21], RMSE ↓ = 13.954. EReFormer [21], RMSE log ↓ = 0.395. EReFormer [21], SI log ↓ = 0.124. EReFormer [21], δ < 1 . 25 ↑ = 0.486. EReFormer [21], δ < 1 . 25 2 ↑ = 0.776. EReFormer [21], δ < 1 . 25 3 ↑ = 0.892. DepthAnyEvent, Abs Rel ↓ = 0.277. DepthAnyEvent, Sq Rel ↓ = 0.170. DepthAnyEvent, RMSE ↓ = 11.117. DepthAnyEvent, RMSE log ↓ = 0.292. DepthAnyEvent, SI log ↓ = 0.051. DepthAnyEvent, δ < 1 . 25 ↑ = 0.585. DepthAnyEvent, δ < 1 . 25 2 ↑ = 0.860. DepthAnyEvent, δ < 1 . 25 3 ↑ = 0.955. DepthAnyEvent-R, Abs Rel ↓ = 0.252. DepthAnyEvent-R, Sq Rel ↓ = 0.128. DepthAnyEvent-R, RMSE ↓ = 9.824. DepthAnyEvent-R, RMSE log ↓ = 0.268. DepthAnyEvent-R, SI log ↓ = 0.045. DepthAnyEvent-R, δ < 1 . 25 ↑ = 0.592. DepthAnyEvent-R, δ < 1 . 25 2 ↑ = 0.900. DepthAnyEvent-R, δ < 1 . 25 3 ↑ = 0.971HEADINGS:
# 5.4. Supervised Fine-tuning
CONTENT:
Table 4. Metric Depth Evaluation. Training and evaluation on DSEC dataset.HEADINGS:
# 5.4. Supervised Fine-tuning
CONTENT:
Table 5. Ablation Studies. Training and evaluation on MVSEC dataset.

(A),  = DepthAnyEvent-R. (A), Supervision = Distillation. (A), Experiment = Tencode+DAv2. (A), Abs Rel ↓ = 0.399. (A), Sq Rel ↓ = 0.781. (A), RMSE ↓ = 6.830. (A), RMSE log ↓ = 0.509. (A), SI log ↓ = 0.281. (A), δ < 1 . 25 = 0.462. (A), δ < 1 . 25 2 ↑ = 0.735. (A), δ < 1 . 25 3 ↑ = 0.866. (B),  = DepthAnyEvent-R. (B), Supervision = Distillation. (B), Experiment = Tencode+DepthPro. (B), Abs Rel ↓ = 0.429. (B), Sq Rel ↓ = 0.942. (B), RMSE ↓ = 7.472. (B), RMSE log ↓ = 0.452. (B), SI log ↓ = 0.208. (B), δ < 1 . 25 = 0.444. (B), δ < 1 . 25 2 ↑ = 0.726. (B), δ < 1 . 25 3 ↑ = 0.869. (C),  = . (C), Supervision = Ground-truth. (C), Experiment = Tencode+DAv2. (C), Abs Rel ↓ = 0.365. (C), Sq Rel ↓ = 0.691. (C), RMSE ↓ = 6.465. (C), RMSE log ↓ = 0.483. (C), SI log ↓ = 0.258. (C), δ < 1 . 25 = 0.489. (C), δ < 1 . 25 2 ↑ = 0.751. (C), δ < 1 . 25 3 ↑ = 0.878. (D),  = . (D), Supervision = Ground-truth. (D), Experiment = VoxelGrid+DAv2. (D), Abs Rel ↓ = 0.382. (D), Sq Rel ↓ = 0.719. (D), RMSE ↓ = 6.932. (D), RMSE log ↓ = 0.444. (D), SI log ↓ = 0.215. (D), δ < 1 . 25 = 0.473. (D), δ < 1 . 25 2 ↑ = 0.742. (D), δ < 1 . 25 3 ↑ = 0.877. (E),  = . (E), Supervision = Ground-truth. (E), Experiment = Tencode+DAv2 (no pretrain). (E), Abs Rel ↓ = 0.446. (E), Sq Rel ↓ = 0.799. (E), RMSE ↓ = 7.492. (E), RMSE log ↓ = 0.506. (E), SI log ↓ = 0.260. (E), δ < 1 . 25 = 0.390. (E), δ < 1 . 25 2 ↑ = 0.678. (E), δ < 1 . 25 3 ↑ = 0.845. (F),  = . (F), Supervision = Ground-truth + Distillation. (F), Experiment = Tencode+DAv2. (F), Abs Rel ↓ = 0.362. (F), Sq Rel ↓ = 0.697. (F), RMSE ↓ = 6.511. (F), RMSE log ↓ = 0.438. (F), SI log ↓ = 0.211. (F), δ < 1 . 25 = 0.494. (F), δ < 1 . 25 2 ↑ = 0.760. (F), δ < 1 . 25 3 ↑ = 0.890HEADINGS:
# 5.4. Supervised Fine-tuning
CONTENT:
Table 6. Computational Analysis. Inference time on A100 GPU.

E2Depth [15], Inference (ms) = 1.50. E2Depth [15], Memory (MB) = 242. EReFormer [21], Inference (ms) = 35.75. EReFormer [21], Memory (MB) = 534. DepthAnyEvent, Inference (ms) = 1.26. DepthAnyEvent, Memory (MB) = 71. DepthAnyEvent-R, Inference (ms) = 9.20. DepthAnyEvent-R, Memory (MB) = 202
predictions by the different models, highlighting the superior accuracy achieved by DepthAnyEvent and, even higher, by DepthAnyEvent-R.HEADINGS:
# 5.5. Cross-Modal Distillation
CONTENT:
We now assess the effectiveness of our cross-modal distillation strategy compared to conventional, supervised training requiring the availability of costly depth annotations from active sensors. Table 3 collects the results achieved by each model under the training configuration considered so far, as well as after being trained according to our distillation approach. In most cases, we can notice how the models trained through distillation are comparable, and sometimes even better than their supervised counterparts.
Figure 7 show some qualitative examples from the DSEC dataset, comparing the predictions by DepthAnyEvent and DepthAnyEvent-R when trained with ground-truth or through distillation. In both cases, distilled models are even more accurate than those supervised with ground-truth.HEADINGS:
# 5.6. Metric Depth Evaluation
CONTENT:
Finally, we assess the accuracy of our models when trained to predict metric rather than affine-invariant depth. Table 4 collects the results achieved by existing networks and ours when trained on the DSEC dataset for metric depth prediction, evaluated on the validation set of the very same dataset. We can appreciate how our two architectures achieve the best results, with DepthAnyEvent-R consistently yielding the best results on any evaluation metrics.HEADINGS:
# 5.7. Ablation Studies
CONTENT:
We conclude with a study about the impact of different modules in our framework. In the former case, we train different instances of DepthAnyEvent-R on the MVSEC
dataset and evaluate on its validation set. Results are collected in Table 5, with row (A) representing the configuration used in the previous experiments.
Different VFMs for distillation. Row (B) shows that replacing Depth Anything v2 with a different VFM for distillation - i.e., Depth Pro - yields close results, although slightly worse on most metrics.
Input representation. In rows (C) and (D), we report the results achieved by training our model with ground-truth labels, when processing either Tencode or a voxel-grid representation used to encode raw events. The former yields almost consistently better results.
Pre-training. By training our model starting from DAv2 pretrained weights, we can greatly improve its performance. Indeed, when training DepthAnyEvent-R from scratch (E), the accuracy consistently drops.HEADINGS:
# 5.7. Ablation Studies
CONTENT:
Combining distillation with ground-truth labels. Finally, we show how deploying both our cross-modal distillation paradigm and ground-truth annotations (when available) further improves the final model on most metrics.HEADINGS:
# 5.8. Runtime and Memory Requirements
CONTENT:
Table 6 reports a computational analysis for any model involved in our evaluation. DepthAnyEvent achieves the fastest predictions, using as few as 80MB for a single inference. E2Depth exposes a very similar inference time, although requiring nearly 4 × the memory, while EReFormer runs consistently slower and increases the memory usage to up to 0.5GB. Compared to DepthAnyEvent, the DepthAnyEvent-R variant runs slower, yet still in real-time, and yields more accurate predictions.HEADINGS:
# 6. Conclusions
CONTENT:
In this paper, we presented a novel approach to event-based monocular depth estimation that leverages the power of pre-trained Visual Foundation Models. Our cross-modal distillation strategy effectively transfers knowledge from frame-based models to the event domain, addressing the crucial challenge of limited ground truth data for event cameras. Experimental results with synthetic and real-world
datasets validate our method, showing competitive performance compared to fully supervised methods without requiring expensive depth annotations. Moreover, we have demonstrated two effective methods for adapting VFMs to event data: a vanilla adaptation and a recurrent architecture that better captures the nature of event streams, yielding state-of-the-art performance.
Acknowledgment. This study was carried out within the MOST - Sustainable Mobility National Research Center and received funding from the European Union Next-GenerationEU PIANO NAZIONALE DI RIPRESA E RESILIENZA (PNRR) MISSIONE 4 COMPONENTE 2, INVESTIMENTO 1.4 - D.D. 1033 17/06/2022, CN00000023. This manuscript reflects only the authors' views and opinions, neither the European Union nor the European Commission can be considered responsible for them.HEADINGS:
# 6. Conclusions
CONTENT:
We also acknowledge the CINECA award under the ISCRA initiative for the availability of high-performance computing resources and support.HEADINGS:
# References
CONTENT:
- [1] Luca Bartolomei, Matteo Poggi, Andrea Conti, and Stefano Mattoccia. Lidar-event stereo fusion with hallucinations. In European Conference on Computer Vision , pages 125-145. Springer, 2024. 3, 6
- [2] Kenneth Chaney, Fernando Cladera, Ziyun Wang, Anthony Bisulco, M Ani Hsieh, Christopher Korpela, Vijay Kumar, Camillo J Taylor, and Kostas Daniilidis. M3ed: Multirobot, multi-sensor, multi-environment event dataset. In 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW) , pages 4016-4023. IEEE, 2023. 3
- [3] Mingyue Cui, Yuzhang Zhu, Yechang Liu, Yunchao Liu, Gang Chen, and Kai Huang. Dense depth-map estimation based on fusion of event camera and sparse lidar. IEEE Transactions on Instrumentation and Measurement , 71:111, 2022. 2
- [4] Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, and Vladlen Koltun. Carla: An open urban driving simulator. In Conference on robot learning , pages 1-16. PMLR, 2017. 6
- [5] Yiqun Duan, Xianda Guo, and Zheng Zhu. DiffusionDepth: Diffusion denoising approach for monocular depth estimation. arXiv preprint arXiv:2303.05021 , 2023. 2
- [6] David Eigen, Christian Puhrsch, and Rob Fergus. Depth map prediction from a single image using a multi-scale deep network. In Advances in Neural Information Processing Systems . Curran Associates, Inc., 2014. 2
- [7] Guillermo Gallego, Tobi Delbruck, Garrick Michael Orchard, Chiara Bartolozzi, Brian Taba, Andrea Censi, Stefan Leutenegger, Andrew Davison, Jorg Conradt, Kostas Daniilidis, and Davide Scaramuzza. Event-based vision: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence , pages 154-180, 2022. 2
- [8] Daniel Gehrig, Michelle R¨ uegg, Mathias Gehrig, Javier Hidalgo-Carri´ o, and Davide Scaramuzza. Combining events and frames using recurrent asynchronous multimodal net-HEADINGS:
# References
CONTENT:
works for monocular depth prediction. IEEE Robotics and Automation Letters , 6(2):2822-2829, 2021. 2, 3, 5, 6HEADINGS:
# References
CONTENT:
- [9] Mathias Gehrig, Willem Aarents, Daniel Gehrig, and Davide Scaramuzza. Dsec: A stereo event camera dataset for driving scenarios. IEEE Robotics and Automation Letters , 6(3): 4947-4954, 2021. 3, 6
- [10] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous driving? the KITTI vision benchmark suite. In Conference on Computer Vision and Pattern Recognition (CVPR) , 2012. 2
- [11] Suman Ghosh and Guillermo Gallego. Event-based stereo depth estimation: A survey. arXiv preprint arXiv:2409.17680 , 2024. 3
- [12] Cl´ ement Godard, Oisin Mac Aodha, and Gabriel J. Brostow. Unsupervised monocular depth estimation with leftright consistency. CoRR , abs/1609.03677, 2016. 2
- [13] Cl´ ement Godard, Oisin Mac Aodha, and Gabriel J. Brostow. Digging into self-supervised monocular depth estimation. CoRR , abs/1806.01260, 2018. 2
- [14] Vitor Guizilini, Igor Vasiljevic, Dian Chen, Rares , Ambrus , , and Adrien Gaidon. Towards zero-shot scale-aware monocular depth estimation. In ICCV , 2023. 2
- [15] Javier Hidalgo-Carri´ o, Daniel Gehrig, and Davide Scaramuzza. Learning monocular dense depth from events. CoRR , abs/2010.08350, 2020. 1, 2, 3, 4, 5, 6
- [16] Ze Huang, Li Sun, Cheng Zhao, Song Li, and Songzhi Su. Eventpoint: Self-supervised interest point detection and description for event-based camera. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) , pages 5396-5405, 2023. 3, 4, 5
- [17] Yuanfeng Ji, Zhe Chen, Enze Xie, Lanqing Hong, Xihui Liu, Zhaoqiang Liu, Tong Lu, Zhenguo Li, and Ping Luo. DDP: Diffusion model for dense visual prediction. In ICCV , 2023. 2
- [18] Iro Laina, Christian Rupprecht, Vasileios Belagiannis, Federico Tombari, and Nassir Navab. Deeper depth prediction with fully convolutional residual networks. In 2016 Fourth international conference on 3D vision (3DV) , pages 239248. IEEE, 2016. 2
- [19] Katrin Lasinger, Ren´ e Ranftl, Konrad Schindler, and Vladlen Koltun. Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. CoRR , abs/1907.01341, 2019. 3
- [20] Zhengqi Li and Noah Snavely. Megadepth: Learning singleview depth prediction from internet photos. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 2041-2050, 2018. 2
- [21] Xu Liu, Jianing Li, Jinqiao Shi, Xiaopeng Fan, Yonghong Tian, and Debin Zhao. Event-based monocular depth estimation with recurrent transformers. IEEE Transactions on Circuits and Systems for Video Technology , 34(8):7417-7429, 2024. 2, 3, 4, 5, 6
- [22] Yeongwoo Nam, Mohammad Mostafavi, Kuk-Jin Yoon, and Jonghyun Choi. Stereo depth from events cameras: Concentrate and focus on the future. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 6114-6123, 2022. 4HEADINGS:
# References
CONTENT:
- [23] Pushmeet Kohli Nathan Silberman, Derek Hoiem and Rob Fergus. Indoor segmentation and support inference from rgbd images. In ECCV , 2012. 2
- [24] Maxime Oquab, Timoth´ ee Darcet, Th´ eo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel HAZIZA, Francisco Massa, Alaaeldin El-Nouby, Mido Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. DINOv2: Learning robust visual features without supervision. Transactions on Machine Learning Research , 2024. Featured Certification. 5
- [25] Ren´ e Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense prediction. ICCV , 2021. 2
- [26] Ren´ e Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. IEEE Transactions on Pattern Analysis and Machine Intelligence , 44(3), 2022. 2
- [27] Ashutosh Saxena, Min Sun, and Andrew Y. Ng. Make3d: Learning 3d scene structure from a single still image. IEEE Transactions on Pattern Analysis and Machine Intelligence , 31(5):824-840, 2009. 2
- [28] Saurabh Saxena, Abhishek Kar, Mohammad Norouzi, and David J Fleet. Monocular depth estimation using diffusion models. arXiv preprint arXiv:2302.14816 , 2023. 2
- [29] Cedric Scheerlinck, Henri Rebecq, Timo Stoffregen, Nick Barnes, Robert Mahony, and Davide Scaramuzza. CED: color event camera dataset. In IEEE Conf. Comput. Vis. Pattern Recog. Workshops (CVPRW) , 2019. 2
- [30] Jiahao Shao, Yuanbo Yang, Hongyu Zhou, Youmin Zhang, Yujun Shen, Matteo Poggi, and Yiyi Liao. Learning temporally consistent video depth from video diffusion priors. arXiv preprint arXiv:2406.01493 , 2024. 2
- [31] Xingjian Shi, Zhourong Chen, Hao Wang, Dit-Yan Yeung, Wai-Kin Wong, and Wang-chun Woo. Convolutional lstm network: A machine learning approach for precipitation nowcasting. Advances in neural information processing systems , 28, 2015. 5
- [32] Gemma Taverni, Diederik Paul Moeys, Chenghan Li, Celso Cavaco, Vasyl Motsnyi, David San Segundo Bello, and Tobi Delbruck. Front and back illuminated dynamic and active pixel vision sensors comparison. IEEE Transactions on Circuits and Systems II: Express Briefs , 65(5):677-681, 2018. 2
- [33] Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything: Unleashing the power of large-scale unlabeled data. In CVPR , 2024. 2
- [34] Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything v2. arXiv:2406.09414 , 2024. 2, 5
- [35] Wei Yin, Xinlong Wang, Chunhua Shen, Yifan Liu, Zhi Tian, Songcen Xu, Changming Sun, and Dou Renyin. Diversedepth: Affine-invariant depth prediction using diverse data. arXiv preprint arXiv:2002.00569 , 2020. 2
- [36] Wei Yin, Chi Zhang, Hao Chen, Zhipeng Cai, Gang Yu, Kaixuan Wang, Xiaozhi Chen, and Chunhua Shen. Metric3D: Towards zero-shot metric 3d prediction from a single image. In ICCV , 2023. 2
- [37] Chaoqiang Zhao, Youmin Zhang, Matteo Poggi, Fabio Tosi, Xianda Guo, Zheng Zhu, Guan Huang, Yang Tang, and Stefano Mattoccia. Monovit: Self-supervised monocular depth estimation with a vision transformer. In 2022 international conference on 3D vision (3DV) , pages 668-678. IEEE, 2022. 2
- [38] Tinghui Zhou, Matthew Brown, Noah Snavely, and David G. Lowe. Unsupervised learning of depth and ego-motion from video. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , pages 6612-6619, 2017. 2
- [39] Alex Zihao Zhu, Dinesh Thakur, Tolga ¨ Ozaslan, Bernd Pfrommer, Vijay Kumar, and Kostas Daniilidis. The multivehicle stereo event camera dataset: An event camera dataset for 3d perception. IEEE Robotics and Automation Letters , 3 (3):2032-2039, 2018. 3, 6
- [40] Alex Zihao Zhu, Liangzhe Yuan, Kenneth Chaney, and Kostas Daniilidis. Unsupervised event-based learning of optical flow, depth, and egomotion. CoRR , abs/1812.08156, 2018. 2, 3, 5
- [41] Junyu Zhu, Lina Liu, Bofeng Jiang, Feng Wen, Hongbo Zhang, Wanlong Li, and Yong Liu. Self-supervised eventbased monocular depth estimation using cross-modal consistency, 2024. 3