[
  {
    "doc_id": "2509.15226v1",
    "title": "Calibration-Aware Prompt Learning for Medical Vision-Language Models",
    "abstract": "Medical Vision-Language Models (Med-VLMs) have demonstrated remarkable performance across diverse medical imaging tasks by leveraging large-scale image-text pretraining. However, their confidence calibration is largely unexplored, and so remains a significant challenge. As such, miscalibrated predictions can lead to overconfident errors, undermining clinical trust and decision-making reliability. To address this, we introduce CalibPrompt, the first framework to calibrate Med-VLMs during prompt tuning. CalibPrompt optimizes a small set of learnable prompts with carefully designed calibration objectives under scarce labeled data regime. First, we study a regularizer that attempts to align the smoothed accuracy with the predicted model confidences. Second, we introduce an angular separation loss to maximize textual feature proximity toward improving the reliability in confidence estimates of multimodal Med-VLMs. Extensive experiments on four publicly available Med-VLMs and five diverse medical imaging datasets reveal that CalibPrompt consistently improves calibration without drastically affecting clean accuracy. Our code is available at https://github.com/iabh1shekbasu/CalibPrompt.",
    "authors": [
      "Abhishek Basu",
      "Fahad Shamshad",
      "Ashshak Sharifdeen",
      "Karthik Nandakumar",
      "Muhammad Haris Khan"
    ],
    "published_date": "2025-09-18T17:59:58Z",
    "url": "http://arxiv.org/abs/2509.15226v1",
    "pdf_url": "http://arxiv.org/pdf/2509.15226v1",
    "primary_category": "cs.CV",
    "journal_ref": null,
    "doi": null,
    "references": [],
    "sub_categories": [
      "Medical Image Calibration",
      "Prompt Tuning for Med-VLMs",
      "Vision-Language Model Calibration"
    ]
  },
  {
    "doc_id": "2509.15224v1",
    "title": "Depth AnyEvent: A Cross-Modal Distillation Paradigm for Event-Based Monocular Depth Estimation",
    "abstract": "Event cameras capture sparse, high-temporal-resolution visual information, making them particularly suitable for challenging environments with high-speed motion and strongly varying lighting conditions. However, the lack of large datasets with dense ground-truth depth annotations hinders learning-based monocular depth estimation from event data. To address this limitation, we propose a crossmodal distillation paradigm to generate dense proxy labels leveraging a Vision Foundation Model (VFM). Our strategy requires an event stream spatially aligned with RGB frames, a simple setup even available off-the-shelf, and exploits the robustness of large-scale VFMs. Additionally, we propose to adapt VFMs, either a vanilla one like Depth Anything v2 (DAv2), or deriving from it a novel recurrent architecture to infer depth from monocular event cameras. We evaluate our approach with synthetic and real-world datasets, demonstrating that i) our cross-modal paradigm achieves competitive performance compared to fully supervised methods without requiring expensive depth annotations, and ii) our VFM-based models achieve state-of-the-art performance.",
    "authors": [
      "Luca Bartolomei",
      "Enrico Mannocci",
      "Fabio Tosi",
      "Matteo Poggi",
      "Stefano Mattoccia"
    ],
    "published_date": null,
    "url": "http://arxiv.org/abs/2509.15224v1",
    "pdf_url": "http://arxiv.org/pdf/2509.15224v1",
    "primary_category": "cs.CV",
    "journal_ref": null,
    "doi": null,
    "references": [],
    "sub_categories": [
      "Event-Based Depth Estimation",
      "Cross-Modal Distillation",
      "Vision Foundation Model Adaptation"
    ]
  }
]