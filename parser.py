from __future__ import annotations

import inspect
import io
import logging
import re
import warnings
import json
from collections import defaultdict
from collections.abc import Callable
from functools import partial
from io import BytesIO
from typing import TYPE_CHECKING, Iterable, Iterator, Literal, TypeAlias, get_args

from PIL import Image
from pydantic import BaseModel

import pathway as pw
from pathway.internals import udfs
from pathway.internals.config import _check_entitlements
from pathway.optional_import import optional_imports
from pathway.xpacks.llm import llms, prompts
from pathway.xpacks.llm._utils import _prepare_executor
from pathway.xpacks.llm.constants import DEFAULT_VISION_MODEL

if TYPE_CHECKING:
    with optional_imports("xpack-llm-docs"):
        from unstructured.documents.elements import Element

logger = logging.getLogger(__name__)
import requests
import asyncio
from pdfminer.high_level import extract_text

class DoclingParser(pw.UDF):
    """
    Parse PDFs using `docling` library.
    This class is a wrapper around the `DocumentConverter` from `docling` library with some extra
    functionality to also parse images from the PDFs using vision LLMs.

    Args:
        image_parsing_strategy (Literal["llm"] | None): Strategy for parsing images.
            If set to ``"llm"``, images will be replaced with descriptions generated by the vision LLM.
            In that case you have to provide a vision LLM in the ``multimodal_llm`` argument.
            Defaults to None.
        table_parsing_strategy (Literal["docling", "llm"]): Strategy for parsing tables.
            If set to ``"docling"``, tables will be parsed using ``docling`` library.
            If set to ``"llm"``, tables will be replaced with descriptions generated by the vision LLM.
            This description will contain table data parsed with LLM.
            Defaults to ``"docling"``.
        multimodal_llm (llms.OpenAIChat | llms.LiteLLMChat | None): LLM for parsing the image.
            Provided LLM should support image inputs in the same API format as OpenAI does.
            Required if ``parse_images`` is set to ``True``.
        cache_strategy (udfs.CacheStrategy | None): Defines the caching mechanism.
        pdf_pipeline_options (dict): Additional options for the ``DocumentConverter`` from ``docling``.
            These options will be passed to the ``PdfPipelineOptions`` object and will override the defaults
            that are dynamically created based on other arguments set in this constructor.
            See original code for reference:
            https://github.com/DS4SD/docling/blob/main/docling/datamodel/pipeline_options.py#L288
            Keep in mind that you can also change lower-level configurations like TableStructureOptions; e.g.:
            ``pdf_pipeline_options={"table_structure_options": {"mode": "accurate"}}``.
        chunk (bool): Whether to chunk parsed document into smaller structurally coherent parts.
            Under the hood it will use modified ``HybridChunker`` from ``docling`` library.
            Modification has been made to properly handle additional functionality of this class
            that allows to parse images and tables using vision LLMs. It also modifies how tables
            are transformed into text (instead of creating triplets of row, column and value we
            directly transform table into its markdown format).
            All images and tables are split into separate chunks. Each of them contains a caption.
            Chunks that have similar metadata will be merged together.
            As of now, this chunker is not sensitive for length of the chunks (neither if measured
            in characters or tokens). It will chunk the document based only on the structure.
            If set to False the entire document will be returned as a single chunk.
            Defaults to ``True``.
    """

    if TYPE_CHECKING:
        with optional_imports("xpack-llm-docs"):
            from docling_core.transforms.chunker.hierarchical_chunker import BaseChunk
            from docling_core.types.doc.document import (
                DoclingDocument,
                PictureItem,
                TableItem,
                TextItem,
            )
            from docling_core.types.io import DocumentStream

    def __init__(
        self,
        image_parsing_strategy: Literal["llm"] | None = None,
        table_parsing_strategy: Literal["docling", "llm"] | None = "docling",
        multimodal_llm: llms.OpenAIChat | llms.LiteLLMChat | None = None,
        cache_strategy: udfs.CacheStrategy | None = None,
        pdf_pipeline_options: dict = {},
        chunk: bool = True,
        *,
        async_mode: Literal["batch_async", "fully_async"] = "batch_async",
    ):
        with optional_imports("xpack-llm-docs"):
            from docling.datamodel.pipeline_options import (
                PdfPipelineOptions,
                TableStructureOptions,
            )
            from docling.document_converter import (
                DocumentConverter,
                InputFormat,
                PdfFormatOption,
            )

        from pathway.xpacks.llm import _parser_utils

        self.chunker: _parser_utils._HybridChunker | None = None
        self.multimodal_llm: llms.OpenAIChat | llms.LiteLLMChat | None = None

        if image_parsing_strategy == "llm" or table_parsing_strategy == "llm":
            if multimodal_llm is None:
                warnings.warn(
                    "`image_parsing_strategy` is set to `llm`, but `multimodal_llm` is "
                    f"not specified in DoclingParser, defaulting to `{DEFAULT_VISION_MODEL}`."
                )
                self.multimodal_llm = llms.OpenAIChat(
                    model=DEFAULT_VISION_MODEL,
                    cache_strategy=udfs.DefaultCache(),
                    retry_strategy=udfs.ExponentialBackoffRetryStrategy(max_retries=4),
                    verbose=True,
                )
            else:
                self.multimodal_llm = multimodal_llm
        else:
            self.multimodal_llm = None

        self.image_parsing_strategy = image_parsing_strategy
        self.table_parsing_strategy = table_parsing_strategy

        default_table_structure_options = {
            # whether to perform cell matching; small impact on speed, noticeable impact on quality
            "do_cell_matching": True,
            # mode of table parsing; either `fast` or `accurate`; big impact on speed
            # small impact on quality (might be useful for complex tables though)
            "mode": "fast",
        }
        default_pipeline_options = {
            # whether to parse tables or not; big impact on speed
            "do_table_structure": (
                True if table_parsing_strategy == "docling" else False
            ),
            # whether to make images of pages; small impact on speed
            "generate_page_images": True if self.multimodal_llm is not None else False,
            # whether to make images of pictures; small impact on speed
            "generate_picture_images": (
                True if self.multimodal_llm is not None else False
            ),
            "generate_table_images": (
                True if self.multimodal_llm is not None else False
            ),
            # whether to perform OCR; big impact on speed
            "do_ocr": False,
            # scale of images; small impact on speed
            "images_scale": 2,
        }

        pipeline_options = PdfPipelineOptions(
            **(default_pipeline_options | pdf_pipeline_options)
        )
        pipeline_options.table_structure_options = TableStructureOptions(
            **(
                default_table_structure_options
                | pdf_pipeline_options.get("table_structure_options", {})
            )
        )

        # actual docling converter
        self.converter: DocumentConverter = DocumentConverter(
            format_options={
                InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)
            },
            # TODO: Add more file types
        )

        # modified Docling's chunker
        if chunk:
            self.chunker = _parser_utils._HybridChunker(merge_peers=True)

        executor = _prepare_executor(async_mode=async_mode)
        super().__init__(cache_strategy=cache_strategy, executor=executor)

    def format_chunk(self, chunk: BaseChunk) -> tuple[str, dict]:
        text = ""
        metadata = chunk.meta.export_json_dict()
        headings = metadata.pop("headings", [])
        caption = "\n".join(metadata.pop("captions", []))

        # prepend each heading (with appropriate number of #s) to the text
        if len(headings) > 0:
            text += "HEADINGS:\n"
            for level, heading in enumerate(headings, start=1):
                text += "#" * level + " " + heading + "\n"

        text += "CONTENT:\n" + chunk.text
        # add caption if present
        if caption:
            text += "\n\nCAPTION:\n" + caption

        # move page numbers from the chunk's metadata to be higher in the stack
        metadata["pages"] = sorted(
            {
                prov_item["page_no"]
                for item in metadata["doc_items"]
                for prov_item in item["prov"]
            }
        )

        # no need to keep these in metadata
        metadata.pop("version")
        metadata.pop("schema_name")
        metadata.pop("origin")

        return (text, metadata)

    def format_document(self, doc: DoclingDocument) -> tuple[str, dict]:
        from docling_core.types.doc.document import PictureItem, TableItem, TextItem
        from docling_core.types.doc.labels import DocItemLabel

        text = ""

        for item, level in doc.iterate_items():

            if isinstance(item, TextItem):
                label: DocItemLabel = item.label

                if label in [
                    DocItemLabel.CAPTION,
                    DocItemLabel.PAGE_FOOTER,
                    DocItemLabel.FOOTNOTE,
                ]:
                    continue

                if label == DocItemLabel.TITLE:
                    text += "# "
                if label == DocItemLabel.SECTION_HEADER:
                    text += "#" * (level + 1) + " "

                text += item.text

            elif isinstance(item, TableItem):
                table_df = item.export_to_dataframe()
                text += table_df.to_markdown(index=False)
                captions = [c.text for c in [r.resolve(doc) for r in item.captions]]
                if len(captions):
                    text += "\n\n" + "\n\n".join(captions)

            elif isinstance(item, PictureItem):
                captions = [cap.resolve(doc).text for cap in item.captions]
                if len(captions):
                    text += "\n\n".join(captions)

            else:
                continue

            text += "\n\n"

        return (text, {})

    async def parse_visual_data(
        self,
        b64_imgs: list[str] | str,
        prompt: str = prompts.DEFAULT_IMAGE_PARSE_PROMPT,
    ) -> list[str]:
        """
        Perform OCR using the vision LLM on the given images.
        In this context image could be an actual picture (e.g. of a corgi) or an image of a table.
        Image must be encoded using base 64 format (with the prefix "data:image/jpeg;base64,").

        Args:
            b64_imgs (list[str] | str): List of base64 encoded images.
            prompt (str): The prompt used by the language model for parsing.
        """

        from pathway.xpacks.llm import _parser_utils

        if not self.multimodal_llm:
            raise ValueError(
                "Image parsing is not enabled, cannot replace images with descriptions."
            )

        if isinstance(b64_imgs, str):
            b64_imgs = [b64_imgs]

        # remove the prefix from the base64 encoded images as these are added later on
        b64_imgs = [i.replace("data:image/png;base64,", "") for i in b64_imgs]

        image_descriptions, _ = await _parser_utils._parse_b64_images(
            b64_imgs,
            self.multimodal_llm,
            prompt,
            run_mode="parallel",
            parse_details=False,
            detail_parse_schema=None,
            parse_fn=_parser_utils.parse_image,
            parse_image_details_fn=None,
        )

        return image_descriptions

    async def parse(self, contents: bytes) -> list[tuple[str, dict]]:

        with optional_imports("xpack-llm-docs"):
            from docling_core.transforms.chunker.hierarchical_chunker import BaseChunk
            from docling_core.types.doc.document import PictureItem, TableItem
            from docling_core.types.doc.labels import DocItemLabel
            from docling_core.types.io import DocumentStream

        stream = DocumentStream(name="document.pdf", stream=BytesIO(contents))

        # parse document
        parsing_result = self.converter.convert(stream)
        doc = parsing_result.document

        # analyze images and/or tables with multimodal llm
        if self.multimodal_llm:
            if self.image_parsing_strategy == "llm":
                picture_items: list[PictureItem] = []
                b64_picture_imgs = []
                # collect all docling items that are pictures and their base64 encoded images
                for item, _ in doc.iterate_items():
                    if isinstance(item, PictureItem):
                        if item.image is not None:
                            b64_picture_imgs.append(str(item.image.uri))
                            picture_items.append(item)
                # parse all images in one batch
                picture_descriptions = await self.parse_visual_data(
                    b64_picture_imgs, prompts.DEFAULT_IMAGE_PARSE_PROMPT
                )
                # modify document's items with the parsed descriptions (saved as additional captions)
                for item, description in zip(picture_items, picture_descriptions):
                    new_text_item = doc.add_text(
                        DocItemLabel.CAPTION, description, parent=item
                    )
                    item.captions.append(new_text_item.get_ref())

            if self.table_parsing_strategy == "llm":
                table_items: list[TableItem] = []
                b64_table_imgs = []
                for item, _ in doc.iterate_items():
                    if isinstance(item, TableItem):
                        if item.image is not None:
                            b64_table_imgs.append(str(item.image.uri))
                            table_items.append(item)
                table_descriptions = await self.parse_visual_data(
                    b64_table_imgs, prompts.DEFAULT_TABLE_PARSE_PROMPT
                )
                for item, description in zip(table_items, table_descriptions):
                    new_text_item = doc.add_text(
                        DocItemLabel.CAPTION, description, parent=item
                    )
                    item.captions.append(new_text_item.get_ref())

        # chunk the document
        chunks: list[tuple[str, dict]] = []
        if self.chunker:

            docling_chunks: Iterator[BaseChunk] = self.chunker.chunk(doc)
            for chunk in docling_chunks:
                md_text, meta = self.format_chunk(chunk)
                chunks.append((md_text, meta))

        # if chunking is disabled then format the document into markdown and treat it as one, large chunk
        else:
            md_text, meta = self.format_document(doc)
            chunks.append((md_text, meta))

        return chunks

    async def __wrapped__(self, contents: bytes, **kwargs) -> list[tuple[str, dict]]:
        return await self.parse(contents)

doc_ids = []
file_path = "arxiv_papers.jsonl"
try:
    with open(file_path, 'r') as f:
        for line in f:
            try:
                # Load the JSON object from each line
                data = json.loads(line)
                # Get the 'doc_id' and add it to our list
                if 'doc_id' in data:
                    doc_ids.append(data['doc_id'])
            except json.JSONDecodeError:
                # Skip lines that are not valid JSON
                print(f"Warning: Could not decode JSON from line: {line.strip()}")
except FileNotFoundError:
    print(f"Error: The file '{file_path}' was not found.")

for pdf_id in doc_ids:
    try:
        # Ensure output directory exists
        import os
        os.makedirs("papers_text", exist_ok=True)

        # 1. Download the PDF file from arXiv
        pdf_url = f"https://arxiv.org/pdf/{pdf_id}.pdf"
        headers = {"User-Agent": "Mozilla/5.0 (compatible; PaperParser/1.0)"}
        response = requests.get(pdf_url, headers=headers, timeout=30)
        if response.status_code != 200:
            print(f"Failed to download PDF for {pdf_id}: HTTP {response.status_code}")
            continue
        pdf_bytes = response.content

        # Try lightweight text extraction first (pdfminer)
        text_out = ""
        try:
            text_out = extract_text(io.BytesIO(pdf_bytes)) or ""
        except Exception as e:
            print(f"pdfminer extraction failed for {pdf_id}: {e}")
            text_out = ""

        if not text_out.strip():
            # Fallback to DoclingParser (heavier), if available
            try:
                parser = DoclingParser(pdf_pipeline_options={"do_table_structure": False})

                async def main():
                    results = await parser.parse(pdf_bytes)
                    return "\n\n".join(t for t, _ in results)

                text_out = asyncio.run(main())
            except Exception as e:
                print(f"Docling fallback failed for {pdf_id}: {e}")
                text_out = ""

        if not text_out.strip():
            print(f"No text extracted for {pdf_id}, skipping save.")
            continue

        out_path = os.path.join("papers_text", f"{pdf_id}.txt")
        with open(out_path, "w", encoding="utf-8") as f:
            f.write(text_out)
        print(f"Saved text to {out_path}")

    except Exception as e:
        print(f"Unexpected error processing {pdf_id}: {e}")