{"doc_id": "2509.15225v1", "title": "Lost in Translation? Vocabulary Alignment for Source-Free Domain   Adaptation in Open-Vocabulary Semantic Segmentation", "abstract": "We introduce VocAlign, a novel source-free domain adaptation framework specifically designed for VLMs in open-vocabulary semantic segmentation. Our method adopts a student-teacher paradigm enhanced with a vocabulary alignment strategy, which improves pseudo-label generation by incorporating additional class concepts. To ensure efficiency, we use Low-Rank Adaptation (LoRA) to fine-tune the model, preserving its original capabilities while minimizing computational overhead. In addition, we propose a Top-K class selection mechanism for the student model, which significantly reduces memory requirements while further improving adaptation performance. Our approach achieves a notable 6.11 mIoU improvement on the CityScapes dataset and demonstrates superior performance on zero-shot segmentation benchmarks, setting a new standard for source-free adaptation in the open-vocabulary setting.", "authors": ["Silvio Mazzucco", "Carl Persson", "Mattia Segu", "Pier Luigi Dovesi", "Federico Tombari", "Luc Van Gool", "Matteo Poggi"], "published_date": "2025-09-18T17:59:58Z", "url": "http://arxiv.org/abs/2509.15225v1", "pdf_url": "http://arxiv.org/pdf/2509.15225v1", "primary_category": "cs.CV", "references": [], "sub_categories": ["Source-Free Domain Adaptation", "Open-Vocabulary Semantic Segmentation", "Vision-Language Models"]}
{"doc_id": "2509.15226v1", "title": "Calibration-Aware Prompt Learning for Medical Vision-Language Models", "abstract": "Medical Vision-Language Models (Med-VLMs) have demonstrated remarkable performance across diverse medical imaging tasks by leveraging large-scale image-text pretraining. However, their confidence calibration is largely unexplored, and so remains a significant challenge. As such, miscalibrated predictions can lead to overconfident errors, undermining clinical trust and decision-making reliability. To address this, we introduce CalibPrompt, the first framework to calibrate Med-VLMs during prompt tuning. CalibPrompt optimizes a small set of learnable prompts with carefully designed calibration objectives under scarce labeled data regime. First, we study a regularizer that attempts to align the smoothed accuracy with the predicted model confidences. Second, we introduce an angular separation loss to maximize textual feature proximity toward improving the reliability in confidence estimates of multimodal Med-VLMs. Extensive experiments on four publicly available Med-VLMs and five diverse medical imaging datasets reveal that CalibPrompt consistently improves calibration without drastically affecting clean accuracy. Our code is available at https://github.com/iabh1shekbasu/CalibPrompt.", "authors": ["Abhishek Basu", "Fahad Shamshad", "Ashshak Sharifdeen", "Karthik Nandakumar", "Muhammad Haris Khan"], "published_date": "2025-09-18T17:59:58Z", "url": "http://arxiv.org/abs/2509.15226v1", "pdf_url": "http://arxiv.org/pdf/2509.15226v1", "primary_category": "cs.CV", "references": ["Expected Calibration Error (ECE) [21]: Measures the weighted average difference between predicted confidence and actual accuracy across 10 fixed-width bins.", "Adaptive Calibration Error (ACE) [23]: Similar to ECE but uses adaptive binning to ensure equal sample counts per bin, improving reliability on skewed confidence distributions.", "Maximum Calibration Error (MCE) [21]: Reports the largest absolute difference between confidence and accuracy across all bins, highlighting worst-case miscalibration.", "ECE with Kernel Density Estimation (ECEKDE) [27]: A binning-free metric that uses kernel density estimation to compute a continuous calibration error estimate."], "sub_categories": ["Medical Vision-Language Models", "Prompt Tuning", "Confidence Calibration"]}
{"doc_id": "2509.15224v1", "title": "Depth AnyEvent: A Cross-Modal Distillation Paradigm for Event-Based   Monocular Depth Estimation", "abstract": "Event cameras capture sparse, high-temporal-resolution visual information, making them particularly suitable for challenging environments with high-speed motion and strongly varying lighting conditions. However, the lack of large datasets with dense ground-truth depth annotations hinders learning-based monocular depth estimation from event data. To address this limitation, we propose a cross-modal distillation paradigm to generate dense proxy labels leveraging a Vision Foundation Model (VFM). Our strategy requires an event stream spatially aligned with RGB frames, a simple setup even available off-the-shelf, and exploits the robustness of large-scale VFMs. Additionally, we propose to adapt VFMs, either a vanilla one like Depth Anything v2 (DAv2), or deriving from it a novel recurrent architecture to infer depth from monocular event cameras. We evaluate our approach with synthetic and real-world datasets, demonstrating that i) our cross-modal paradigm achieves competitive performance compared to fully supervised methods without requiring expensive depth annotations, and ii) our VFM-based models achieve state-of-the-art performance.", "authors": ["Luca Bartolomei", "Enrico Mannocci", "Fabio Tosi", "Matteo Poggi", "Stefano Mattoccia"], "published_date": "2025-09-18T17:59:51Z", "url": "http://arxiv.org/abs/2509.15224v1", "pdf_url": "http://arxiv.org/pdf/2509.15224v1", "primary_category": "cs.CV", "references": ["TTI vision benchmark suite. In Conference on Computer Vision and Pattern Recognition (CVPR), 2012.", "Suman Ghosh and Guillermo Gallego. Event-based stereo depth estimation: A survey. arXiv preprint arXiv:2409.17680, 2024.", "Cl\u00b4ement Godard, Oisin Mac Aodha, and Gabriel J. Bros tow. Unsupervised monocular depth estimation with left-right consistency. CoRR, abs/1609.03677, 2016.", "Cl\u00b4ement Godard, Oisin Mac Aodha, and Gabriel J. Bros tow. Digging into self-supervised monocular depth estimation. CoRR, abs/1806.01260, 2018.", "Vitor Guizilini, Igor Vasiljevic, Dian Chen, Rares, Ambrus,, and Adrien Gaidon. Towards zero-shot scale-aware monocular depth estimation. In ICCV, 2023.", "Javier Hidalgo-Carri\u00b4o, Daniel Gehrig, and Davide Scaramuzza. Learning monocular dense depth from events. CoRR, abs/2010.08350, 2020.", "Ze Huang, Li Sun, Cheng Zhao, Song Li, and Songzhi Su. Eventpoint: Self-supervised interest point detection and description for event-based camera. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pages 5396\u20135405, 2023.", "Yuanfeng Ji, Zhe Chen, Enze Xie, Lanqing Hong, Xihui Liu, Zhaoqiang Liu, Tong Lu, Zhenguo Li, and Ping Luo. DDP: Diffusion model for dense visual prediction. In ICCV, 2023.", "Iro Laina, Christian Rupprecht, Vasileios Belagiannis, Federico Tombari, and Nassir Navab. Deeper depth prediction with fully convolutional residual networks. In 2016 Fourth international conference on 3D vision (3DV), pages 239\u2013248. IEEE, 2016.", "Katrin Lasinger, Ren\u00b4e Ranftl, Konrad Schindler, and Vladlen Koltun. Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. CoRR, abs/1907.01341, 2019.", "Zhengqi Li and Noah Snavely. Megadepth: Learning single-view depth prediction from internet photos. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2041\u20132050, 2018.", "Xu Liu, Jianing Li, Jinqiao Shi, Xiaopeng Fan, Yonghong Tian, and Debin Zhao. Event-based monocular depth estimation with recurrent transformers. IEEE Transactions on Circuits and Systems for Video Technology, 34(8):7417\u20137429, 2024.", "Yeongwoo Nam, Mohammad Mostafavi, Kuk-Jin Yoon, and Jonghyun Choi. Stereo depth from events cameras: Concentrate and focus on the future. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 6114\u20136123, 2022.", "Pushmeet Kohli Nathan Silberman, Derek Hoiem and Rob Fergus. Indoor segmentation and support inference from rgbd images. In ECCV, 2012.", "Maxime Oquab, Timoth\u00b4ee Darcet, Th\u00b4eo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel HAZIZA, Francisco Massa, Alaaeldin El-Nouby, Mido Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. DINOv2: Learning robust visual features without supervision. Transactions on Machine Learning Research, 2024. Featured Certification.", "Ren\u00b4e Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense prediction. ICCV, 2021.", "Ren\u00b4e Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(3), 2022.", "Ashutosh Saxena, Min Sun, and Andrew Y. Ng. Make3d: Learning 3d scene structure from a single still image. IEEE Transactions on Pattern Analysis and Machine Intelligence, 31(5):824\u2013840, 2009.", "Saurabh Saxena, Abhishek Kar, Mohammad Norouzi, and David J Fleet. Monocular depth estimation using diffusion models. arXiv preprint arXiv:2302.14816, 2023.", "Cedric Scheerlinck, Henri Rebecq, Timo Stoffregen, Nick Barnes, Robert Mahony, and Davide Scaramuzza. CED: color event camera dataset. In IEEE Conf. Comput. Vis. Pattern Recog. Workshops (CVPRW), 2019.", "Jiahao Shao, Yuanbo Yang, Hongyu Zhou, Youmin Zhang, Yujun Shen, Matteo Poggi, and Yiyi Liao. Learning temporally consistent video depth from video diffusion priors. arXiv preprint arXiv:2406.01493, 2024.", "Xingjian Shi, Zhourong Chen, Hao Wang, Dit-Yan Yeung, Wai-Kin Wong, and Wang-chun Woo. Convolutional lstm network: A machine learning approach for precipitation nowcasting. Advances in neural information processing systems, 28, 2015.", "Gemma Taverni, Diederik Paul Moeys, Chenghan Li, Celso Cavaco, Vasyl Motsnyi, David San Segundo Bello, and Tobi Delbruck. Front and back illuminated dynamic and active pixel vision sensors comparison. IEEE Transactions on Circuits and Systems II: Express Briefs, 65(5):677\u2013681, 2018.", "Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything: Unleashing the power of large-scale unlabeled data. In CVPR, 2024.", "Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything v2. arXiv:2406.09414, 2024.", "Wei Yin, Xinlong Wang, Chunhua Shen, Yifan Liu, Zhi Tian, Songcen Xu, Changming Sun, and Dou Renyin. Diversedepth: Affine-invariant depth prediction using diverse data. arXiv preprint arXiv:2002.00569, 2020.", "Wei Yin, Chi Zhang, Hao Chen, Zhipeng Cai, Gang Yu, Kaixuan Wang, Xiaozhi Chen, and Chunhua Shen. Metric3D: Towards zero-shot metric 3d prediction from a single image. In ICCV, 2023.", "Chaoqiang Zhao, Youmin Zhang, Matteo Poggi, Fabio Tosi, Xianda Guo, Zheng Zhu, Guan Huang, Yang Tang, and Stefano Mattoccia. Monovit: Self-supervised monocular depth estimation with a vision transformer. In 2022 international conference on 3D vision (3DV), pages 668\u2013678. IEEE, 2022.", "Tinghui Zhou, Matthew Brown, Noah Snavely, and David G. Lowe. Unsupervised learning of depth and ego-motion from video. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 6612\u20136619, 2017.", "Alex Zihao Zhu, Dinesh Thakur, Tolga \u00a8Ozaslan, Bernd Pfrommer, Vijay Kumar, and Kostas Daniilidis. The multi-vehicle stereo event camera dataset: An event camera dataset for 3d perception. IEEE Robotics and Automation Letters, 3(3):2032\u20132039, 2018.", "Alex Zihao Zhu, Liangzhe Yuan, Kenneth Chaney, and Kostas Daniilidis. Unsupervised event-based learning of optical flow, depth, and egomotion. CoRR, abs/1812.08156, 2018.", "Junyu Zhu, Lina Liu, Bofeng Jiang, Feng Wen, Hongbo Zhang, Wanlong Li, and Yong Liu. Self-supervised event-based monocular depth estimation using cross-modal consistency, 2024."], "sub_categories": ["Event-Based Monocular Depth Estimation", "Cross-Modal Distillation", "Vision Foundation Models"]}
{"doc_id": "2509.15222v1", "title": "Two Web Toolkits for Multimodal Piano Performance Dataset Acquisition   and Fingering Annotation", "abstract": "Piano performance is a multimodal activity that intrinsically combines physical actions with the acoustic rendition. Despite growing research interest in analyzing the multimodal nature of piano performance, the laborious process of acquiring large-scale multimodal data remains a significant bottleneck, hindering further progress in this field. To overcome this barrier, we present an integrated web toolkit comprising two graphical user interfaces (GUIs): (i) PiaRec, which supports the synchronized acquisition of audio, video, MIDI, and performance metadata. (ii) ASDF, which enables the efficient annotation of performer fingering from the visual data. Collectively, this system can streamline the acquisition of multimodal piano performance datasets.", "authors": ["Junhyung Park", "Yonghyun Kim", "Joonhyung Bae", "Kirak Kim", "Taegyun Kwon", "Alexander Lerch", "Juhan Nam"], "published_date": "2025-09-18T17:59:24Z", "url": "http://arxiv.org/abs/2509.15222v1", "pdf_url": "http://arxiv.org/pdf/2509.15222v1", "primary_category": "cs.SD", "references": ["K. Jensen and S. R. Frimodt-M\u00f8ller, \u201cMultimodal analysis of piano performances portraying different emotions,\u201d in International Symposium on Computer Music Modeling and Retrieval. Springer, 2012, pp. 469\u2013479.", "K. Riley, E. E. Coons, and D. Marcarian, \u201cThe use of multimodal feedback in retraining complex technical skills of piano performance,\u201d Medical Problems of Performing Artists, vol. 20, no. 2, pp. 82\u201388, 2005.", "P. Parmar, J. Reddy, and B. Morris, \u201cPiano skills assessment,\u201d in 2021 IEEE 23rd international workshop on multimedia signal processing (MMSP), 2021, pp. 1\u20135.", "J. Swinkin, \u201cKeyboard fingering and interpretation: A comparison of historical and modern approaches,\u201d Performance practice review, vol. 12, no. 1, p. 1, 2007.", "Y. Kim, J. Park, J. Bae, K. Kim, T. Kwon, A. Lerch, and J. Nam, \u201cPianovam: A multimodal piano performance dataset,\u201d in Proceedings of the 26th International Society for Music Information Retrieval Conference (IS-MIR), 2025.", "F. Zhang, V. Bazarevsky, A. Vakunov, A. Tkachenka, G. Sung, C.-L. Chang, and M. Grundmann, \u201cMediapipe hands: On-device real-time hand tracking,\u201d 2020. [Online]. Available: https://arxiv.org/abs/2006.10214"], "sub_categories": ["Multimodal Piano Performance Dataset Acquisition", "Fingering Annotation", "Web-Based Toolkits"]}
{"doc_id": "2509.15221v1", "title": "ScaleCUA: Scaling Open-Source Computer Use Agents with Cross-Platform   Data", "abstract": "Vision-Language Models (VLMs) have enabled computer use agents (CUAs) that operate GUIs autonomously, showing great potential, yet progress is limited by the lack of large-scale, open-source computer use data and foundation models. In this work, we introduce ScaleCUA, a step toward scaling open-source CUAs. It offers a large-scale dataset spanning 6 operating systems and 3 task domains, built via a closed-loop pipeline uniting automated agents with human experts. Trained on this scaled-up data, ScaleCUA can operate seamlessly across platforms. Specifically, it delivers strong gains over baselines (+26.6 on WebArena-Lite-v2, +10.7 on ScreenSpot-Pro) and sets new state-of-the-art results (94.4% on MMBench-GUI L1-Hard, 60.6% on OSWorld-G, 47.4% on WebArena-Lite-v2). These findings underscore the power of data-driven scaling for general-purpose computer use agents. We will release data, models, and code to advance future research: https://github.com/OpenGVLab/ScaleCUA.", "authors": ["Zhaoyang Liu", "JingJing Xie", "Zichen Ding", "Zehao Li", "Bowen Yang", "Zhenyu Wu", "Xuehui Wang", "Qiushi Sun", "Shi Liu", "Weiyun Wang", "Shenglong Ye", "Qingyun Li", "Zeyue Tian", "Gen Luo", "Xiangyu Yue", "Biqing Qi", "Kai Chen", "Bowen Zhou", "Yu Qiao", "Qifeng Chen", "Wenhai Wang"], "published_date": "2025-09-18T17:59:22Z", "url": "http://arxiv.org/abs/2509.15221v1", "pdf_url": "http://arxiv.org/pdf/2509.15221v1", "primary_category": "cs.CV", "references": [], "sub_categories": ["Computer Use Agents", "Cross-Platform GUI Interaction", "Vision-Language Models for GUI"]}
{"doc_id": "2509.15220v1", "title": "Lightweight and Accurate Multi-View Stereo with Confidence-Aware   Diffusion Model", "abstract": "To reconstruct the 3D geometry from calibrated images, learning-based multi-view stereo (MVS) methods typically perform multi-view depth estimation and then fuse depth maps into a mesh or point cloud. To improve the computational efficiency, many methods initialize a coarse depth map and then gradually refine it in higher resolutions. Recently, diffusion models achieve great success in generation tasks. Starting from a random noise, diffusion models gradually recover the sample with an iterative denoising process. In this paper, we propose a novel MVS framework, which introduces diffusion models in MVS. Specifically, we formulate depth refinement as a conditional diffusion process. Considering the discriminative characteristic of depth estimation, we design a condition encoder to guide the diffusion process. To improve efficiency, we propose a novel diffusion network combining lightweight 2D U-Net and convolutional GRU. Moreover, we propose a novel confidence-based sampling strategy to adaptively sample depth hypotheses based on the confidence estimated by diffusion model. Based on our novel MVS framework, we propose two novel MVS methods, DiffMVS and CasDiffMVS. DiffMVS achieves competitive performance with state-of-the-art efficiency in run-time and GPU memory. CasDiffMVS achieves state-of-the-art performance on DTU, Tanks & Temples and ETH3D. Code is available at: https://github.com/cvg/diffmvs.", "authors": ["Fangjinhua Wang", "Qingshan Xu", "Yew-Soon Ong", "Marc Pollefeys"], "published_date": "2025-09-18T17:59:19Z", "url": "http://arxiv.org/abs/2509.15220v1", "pdf_url": "http://arxiv.org/pdf/2509.15220v1", "primary_category": "cs.CV", "references": ["larization networks for multi-view stereo,\u201d arXiv preprint arXiv:2110.06436, 2021.", "X. Gu, Z. Fan, S. Zhu, Z. Dai, F. Tan, and P. Tan, \u201cCascade cost volume for high-resolution multi-view stereo and stereo matching,\u201d in CVPR, 2020.", "S. Cheng, Z. Xu, S. Zhu, Z. Li, L. E. Li, R. Ramamoorthi, and H. Su, \u201cDeep stereo using adaptive thin volume representation with uncertainty awareness,\u201d in CVPR, 2020.", "J. Yang, W. Mao, J. M. Alvarez, and M. Liu, \u201cCost volume pyramid based depth inference for multi-view stereo,\u201d in CVPR, 2020.", "F. Wang, S. Galliani, C. Vogel, P. Speciale, and M. Pollefeys, \u201cPatch-matchnet: Learned multi-view patchmatch stereo,\u201d in CVPR, June 2021, pp. 14 194\u201314 203.", "J. Ho, A. Jain, and P. Abbeel, \u201cDenoising diffusion probabilistic models,\u201d Advances in Neural Information Processing Systems, vol. 33, pp. 6840\u20136851, 2020.", "R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, \u201cHigh-resolution image synthesis with latent diffusion models,\u201d in CVPR, 2022, pp. 10 684\u201310 695.", "Y. Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Ermon, and B. Poole, \u201cScore-based generative modeling through stochastic differential equations,\u201d arXiv preprint arXiv:2011.13456, 2020.", "R. Shao, Z. Zheng, H. Zhang, J. Sun, and Y. Liu, \u201cDiffustereo: High quality human reconstruction via diffusion-based stereo using sparse cameras,\u201d in ECCV, 2022, pp. 702\u2013720.", "Z. Teed and J. Deng, \u201cRaft: Recurrent all-pairs field transforms for optical flow,\u201d in ECCV. Springer, 2020, pp. 402\u2013419.", "F. Wang, S. Galliani, C. Vogel, and M. Pollefeys, \u201cItermvs: iterative probability estimation for efficient multi-view stereo,\u201d in CVPR, 2022, pp. 8606\u20138615.", "C. Ren, Q. Xu, S. Zhang, and J. Yang, \u201cHierarchical prior mining for non-local multi-view stereo,\u201d in ICCV, 2023, pp. 3611\u20133620.", "A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and I. Polosukhin, \u201cAttention is all you need,\u201d Advances in neural information processing systems, vol. 30, 2017.", "S. Chen, P. Sun, Y. Song, and P. Luo, \u201cDiffusiondet: Diffusion model for object detection,\u201d arXiv preprint arXiv:2211.09788, 2022.", "A. O. Ulusoy, M. J. Black, and A. Geiger, \u201cSemantic multi-view stereo: Jointly estimating objects and voxels,\u201d in CVPR, 2017.", "K. N. Kutulakos and S. M. Seitz, \u201cA theory of shape by space carving,\u201d IJCV, vol. 38, no. 3, pp. 199\u2013218, 2000.", "S. M. Seitz and C. R. Dyer, \u201cPhotorealistic scene reconstruction by voxel coloring,\u201d IJCV, vol. 35, no. 2, pp. 151\u2013173, 1999.", "I. Kostrikov, E. Horbert, and B. Leibe, \u201cProbabilistic labeling cost for high-accuracy multi-view reconstruction,\u201d in CVPR, 2014, pp. 1534\u20131541.", "M. Lhuillier and L. Quan, \u201cA quasi-dense approach to surface reconstruction from uncalibrated images,\u201d PAMI, vol. 27, no. 3, pp. 418\u2013433, 2005.", "Y. Furukawa and J. Ponce, \u201cAccurate, dense, and robust multiview stereopsis,\u201d PAMI, 2010.", "J. L. Sch\u00a8onberger, E. Zheng, J.-M. Frahm, and M. Pollefeys, \u201cPixel-wise view selection for unstructured multi-view stereo,\u201d in ECCV, 2016, pp. 501\u2013518.", "Q. Xu and W. Tao, \u201cPlanar prior assisted patchmatch multi-view stereo,\u201d in AAAI, 2020, pp. 12 516\u201312 523.", "Q. Xu, W. Kong, W. Tao, and M. Pollefeys, \u201cMulti-scale geometric consistency guided and planar prior assisted multi-view stereo,\u201d PAMI, 2022.", "C. Barnes, E. Shechtman, A. Finkelstein, and D. B. Goldman, \u201cPatchmatch: A randomized correspondence algorithm for structural image editing,\u201d ACM Trans. Graph., vol. 28, no. 3, p. 24, 2009.", "Y. Wang, Z. Zeng, T. Guan, W. Yang, Z. Chen, W. Liu, L. Xu, and Y. Luo, \u201cAdaptive patch deformation for textureless-resilient multi-view stereo,\u201d in CVPR, 2023, pp. 1621\u20131630.", "H. Aan\u00e6s, R. R. Jensen, G. Vogiatzis, E. Tola, and A. B. Dahl, \u201cLarge-scale data for multiple-view stereopsis,\u201d IJCV, 2016.", "Y. Yao, Z. Luo, S. Li, J. Zhang, Y. Ren, L. Zhou, T. Fang, and L. Quan, \u201cBlendedmvs: A large-scale dataset for generalized multi-view stereo networks,\u201d in CVPR, 2020, pp. 1790\u20131799.", "R. T. Collins, \u201cA space-sweep approach to true multi-image matching,\u201d in CVPR, 1996, pp. 358\u2013363.", "O. Ronneberger, P. Fischer, and T. Brox, \u201cU-net: Convolutional networks for biomedical image segmentation,\u201d in International Conference on Medical image computing and computer-assisted intervention. Springer, 2015, pp. 234\u2013241.", "J. Yan, Z. Wei, H. Yi, M. Ding, R. Zhang, Y. Chen, G. Wang, and Y.-W. Tai, \u201cDense hybrid recurrent multi-view stereo net with dynamic consistency checking,\u201d in ECCV. Springer, 2020, pp. 674\u2013689.", "Q. Xu and W. Tao, \u201cPVSNet: Pixelwise visibility-aware multi-view stereo network,\u201d ArXiv, 2020.", "J. Zhang, Y. Yao, S. Li, Z. Luo, and T. Fang, \u201cVisibility-aware multi-view stereo network,\u201d in BMVC, 2020.", "X. Ma, Y. Gong, Q. Wang, J. Huang, L. Chen, and F. Yu, \u201cEppmvsnet: Epipolar-assembling based depth prediction for multi-view stereo,\u201d in ICCV, 2021, pp. 5732\u20135740.", "Q. Xu, W. Su, Y. Qi, W. Tao, and M. Pollefeys, \u201cLearning inverse depth regression for pixelwise visibility-aware multi-view stereo networks,\u201d IJCV, vol. 130, no. 8, pp. 2040\u20132059, 2022.", "X. Wang, Z. Zhu, G. Huang, F. Qin, Y. Ye, Y. He, X. Chi, and X. Wang, \u201cMvster: epipolar transformer for efficient multi-view stereo,\u201d in ECCV, 2022, pp. 573\u2013591.", "K. Cho, B. V. Merri\u00a8enboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, and Y. Bengio, \u201cLearning phrase representations using RNN encoder-decoder for statistical machine translation,\u201d arXiv preprint arXiv:1406.1078, 2014.", "S. Wang, B. Li, and Y. Dai, \u201cEfficient multi-view stereo by iterative dynamic cost volume,\u201d in CVPR, 2022, pp. 8655\u20138664.", "Y. Song and S. Ermon, \u201cGenerative modeling by estimating gradients of the data distribution,\u201d Advances in neural information processing systems, vol. 32, 2019.", "P. Dhariwal and A. Nichol, \u201cDiffusion models beat gans on image synthesis,\u201d Advances in Neural Information Processing Systems, vol. 34, pp. 8780\u20138794, 2021.", "J. Ho, T. Salimans, A. Gritsenko, W. Chan, M. Norouzi, and D. J. Fleet, \u201cVideo diffusion models,\u201d NeurIPS, vol. 35, pp. 8633\u20138646, 2022.", "Y. Duan, X. Guo, and Z. Zhu, \u201cDiffusiondepth: Diffusion denoising approach for monocular depth estimation,\u201d arXiv preprint arXiv:2303.05021, 2023.", "Y. Ji, Z. Chen, E. Xie, L. Hong, X. Liu, Z. Liu, T. Lu, Z. Li, and P. Luo, \u201cDdp: Diffusion model for dense visual prediction,\u201d arXiv preprint arXiv:2303.17559, 2023.", "B. Ke, A. Obukhov, S. Huang, N. Metzger, R. C. Daudt, and K. Schindler, \u201cRepurposing diffusion-based image generators for monocular depth estimation,\u201d in CVPR, 2024, pp. 9492\u20139502.", "X. Fu, W. Yin, M. Hu, K. Wang, Y. Ma, P. Tan, S. Shen, D. Lin, and X. Long, \u201cGeowizard: Unleashing the diffusion priors for 3d geometry estimation from a single image,\u201d in ECCV. Springer, 2024, pp. 241\u2013258.", "C. Ye, L. Qiu, X. Gu, Q. Zuo, Y. Wu, Z. Dong, L. Bo, Y. Xiu, and X. Han, \u201cStablenormal: Reducing diffusion variance for stable and sharp normal,\u201d ACM Transactions on Graphics (TOG), vol. 43, no. 6, pp. 1\u201318, 2024.", "H. Guo, H. Zhu, S. Peng, H. Lin, Y. Yan, T. Xie, W. Wang, X. Zhou, and H. Bao, \u201cMulti-view reconstruction via sfm-guided monocular depth estimation,\u201d in CVPR, 2025.", "J. Nam, G. Lee, S. Kim, H. Kim, H. Cho, S. Kim, and S. Kim, \u201cDiffusion model for dense matching,\u201d in The Twelfth International Conference on Learning Representations, 2023.", "J. Wang, C. Rupprecht, and D. Novotny, \u201cPosediffusion: Solving pose estimation via diffusion-aided bundle adjustment,\u201d in ICCV, 2023, pp. 9773\u20139783.", "J. Y. Zhang, A. Lin, M. Kumar, T.-H. Yang, D. Ramanan, and S. Tulsiani, \u201cCameras as rays: Pose estimation via ray diffusion,\u201d ICLR, 2024.", "Y. Lu, J. Zhang, T. Fang, J.-D. Nahmias, Y. Tsin, L. Quan, X. Cao, Y. Yao, and S. Li, \u201cMatrix3d: Large photogrammetry model all-in-one,\u201d CVPR, 2025.", "S. Saxena, C. Herrmann, J. Hur, A. Kar, M. Norouzi, D. Sun, and D. J. Fleet, \u201cThe surprising effectiveness of diffusion models for optical flow and monocular depth estimation,\u201d NeurIPS, vol. 36, pp. 39 443\u201339 469, 2023.", "Q. Dong, B. Zhao, and Y. Fu, \u201cOpen-ddvm: A reproduction and extension of diffusion model for optical flow estimation,\u201d arXiv preprint arXiv:2312.01746, 2023.", "M.-G. Park and K.-J. Yoon, \u201cLearning and selecting confidence measures for robust stereo matching,\u201d IEEE transactions on pattern analysis and machine intelligence, vol. 41, no. 6, pp. 1397\u20131411, 2018.", "M. Poggi and S. Mattoccia, \u201cLearning from scratch a confidence measure.\u201d in BMVC, 2016.", "R. Haeusler, R. Nair, and D. Kondermann, \u201cEnsemble learning for confidence measures in stereo vision,\u201d in CVPR, 2013, pp. 305\u2013312.", "F. Tosi, M. Poggi, A. Benincasa, and S. Mattoccia, \u201cBeyond local reasoning for stereo confidence estimation with deep learning,\u201d in ECCV, 2018, pp. 319\u2013334.", "A. Shaked and L. Wolf, \u201cImproved stereo matching with constant highway networks and reflective confidence learning,\u201d in CVPR, 2017, pp. 4641\u20134650.", "S. Kim, S. Kim, D. Min, and K. Sohn, \u201cLaf-net: Locally adaptive fusion networks for stereo confidence estimation,\u201d in CVPR, 2019, pp. 205\u2013214.", "W. Su, Q. Xu, and W. Tao, \u201cUncertainty guided multi-view stereo network for depth estimation,\u201d IEEE Transactions on Circuits and Systems for Video Technology, vol. 32, no. 11, pp. 7796\u20137808, 2022.", "S. Saxena, A. Kar, M. Norouzi, and D. J. Fleet, \u201cMonocular depth estimation using diffusion models,\u201d arXiv preprint arXiv:2302.14816, 2023.", "L. Karazija, I. Laina, A. Vedaldi, and C. Rupprecht, \u201cDiffusion models for zero-shot open-vocabulary segmentation,\u201d arXiv preprint arXiv:2306.09316, 2023.", "J. Liu, G. Wang, W. Ye, C. Jiang, J. Han, Z. Liu, G. Zhang, D. Du, and H. Wang, \u201cDifflow3d: Toward robust uncertainty-aware scene flow estimation with diffusion model,\u201d arXiv preprint arXiv:2311.17456, 2023.", "J. Song, C. Meng, and S. Ermon, \u201cDenoising diffusion implicit models,\u201d arXiv preprint arXiv:2010.02502, 2020.", "S. Wang, V. Leroy, Y. Cabon, B. Chidlovskii, and J. Revaud, \u201cDust3r: Geometric 3d vision made easy,\u201d in CVPR, 2024, pp. 20 697\u201320 709.", "J. Ho and T. Salimans, \u201cClassifier-free diffusion guidance,\u201d arXiv preprint arXiv:2207.12598, 2022.", "T.-Y. Lin, P. Doll\u00b4ar, R. B. Girshick, K. He, B. Hariharan, and S. J. Belongie, \u201cFeature pyramid networks for object detection,\u201d CVPR, 2017.", "L. Lipson, Z. Teed, and J. Deng, \u201cRaft-stereo: Multilevel recurrent field transforms for stereo matching,\u201d arXiv preprint arXiv:2109.07547, 2021.", "Z. Ma, Z. Teed, and J. Deng, \u201cMultiview stereo with cascaded epipolar raft,\u201d arXiv preprint arXiv:2205.04502, 2022.", "M. Ji, J. Gall, H. Zheng, Y. Liu, and L. Fang, \u201cSurfacenet: An end-to-end 3D neural network for multiview stereopsis,\u201d in ICCV, 2017.", "A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. K\u00a8opf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala, \u201cPytorch: An imperative style, high-performance deep learning library,\u201d ArXiv, 2019.", "D. P. Kingma and J. Ba, \u201cAdam: A method for stochastic optimization.\u201d in ICLR, 2015.", "J. Chang, J. He, T. Zhang, J. Yu, and F. Wu, \u201cEi-mvsnet: Epipolar-guided multi-view stereo network with interval-aware label,\u201d TIP, vol. 33, pp. 753\u2013766, 2024.", "S. Wang, B. Li, and Y. Dai, \u201cEfficient multi-view stereo by dynamic cost volume and cross-scale propagation,\u201d CSVT, 2024.", "V. K. Vats, S. Joshi, D. J. Crandall, M. A. Reza, and S.-h. Jung, \u201cGcmvsnet: Multi-view, multi-scale, geometrically-consistent multi-view stereo,\u201d in Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, 2024, pp. 3242\u20133252.", "W. Su and W. Tao, \u201cContext-aware multi-view stereo network for efficient edge-preserving depth estimation,\u201d IJCV, pp. 1\u201325, 2025."], "sub_categories": ["Multi-View Stereo", "Diffusion Models for Depth Refinement", "Confidence-Aware Sampling"]}
{"doc_id": "2509.15219v1", "title": "Out-of-Sight Trajectories: Tracking, Fusion, and Prediction", "abstract": "Trajectory prediction is a critical task in computer vision and autonomous systems, playing a key role in autonomous driving, robotics, surveillance, and virtual reality. Existing methods often rely on complete and noise-free observational data, overlooking the challenges associated with out-of-sight objects and the inherent noise in sensor data caused by limited camera coverage, obstructions, and the absence of ground truth for denoised trajectories. These limitations pose safety risks and hinder reliable prediction in real-world scenarios. In this extended work, we present advancements in Out-of-Sight Trajectory (OST), a novel task that predicts the noise-free visual trajectories of out-of-sight objects using noisy sensor data. Building on our previous research, we broaden the scope of Out-of-Sight Trajectory Prediction (OOSTraj) to include pedestrians and vehicles, extending its applicability to autonomous driving, robotics, surveillance, and virtual reality. Our enhanced Vision-Positioning Denoising Module leverages camera calibration to establish a vision-positioning mapping, addressing the lack of visual references, while effectively denoising noisy sensor data in an unsupervised manner. Through extensive evaluations on the Vi-Fi and JRDB datasets, our approach achieves state-of-the-art performance in both trajectory denoising and prediction, significantly surpassing previous baselines. Additionally, we introduce comparisons with traditional denoising methods, such as Kalman filtering, and adapt recent trajectory prediction models to our task, providing a comprehensive benchmark. This work represents the first initiative to integrate vision-positioning projection for denoising noisy sensor trajectories of out-of-sight agents, paving the way for future advances. The code and preprocessed datasets are available at github.com/Hai-chao-Zhang/OST", "authors": ["Haichao Zhang", "Yi Xu", "Yun Fu"], "published_date": "2025-09-18T17:59:16Z", "url": "http://arxiv.org/abs/2509.15219v1", "pdf_url": "http://arxiv.org/pdf/2509.15219v1", "primary_category": "cs.CV", "references": ["Alexandre Alahi, Albert Haque, and Li Fei-Fei. Rgb-w: When vision meets wireless. In Proceedings of the IEEE International Conference on Computer Vision, pages 3289\u20133297, 2015.", "Peter B\u00a8uhlmann and Bin Yu. Boosting with the l 2 loss: regression and classification. Journal of the American Statistical Association, 98(462):324\u2013339, 2003.", "Bryan Bo Cao, Abrar Alali, Hansi Liu, Nicholas Meegan, Marco Gruteser, Kristin Dana, Ashwin Ashok, and Shubham Jain. Vitag: Online wifi fine time measurements aided vision-motion identity association in multi-person environments. In 2022 19th Annual IEEE International Conference on Sensing, Communication, and Networking (SECON), pages 19\u201327. IEEE, 2022.", "Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555, 2014.", "Corrado Di Natale, Eugenio Martinelli, and Arnaldo D\u2019Amico. Counteraction of environmental disturbances of electronic nose data by independent component analysis. Sensors and Actuators B: Chemical, 82(2-3):158\u2013165, 2002.", "Ryo Fujii, Jayakorn Vongkulbhisal, Ryo Hachiuma, and Hideo Saito. A two-block rnn-based trajectory prediction from incomplete trajectory. IEEE Access, 9:56140\u201356151, 2021.", "Roger Girgis, Florian Golemo, Felipe Codevilla, Martin Weiss, Jim Aldon D\u2019Souza, Samira Ebrahimi Kahou, Felix Heide, and Christopher Pal. Latent variable sequential set transformers for joint multi-agent motion prediction. In International Conference on Learning Representations, 2022.", "Francesco Giuliari, Irtiza Hasan, Marco Cristani, and Fabio Galasso. Transformer networks for trajectory forecasting. In 2020 25th international conference on pattern recognition (ICPR), pages 10335\u201310342. IEEE, 2021.", "Niels Haering, P\u00b4eter L Venetianer, and Alan Lipton. The evolution of video surveillance: an overview. Machine Vision and Applications, 19(5):279\u2013290, 2008.", "Sepp Hochreiter and J\u00a8urgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735\u20131780, 1997.", "Jihua Huang and H-S Tan. A low-order dgps-based vehicle positioning system under urban environment. IEEE/ASME Transactions on mechatronics, 11(5):567\u2013575, 2006.", "Neeraj Kumar Jain, RK Saini, and Preeti Mittal. A review on traffic monitoring system techniques. Soft computing: Theories and applications: Proceedings of SoCTA 2017, pages 569\u2013577, 2019.", "Rudolf Emil Kalman. A new approach to linear filtering and prediction problems. Transactions of the ASME\u2013Journal of Basic Engineering, 82:35\u201345, 1960.", "Hansi Liu, Abrar Alali, Mohamed Ibrahim, Bryan Bo Cao, Nicholas Meegan, Hongyu Li, Marco Gruteser, Shubham Jain, Kristin Dana, Ashwin Ashok, et al. Vi-fi: Associating moving subjects across vision and wireless sensors. In 2022 21st ACM/IEEE International Conference on Information Processing in Sensor Networks (IPSN), pages 208\u2013219. IEEE, 2022.", "Hansi Liu, Abrar Alali, Mohamed Ibrahim, Hongyu Li, Marco Gruteser, Shubham Jain, Kristin Dana, Ashwin Ashok, Bin Cheng, and Hongsheng Lu. Lost and found! associating target persons in camera surveillance footage with smartphone identifiers. In Proceedings of the 19th Annual International Conference on Mobile Systems, Applications, and Services, pages 499\u2013500, 2021.", "Hansi Liu, Hongsheng Lu, Kristin Data, and Marco Gruteser. Vifi-loc: Multi-modal pedestrian localization using gan with camera-phone correspondences. In Proceedings of the 25th International Conference on Multimodal Interaction, pages 661\u2013669, 2023.", "Sijia Liu, Sundeep Prabhakar Chepuri, Makan Fardad, Engin Mas\u00b8azade, Geert Leus, and Pramod K Varshney. Sensor selection for estimation with correlated measurement noise. IEEE Transactions on Signal Processing, 64(13):3509\u20133522, 2016.", "Yiheng Liu, Wengang Zhou, Mao Xi, Sanjing Shen, and Houqiang Li. Vision meets wireless positioning: Effective person re-identification with recurrent context propagation. In Proceedings of the 28th ACM International Conference on Multimedia, pages 1103\u20131111, 2020.", "Roberto Martin-Martin, Mihir Patel, Hamid Rezatofighi, Abhijeet Shenoi, JunYoung Gwak, Eric Frankel, Amir Sadeghian, and Silvio Savarese. JRDB: A dataset and benchmark of egocentric robot visual perception of humans in built environments. IEEE transactions on pattern analysis and machine intelligence, 2021.", "Nishant Nikhil and Brendan Tran Morris. Convolutional neural network for trajectory prediction. In Proceedings of the European Conference on Computer Vision (ECCV) Workshops, pages 0\u20130, 2018.", "Savvas Papaioannou, Hongkai Wen, Zhuoling Xiao, Andrew Markham, and Niki Trigoni. Accurate positioning via cross-modality training. In Proceedings of the 13th ACM Conference on Embedded Networked Sensor Systems, pages 239\u2013251, 2015.", "Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Mark Chen, Rewon Child, Vedant Misra, Pamela Mishkin, Gretchen Krueger, Sandhini Agarwal, and Ilya Sutskever. Dall\u00b7e: Creating images from text. https://openai.com/dall-e, 2021.", "Edoardo Mello Rella, Jan-Nico Zaech, Alexander Liniger, and Luc Van Gool. Decoder fusion rnn: Context and interaction aware decoders for trajectory prediction. In 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 5937\u20135943. IEEE, 2021.", "David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning internal representations by error propagation, parallel distributed processing, explorations in the microstructure of cognition, ed. de rumelhart and j. mcclelland. vol. 1. 1986. Biometrika, 71:599\u2013607, 1986.", "Zhiyuan Shi, Min Xu, Quan Pan, Bing Yan, and Haimin Zhang. Lstm-based flight trajectory prediction. In 2018 International Joint Conference on Neural Networks (IJCNN), pages 1\u20138. IEEE, 2018.", "Dennis Strelow, Jeffrey Mishler, David Koes, and Sanjiv Singh. Precise omnidirectional camera calibration. In Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. CVPR 2001, volume 1, pages I\u2013I. IEEE, 2001.", "Hung Tran, Vuong Le, and Truyen Tran. Goal-driven long-term trajectory prediction. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 796\u2013805, 2021.", "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.", "Yi Xu, Armin Bazarjani, Hyung-gun Chi, Chiho Choi, and Yun Fu. Uncovering the missing pattern: Unified framework towards trajectory imputation and prediction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9632\u20139643, 2023.", "Haichao Zhang, Yi Xu, Hongsheng Lu, Takayuki Shimizu, and Yun Fu. Layout sequence prediction from noisy mobile modality. In Proceedings of the 31st ACM International Conference on Multimedia, pages 3965\u20133974, 2023.", "Haichao Zhang, Yi Xu, Hongsheng Lu, Takayuki Shimizu, and Yun Fu. Oostraj: Out-of-sight trajectory prediction with vision-positioning denoising. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14802\u201314811, 2024.", "Hongsong Zhao, Lingjuan Miao, and Haijun Shao. Adaptive two-stage kalman filter for sins/odometer integrated navigation systems. The Journal of Navigation, 70(2):242\u2013261, 2017.", "Qihua Zhou, Song Guo, Jun Pan, Jiacheng Liang, Jingcai Guo, Zhenda Xu, and Jingren Zhou. Pass: Patch automatic skip scheme for efficient on-device video perception. IEEE Transactions on Pattern Analysis and Machine Intelligence, pages 1\u201318, 2024.", "Zikang Zhou, Luyao Ye, Jianping Wang, Kui Wu, and Kejie Lu. Hivt: Hierarchical vector transformer for multi-agent motion prediction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8823\u20138833, 2022."], "sub_categories": ["Out-of-Sight Trajectory Prediction", "Sensor Fusion", "Trajectory Denoising"]}
{"doc_id": "2509.15217v1", "title": "Generalizable Geometric Image Caption Synthesis", "abstract": "Multimodal large language models have various practical applications that demand strong reasoning abilities. Despite recent advancements, these models still struggle to solve complex geometric problems. A key challenge stems from the lack of high-quality image-text pair datasets for understanding geometric images. Furthermore, most template-based data synthesis pipelines typically fail to generalize to questions beyond their predefined templates. In this paper, we bridge this gap by introducing a complementary process of Reinforcement Learning with Verifiable Rewards (RLVR) into the data generation pipeline. By adopting RLVR to refine captions for geometric images synthesized from 50 basic geometric relations and using reward signals derived from mathematical problem-solving tasks, our pipeline successfully captures the key features of geometry problem-solving. This enables better task generalization and yields non-trivial improvements. Furthermore, even in out-of-distribution scenarios, the generated dataset enhances the general reasoning capabilities of multimodal large language models, yielding accuracy improvements of $2.8\\%\\text{-}4.8\\%$ in statistics, arithmetic, algebraic, and numerical tasks with non-geometric input images of MathVista and MathVerse, along with $2.4\\%\\text{-}3.9\\%$ improvements in Art, Design, Tech, and Engineering tasks in MMMU.", "authors": ["Yue Xin", "Wenyuan Wang", "Rui Pan", "Ruida Wang", "Howard Meng", "Renjie Pi", "Shizhe Diao", "Tong Zhang"], "published_date": "2025-09-18T17:59:11Z", "url": "http://arxiv.org/abs/2509.15217v1", "pdf_url": "http://arxiv.org/pdf/2509.15217v1", "primary_category": "cs.AI", "references": [], "sub_categories": ["Geometric Image Caption Synthesis", "Reinforcement Learning with Verifiable Rewards", "Multimodal Large Language Models"]}
{"doc_id": "2509.15212v1", "title": "RynnVLA-001: Using Human Demonstrations to Improve Robot Manipulation", "abstract": "This paper presents RynnVLA-001, a vision-language-action(VLA) model built upon large-scale video generative pretraining from human demonstrations. We propose a novel two-stage pretraining methodology. The first stage, Ego-Centric Video Generative Pretraining, trains an Image-to-Video model on 12M ego-centric manipulation videos to predict future frames conditioned on an initial frame and a language instruction. The second stage, Human-Centric Trajectory-Aware Modeling, extends this by jointly predicting future keypoint trajectories, thereby effectively bridging visual frame prediction with action prediction. Furthermore, to enhance action representation, we propose ActionVAE, a variational autoencoder that compresses sequences of actions into compact latent embeddings, reducing the complexity of the VLA output space. When finetuned on the same downstream robotics datasets, RynnVLA-001 achieves superior performance over state-of-the-art baselines, demonstrating that the proposed pretraining strategy provides a more effective initialization for VLA models.", "authors": ["Yuming Jiang", "Siteng Huang", "Shengke Xue", "Yaxi Zhao", "Jun Cen", "Sicong Leng", "Kehan Li", "Jiayan Guo", "Kexiang Wang", "Mingxiu Chen", "Fan Wang", "Deli Zhao", "Xin Li"], "published_date": "2025-09-18T17:58:02Z", "url": "http://arxiv.org/abs/2509.15212v1", "pdf_url": "http://arxiv.org/pdf/2509.15212v1", "primary_category": "cs.CV", "references": ["Yavary, Arhan Jain, Ashwin Balakrishna, Ayzaan Wahid, Ben Burgess-Limerick, Beomjoon Kim, Bernhard Sch\u00f6lkopf, Blake Wulfe, Brian Ichter, Cewu Lu, Charles Xu, Charlotte Le, Chelsea Finn, Chen Wang, Chenfeng Xu, Cheng Chi, Chenguang Huang, Christine Chan, Christopher Agia, Chuer Pan, Chuyuan Fu, Coline Devin, Danfei Xu, Daniel Morton, Danny Driess, Daphne Chen, Deepak Pathak, Dhruv Shah, Dieter B\u00fcchler, Dinesh Jayaraman, Dmitry Kalashnikov, Dorsa Sadigh, Edward Johns, Ethan Paul Foster, Fangchen Liu, Federico Ceola, Fei Xia, Feiyu Zhao, Freek Stulp, Gaoyue Zhou, Gaurav S. Sukhatme, Gautam Salhotra, Ge Yan, Gilbert Feng, Giulio Schiavi, Glen Berseth, Gregory Kahn, Guanzhi Wang, Hao Su, Haoshu Fang, Haochen Shi, Henghui Bao, Heni Ben Amor, Henrik I. Christensen, Hiroki Furuta, Homer Walke, Hongjie Fang, Huy Ha, Igor Mordatch, Ilija Radosavovic, Isabel Leal, Jacky Liang, Jad Abou-Chakra, Jaehyung Kim, Jaimyn Drake, Jan Peters, Jan Schneider, Jasmine Hsu, Jeannette Bohg, Jeffrey Bingham, Jeffrey Wu, Jensen Gao, Jiaheng Hu, Jiajun Wu, Jialin Wu, Jiankai Sun, Jianlan Luo, Jiayuan Gu, Jie Tan, Jihoon Oh, Jimmy Wu, Jingpei Lu, Jingyun Yang, Jitendra Malik, Jo\u00e3o Silv\u00e9rio, Joey Hejna, Jonathan Booher, Jonathan Tompson, Jonathan Yang, Jordi Salvador, Joseph J. Lim, Junhyek Han, Kaiyuan Wang, Kanishka Rao, Karl Pertsch, Karol Hausman, Keegan Go, Keerthana Gopalakrishnan, Ken Goldberg, Kendra Byrne, Kenneth Oslund, Kento Kawaharazuka, Kevin Black, Kevin Lin, Kevin Zhang, Kiana Ehsani, Kiran Lekkala, Kirsty Ellis, Krishan Rana, Krishnan Srinivasan, Kuan Fang, Kunal Pratap Singh, Kuo-Hao Zeng, Kyle Hatch, Kyle Hsu, Laurent Itti, Lawrence Yunliang Chen, Lerrel Pinto, Li Fei-Fei, Liam Tan, Linxi Jim Fan, Lionel Ott, Lisa Lee, Luca Weihs, Magnum Chen, Marion Lepert, Marius Memmel, Masayoshi Tomizuka, Masha Itkina, Mateo Guaman Castro, Max Spero, Maximilian Du, Michael Ahn, Michael C. Yip, Mingtong Zhang, Mingyu Ding, Minho Heo, Mohan Kumar Srirama, Mohit Sharma, Moo Jin Kim, Naoaki Kanazawa, Nicklas Hansen, Nicolas Heess, Nikhil J. Joshi, Niko S\u00fcnderhauf, Ning Liu, Norman Di Palo, Nur Muhammad (Mahi) Shafiullah, Oier Mees, Oliver Kroemer, Osbert Bastani, Pannag R. Sanketi, Patrick Tree Miller, Patrick Yin, Paul Wohlhart, Peng Xu, Peter David Fagan, Peter Mitrano, Pierre Sermanet, Pieter Abbeel, Priya Sundaresan, Qiuyu Chen, Quan Vuong, Rafael Rafailov, Ran Tian, Ria Doshi, Roberto Mart\u00edn-Mart\u00edn, Rohan Baijal, Rosario Scalise, Rose Hendrix, Roy Lin, Runjia Qian, Ruohan Zhang, Russell Mendonca, Rutav Shah, Ryan Hoque, Ryan Julian, Samuel Bustamante, Sean Kirmani, Sergey Levine, Shan Lin, Sherry Moore, Shikhar Bahl, Shivin Dass, Shubham D. Sonawani, Shuran Song, Sichun Xu, Siddhant Haldar, Siddharth Karamcheti, Simeon Adebola, Simon Guist, Soroush Nasiriany, Stefan Schaal, Stefan Welker, Stephen Tian, Subramanian Ramamoorthy, Sudeep Dasari, Suneel Belkhale, Sungjae Park, Suraj Nair, Suvir Mirchandani, Takayuki Osa, Tanmay Gupta, Tatsuya Harada, Tatsuya Matsushima, Ted Xiao, Thomas Kollar, Tianhe Yu, Tianli Ding, Todor Davchev, Tony Z. Zhao, Travis Armstrong, Trevor Darrell, Trinity Chung, Vidhi Jain, Vincent Vanhoucke, Wei Zhan, Wenxuan Zhou, Wolfram Burgard, Xi Chen, Xiaolong Wang, Xinghao Zhu, Xinyang Geng, Xiyuan Liu, Liangwei Xu, Xuanlin Li, Yao Lu, Yecheng Jason Ma, Yejin Kim, Yevgen Chebotar, Yifan Zhou, Yifeng Zhu, Yilin Wu, Ying Xu, Yixuan Wang, Yonatan Bisk, Yoonyoung Cho, Youngwoon Lee, Yuchen Cui, Yue Cao, Yueh-Hua Wu, Yujin Tang, Yuke Zhu, Yunchu Zhang, Yunfan Jiang, Yunshuang Li, Yunzhu Li, Yusuke Iwasawa, Yutaka Matsuo, Zehan Ma, Zhuo Xu, Zichen Jeff Cui, Zichen Zhang, and Zipeng Lin. Open X-Embodiment: robotic learning datasets and RT-X models : Open X-Embodiment collaboration. In IEEE International Conference on Robotics and Automation, pages 6892\u20136903, 2024.", "OpenAI. GPT-4o system card, 2024. https://openai.com/index/hello-gpt-4o/.", "OpenAI. Introducing GPT-4.1 in the API, 2025. https://openai.com/index/gpt-4-1/.", "Maxime Oquab, Timoth\u00e9e Darcet, Th\u00e9o Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. DINOv2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023.", "William Peebles and Saining Xie. Scalable diffusion models with transformers. In IEEE/CVF International Conference on Computer Vision, pages 4172\u20134182, 2023.", "Karl Pertsch, Kyle Stachowicz, Brian Ichter, Danny Driess, Suraj Nair, Quan Vuong, Oier Mees, Chelsea Finn, and Sergey Levine. FAST: efficient action tokenization for vision-language-action models. arXiv preprint arXiv:2501.09747, 2025.", "Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman R\u00e4dle, Chloe Rolland, Laura Gustafson, et al. SAM 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714, 2024.", "Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. LAION-5B: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems, pages 25278\u201325294, 2022.", "Yide Shentu, Philipp Wu, Aravind Rajeswaran, and Pieter Abbeel. From LLMs to actions: Latent codes as bridges in hierarchical robot control. In IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 8539\u20138546, 2024.", "Mustafa Shukor, Dana Aubakirova, Francesco Capuano, Pepijn Kooijmans, Steven Palma, Adil Zouitine, Michel Aractingi, Caroline Pascal, Martino Russi, Andr\u00e9s Marafioti, Simon Alibert, Matthieu Cord, Thomas Wolf, and R\u00e9mi Cad\u00e8ne. SmolVLA: A vision-language-action model for affordable and efficient robotics. arXiv preprint arXiv:2506.01844, 2025.", "Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024.", "Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. In Advances in Neural Information Processing Systems, pages 84839\u201384865, 2024.", "Yang Tian, Sizhe Yang, Jia Zeng, Ping Wang, Dahua Lin, Hao Dong, and Jiangmiao Pang. Predictive inverse dynamics models are scalable learners for robotic manipulation. In International Conference on Learning Representations, 2025.", "Michael Tschannen, Alexey Gritsenko, Xiao Wang, Muhammad Ferjad Naeem, Ibrahim Alabdulmohsin, Nikhil Parthasarathy, Talfan Evans, Lucas Beyer, Ye Xia, Basil Mustafa, et al. SigLIP 2: Multilingual vision-language encoders with improved semantic understanding, localization, and dense features. arXiv preprint arXiv:2502.14786, 2025.", "Homer Walke, Kevin Black, Abraham Lee, Moo Jin Kim, Max Du, Chongyi Zheng, Tony Zhao, Philippe Hansen-Estruch, Quan Vuong, Andre He, Vivek Myers, Kuan Fang, Chelsea Finn, and Sergey Levine. BridgeData V2: A dataset for robot learning at scale. In Conference on Robot Learning, 2023.", "Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-VL: Enhancing vision-language model\u2019s perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024a.", "Xiaofeng Wang, Kang Zhao, Feng Liu, Jiayu Wang, Guosheng Zhao, Xiaoyi Bao, Zheng Zhu, Yingya Zhang, and Xingang Wang. EgoVid-5M: A large-scale video-action dataset for egocentric video generation. arXiv preprint arXiv:2411.08380, 2024b.", "Yating Wang, Haoyi Zhu, Mingyu Liu, Jiange Yang, Hao-Shu Fang, and Tong He. VQ-VLA: Improving vision-language-action models via scaling vector-quantized action tokenizers. arXiv preprint arXiv:2507.01016, 2025.", "Junjie Wen, Yichen Zhu, Jinming Li, Zhibin Tang, Chaomin Shen, and Feifei Feng. DexVLA: vision-language model with plug-in diffusion expert for general robot control. arXiv preprint arXiv:2502.05855, 2025.", "Hongtao Wu, Ya Jing, Chilam Cheang, Guangzeng Chen, Jiafeng Xu, Xinghang Li, Minghuan Liu, Hang Li, and Tao Kong. Unleashing large-scale video generative pre-training for visual robot manipulation. In International Conference on Learning Representations, 2024.", "An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025.", "Zhendong Yang, Ailing Zeng, Chun Yuan, and Yu Li. Effective whole-body pose estimation with two-stages distillation. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4210\u20134220, 2023.", "Hongyin Zhang, Pengxiang Ding, Shangke Lyu, Ying Peng, and Donglin Wang. GEVRM: goal-expressive video generation model for robust visual manipulation. In International Conference on Learning Representations, 2025.", "Jianke Zhang, Yanjiang Guo, Xiaoyu Chen, Yen-Jen Wang, Yucheng Hu, Chengming Shi, and Jianyu Chen. HiRT: enhancing robotic control with hierarchical robot transformers. In Conference on Robot Learning, pages 933\u2013946, 2024.", "Qingqing Zhao, Yao Lu, Moo Jin Kim, Zipeng Fu, Zhuoyang Zhang, Yecheng Wu, Zhaoshuo Li, Qianli Ma, Song Han, Chelsea Finn, Ankur Handa, Tsung-Yi Lin, Gordon Wetzstein, Ming-Yu Liu, and Donglai Xiang. CoT-VLA: Visual chain-of-thought reasoning for vision-language-action models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1702\u20131713, 2025.", "Tony Z Zhao, Vikash Kumar, Sergey Levine, and Chelsea Finn. Learning fine-grained bimanual manipulation with low-cost hardware. arXiv preprint arXiv:2304.13705, 2023.", "Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. Open-Sora: democratizing efficient video production for all. arXiv preprint arXiv:2412.20404, 2024.", "Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, et al. InternVL3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025.", "Brianna Zitkovich, Tianhe Yu, Sichun Xu, Peng Xu, Ted Xiao, Fei Xia, Jialin Wu, Paul Wohlhart, Stefan Welker, Ayzaan Wahid, Quan Vuong, Vincent Vanhoucke, Huong T. Tran, Radu Soricut, Anikait Singh, Jaspiar Singh, Pierre Sermanet, Pannag R. Sanketi, Grecia Salazar, Michael S. Ryoo, Krista Reymann, Kanishka Rao, Karl Pertsch, Igor Mordatch, Henryk Michalewski, Yao Lu, Sergey Levine, Lisa Lee, Tsang-Wei Edward Lee, Isabel Leal, Yuheng Kuang, Dmitry Kalashnikov, Ryan Julian, Nikhil J. Joshi, Alex Irpan, Brian Ichter, Jasmine Hsu, Alexander Herzog, Karol Hausman, Keerthana Gopalakrishnan, Chuyuan Fu, Pete Florence, Chelsea Finn, Kumar Avinava Dubey, Danny Driess, Tianli Ding, Krzysztof Marcin Choromanski, Xi Chen, Yevgen Chebotar, Justice Carbajal, Noah Brown, Anthony Brohan, Montserrat Gonzalez Arenas, and Kehang Han. RT-2: vision-language-action models transfer web knowledge to robotic control. In Conference on Robot Learning, pages 2165\u20132183, 2023."], "sub_categories": ["Vision-Language-Action Models", "Ego-Centric Video Generative Pretraining", "Human Demonstration Learning"]}
{"doc_id": "2509.15208v1", "title": "Geometric Image Synchronization with Deep Watermarking", "abstract": "Synchronization is the task of estimating and inverting geometric transformations (e.g., crop, rotation) applied to an image. This work introduces SyncSeal, a bespoke watermarking method for robust image synchronization, which can be applied on top of existing watermarking methods to enhance their robustness against geometric transformations. It relies on an embedder network that imperceptibly alters images and an extractor network that predicts the geometric transformation to which the image was subjected. Both networks are end-to-end trained to minimize the error between the predicted and ground-truth parameters of the transformation, combined with a discriminator to maintain high perceptual quality. We experimentally validate our method on a wide variety of geometric and valuemetric transformations, demonstrating its effectiveness in accurately synchronizing images. We further show that our synchronization can effectively upgrade existing watermarking methods to withstand geometric transformations to which they were previously vulnerable.", "authors": ["Pierre Fernandez", "Tom\u00e1\u0161 Sou\u010dek", "Nikola Jovanovi\u0107", "Hady Elsahar", "Sylvestre-Alvise Rebuffi", "Valeriu Lacatusu", "Tuan Tran", "Alexandre Mourachko"], "published_date": "2025-09-18T17:56:54Z", "url": "http://arxiv.org/abs/2509.15208v1", "pdf_url": "http://arxiv.org/pdf/2509.15208v1", "primary_category": "cs.CV", "references": ["\u201cHidden: Hiding data with deep networks,\u201d in ECCV, 2018.", "Yuxin Wen, John Kirchenbauer, Jonas Geiping, and Tom Gold-stein, \u201cTree-ring watermarks: Fingerprints for diffusion im-ages that are invisible and robust,\u201d NeurIPS, 2023.", "Fahad Shamshad, Tameem Bakr, Yahia Salaheldin Shaaban,Noor Hazim Hussein, Karthik Nandakumar, and Nils Lukas,\u201cFirst-place solution to neurips 2024 invisible watermark re-moval challenge,\u201din Workshop on GenAI Watermarking(ICLR), 2025.", "Pierre Fernandez, Guillaume Couairon, Herv\u00b4e J\u00b4egou, MatthijsDouze, and Teddy Furon, \u201cThe stable signature: Rooting wa-termarks in latent diffusion models,\u201d ICCV, 2023.", "Nikola Jovanovi\u00b4c, Ismail Labiad, Tom\u00b4a\u02c7s Sou\u02c7cek, MartinVechev, and Pierre Fernandez, \u201cWatermarking autoregressiveimage generation,\u201d arXiv preprint arXiv:2506.16349, 2025.", "David G Lowe,\u201cDistinctive image features from scale-invariant keypoints,\u201d IJCV, 2004.", "Martin A Fischler and Robert C Bolles, \u201cRandom sample con-sensus: a paradigm for model fitting with applications to imageanalysis and automated cartography,\u201d Communications of theACM, vol. 24, no. 6, pp. 381\u2013395, 1981.", "Mahdi Ahmadi, Alireza Norouzi, Nader Karimi, ShadrokhSamavi, and Ali Emami,\u201cRedmark: Framework for resid-ual diffusion watermarking based on deep networks,\u201d ExpertSystems with Applications, 2020.", "Xiyang Luo, Ruohan Zhan, Huiwen Chang, Feng Yang, andPeyman Milanfar, \u201cDistortion agnostic deep watermarking,\u201din CVPR, 2020.", "Tu Bui, Shruti Agarwal, and John Collomosse, \u201cTrustmark:Universal watermarking for arbitrary resolution images,\u201d arXivpreprint arXiv:2311.18297, 2023.", "Xuanyu Zhang, Runyi Li, Jiwen Yu, Youmin Xu, Weiqi Li,and Jian Zhang, \u201cEditguard: Versatile image watermarking fortamper localization and copyright protection,\u201d in Proceedingsof the IEEE/CVF Conference on Computer Vision and PatternRecognition, 2024, pp. 11964\u201311974.", "Tom Sander, Pierre Fernandez, Alain Durmus, Teddy Furon,and Matthijs Douze, \u201cWatermark anything with localized mes-sages,\u201d ICLR, 2025.", "Pierre Fernandez, Hady Elsahar, I Zeki Yalniz, and AlexandreMourachko, \u201cVideo seal: Open and efficient video watermark-ing,\u201d arXiv preprint arXiv:2412.09492, 2024.", "Zhaoyang Jia, Han Fang, and Weiming Zhang, \u201cMbrs: En-hancing robustness of dnn-based watermarking by mini-batchof real and simulated jpeg compression,\u201d in ACM MultimediaConference, 2021, pp. 41\u201349.", "Rui Ma, Mengxi Guo, Yi Hou, Fan Yang, Yuan Li, Huizhu Jia,and Xiaodong Xie, \u201cTowards blind watermarking: Combininginvertible and non-invertible mechanisms,\u201d in ACM Multime-dia Conference, 2022, pp. 1532\u20131542.", "Zijin Yang, Kai Zeng, Kejiang Chen, Han Fang, Weim-ing Zhang, and Nenghai Yu,\u201cGaussian shading: Provableperformance-lossless image watermarking for diffusion mod-els,\u201d in CVPR, 2024.", "Hengchang Guo, Qilong Zhang, Junwei Luo, Feng Guo, Wen-bin Zhang, Xiaodong Su, and Minglei Li,\u201cPractical deepdispersed watermarking with synchronization and fusion,\u201d inACM Multimedia Conference, 2023, pp. 7922\u20137932.", "Ke Wang, Shaowu Wu, Xiaolin Yin, Wei Lu, Xiangyang Luo,and Rui Yang,\u201cRobust image watermarking with synchro-nization using template enhanced-extracted network,\u201d IEEE TCSVT, 2024.", "Xiyang Luo, Michael Goebel, Elnaz Barshan, and Feng Yang,\u201cLeca: A learned approach for efficient cover-agnostic water-marking,\u201d arXiv preprint arXiv:2206.10813, 2022.", "Xiaohui Zhang, Weisi Lin, and Ping Xue, \u201cJust-noticeable dif-ference estimation with pixels in images,\u201d JVCIR, vol. 19, no.1, pp. 30\u201341, 2008.", "Robin Rombach, Andreas Blattmann, Dominik Lorenz, PatrickEsser, and Bj\u00a8orn Ommer, \u201cHigh-resolution image synthesiswith latent diffusion models,\u201d in CVPR, 2022.", "Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, JayWhang, Emily L Denton, Kamyar Ghasemipour, Raphael Gon-tijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al., \u201cPho-torealistic text-to-image diffusion models with deep languageunderstanding,\u201d NeurIPS, 2022.", "Sanghyun Woo, Shoubhik Debnath, Ronghang Hu, XinleiChen, Zhuang Liu, In So Kweon, and Saining Xie, \u201cConvnextv2: Co-designing and scaling convnets with masked autoen-coders,\u201d in CVPR, 2023.", "Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-head, Alexander C Berg, Wan-Yen Lo, et al., \u201cSegment any-thing,\u201d in ICCV, 2023.", "G. Bradski, \u201cThe OpenCV Library,\u201d Dr. Dobb\u2019s Journal ofSoftware Tools, 2000."], "sub_categories": ["Image Synchronization", "Deep Watermarking", "Geometric Transformation Robustness"]}
{"doc_id": "2509.15207v1", "title": "FlowRL: Matching Reward Distributions for LLM Reasoning", "abstract": "We propose FlowRL: matching the full reward distribution via flow balancing instead of maximizing rewards in large language model (LLM) reinforcement learning (RL). Recent advanced reasoning models adopt reward-maximizing methods (\\eg, PPO and GRPO), which tend to over-optimize dominant reward signals while neglecting less frequent but valid reasoning paths, thus reducing diversity. In contrast, we transform scalar rewards into a normalized target distribution using a learnable partition function, and then minimize the reverse KL divergence between the policy and the target distribution. We implement this idea as a flow-balanced optimization method that promotes diverse exploration and generalizable reasoning trajectories. We conduct experiments on math and code reasoning tasks: FlowRL achieves a significant average improvement of $10.0\\%$ over GRPO and $5.1\\%$ over PPO on math benchmarks, and performs consistently better on code reasoning tasks. These results highlight reward distribution-matching as a key step toward efficient exploration and diverse reasoning in LLM reinforcement learning.", "authors": ["Xuekai Zhu", "Daixuan Cheng", "Dinghuai Zhang", "Hengli Li", "Kaiyan Zhang", "Che Jiang", "Youbang Sun", "Ermo Hua", "Yuxin Zuo", "Xingtai Lv", "Qizheng Zhang", "Lin Chen", "Fanghao Shao", "Bo Xue", "Yunchong Song", "Zhenjie Yang", "Ganqu Cui", "Ning Ding", "Jianfeng Gao", "Xiaodong Liu", "Bowen Zhou", "Hongyuan Mei", "Zhouhan Lin"], "published_date": "2025-09-18T17:56:36Z", "url": "http://arxiv.org/abs/2509.15207v1", "pdf_url": "http://arxiv.org/pdf/2509.15207v1", "primary_category": "cs.LG", "references": ["Bartoldson et al., 2025", "Bengio et al., 2023b", "Lee et al., 2024", "Deleu et al., 2024", "Mohammadpour et al., 2024", "He et al., 2025", "Madan et al., 2023", "Bengio et al., 2023b", "Malkin et al., 2022", "Madan et al., 2023", "Cretu et al., 2024", "Liu et al., 2025b", "Zhang et al., 2025a", "Hu et al., 2024", "Yu et al., 2025a"], "sub_categories": ["LLM Reinforcement Learning", "Reward Distribution Matching", "Flow-Balanced Optimization"]}
{"doc_id": "2509.15205v1", "title": "Voyager: An End-to-End Framework for Design-Space Exploration and   Generation of DNN Accelerators", "abstract": "While deep neural networks (DNNs) have achieved state-of-the-art performance in fields from computer vision to natural language processing, efficiently running these computationally demanding models requires hardware accelerators. However, designing these accelerators is a time-consuming, labor-intensive process that does not scale well. While prior efforts have sought to automate DNN accelerator generation, they offer limited parameterization, cannot produce high-performance, tapeout-ready designs, provide limited support for datatypes and quantization schemes, and lack an integrated, end-to-end software compiler. This work proposes Voyager, a high-level synthesis (HLS)-based framework for design space exploration (DSE) and generation of DNN accelerators. Voyager overcomes the limitations of prior work by offering extensive configurability across technology nodes, clock frequencies, and scales, with customizable parameters such as number of processing elements, on-chip buffer sizes, and external memory bandwidth. Voyager supports a wider variety of datatypes and quantization schemes versus prior work, including both built-in floating-point, posit and integer formats, as well as user-defined formats with both per-tensor scaling and microscaling quantization. Voyager's PyTorch-based compiler efficiently maps networks end-to-end on the generated hardware, with support for quantization, fusion, and tiling. We evaluate Voyager on state-of-the-art vision and language models. Voyager enables fast DSE with full-dataset accuracy evaluation for datatypes and quantization schemes. Generated designs achieve a high utilization across models and scales, up to 99.8%, and outperform prior generators with up to 61% lower latency and 56% lower area. Compared to hand-optimized accelerators, Voyager achieves comparable performance, while offering much greater automation in design and workload mapping.", "authors": ["Kartik Prabhu", "Jeffrey Yu", "Xinyuan Allen Pan", "Zhouhua Xie", "Abigail Aleshire", "Zihan Chen", "Ammar Ali Ratnani", "Priyanka Raina"], "published_date": "2025-09-18T17:56:15Z", "url": "http://arxiv.org/abs/2509.15205v1", "pdf_url": "http://arxiv.org/pdf/2509.15205v1", "primary_category": "cs.AR", "references": [], "sub_categories": ["DNN Accelerator Generation", "High-Level Synthesis", "Design Space Exploration"]}
{"doc_id": "2509.15204v1", "title": "Circuit-based chatacterization of finite-temperature quantum phases and   self-correcting quantum memory", "abstract": "Quantum phases at zero temperature can be characterized as equivalence classes under local unitary transformations: two ground states within a gapped phase can be transformed into each other via a local unitary circuit. We generalize this circuit-based characterization of phases to systems at finite-temperature thermal equilibrium described by Gibbs states. We construct a channel circuit that approximately transforms one Gibbs state into another provided the two are connected by a path in parameter space along which a certain correlation-decay condition holds. For finite-dimensional systems of linear size $L$ and approximation error $\\epsilon$, the locality of the circuit is ${\\rm polylog}({\\rm poly}(L)/\\epsilon)$. The correlation-decay condition, which we specify, is expected to be satisfied in the interior of many noncritical thermal phases, including those displaying discrete symmetry breaking and topological order. As an application, we show that any system in the same thermal phase as a zero-temperature topological code coherently preserves quantum information for a macroscopically long time, establishing self-correction as a universal property of thermal phases. As part of the proof, we provide explicit encoding and decoding channel circuits to encode information into, and decode it from, a system in thermal equilibrium.", "authors": ["Ruochen Ma", "Vedika Khemani", "Shengqi Sang"], "published_date": "2025-09-18T17:55:15Z", "url": "http://arxiv.org/abs/2509.15204v1", "pdf_url": "http://arxiv.org/pdf/2509.15204v1", "primary_category": "quant-ph", "references": [], "sub_categories": ["Finite-Temperature Quantum Phases", "Self-Correcting Quantum Memory", "Circuit-Based Characterization"]}
{"doc_id": "2509.15185v1", "title": "Understand Before You Generate: Self-Guided Training for Autoregressive   Image Generation", "abstract": "Recent studies have demonstrated the importance of high-quality visual representations in image generation and have highlighted the limitations of generative models in image understanding. As a generative paradigm originally designed for natural language, autoregressive models face similar challenges. In this work, we present the first systematic investigation into the mechanisms of applying the next-token prediction paradigm to the visual domain. We identify three key properties that hinder the learning of high-level visual semantics: local and conditional dependence, inter-step semantic inconsistency, and spatial invariance deficiency. We show that these issues can be effectively addressed by introducing self-supervised objectives during training, leading to a novel training framework, Self-guided Training for AutoRegressive models (ST-AR). Without relying on pre-trained representation models, ST-AR significantly enhances the image understanding ability of autoregressive models and leads to improved generation quality. Specifically, ST-AR brings approximately 42% FID improvement for LlamaGen-L and 49% FID improvement for LlamaGen-XL, while maintaining the same sampling strategy.", "authors": ["Xiaoyu Yue", "Zidong Wang", "Yuqing Wang", "Wenlong Zhang", "Xihui Liu", "Wanli Ouyang", "Lei Bai", "Luping Zhou"], "published_date": "2025-09-18T17:47:40Z", "url": "http://arxiv.org/abs/2509.15185v1", "pdf_url": "http://arxiv.org/pdf/2509.15185v1", "primary_category": "cs.CV", "references": ["Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 12873\u201312883, 2021.", "Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information processing systems, 27, 2014.", "Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications of the ACM, 63(11): 139\u2013144, 2020.", "Jean-Bastien Grill, Florian Strub, Florent Altch\u00e9, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. Advances in neural information processing systems, 33:21271\u201321284, 2020.", "Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. arXiv preprint arXiv:1911.05722, 2019.", "Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00e1r, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 16000\u201316009, 2022.", "Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems, 30, 2017.", "Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of naacL-HLT, volume 1. Minneapolis, Minnesota, 2019.", "Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han. Autoregressive image generation using residual quantization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11523\u201311532, 2022.", "Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vector quantization. arXiv preprint arXiv:2406.11838, 2024.", "Xiang Li, Kai Qiu, Hao Chen, Jason Kuen, Jiuxiang Gu, Bhiksha Raj, and Zhe Lin. Imagefolder: Autoregressive image generation with folded tokens. arXiv preprint arXiv:2410.01756, 2024.", "Nanye Ma, Mark Goldstein, Michael S Albergo, Nicholas M Boffi, Eric Vanden-Eijnden, and Saining Xie. Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers. arXiv preprint arXiv:2401.08740, 2024.", "Yatian Pang, Peng Jin, Shuo Yang, Bin Lin, Bin Zhu, Zhenyu Tang, Liuhan Chen, Francis EH Tay, Ser-Nam Lim, Harry Yang, et al. Next patch prediction for autoregressive visual generation. arXiv preprint arXiv:2412.15321, 2024.", "Namuk Park, Wonjae Kim, Byeongho Heo, Taekyung Kim, and Sangdoo Yun. What do self-supervised vision transformers learn? arXiv preprint arXiv:2305.00729, 2023.", "William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4195\u20134205, 2023.", "Alec Radford. Improving language understanding by generative pre-training. 2018.", "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of machine learning research, 21(140):1\u201367, 2020.", "Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684\u201310695, 2022.", "Axel Sauer, Katja Schwarz, and Andreas Geiger. Stylegan-xl: Scaling stylegan to large diverse datasets. In ACM SIGGRAPH 2022 conference proceedings, pages 1\u201310, 2022.", "Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024.", "Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024.", "Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. arXiv preprint arXiv:2404.02905, 2024.", "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.", "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.", "Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024.", "Yuqing Wang, Shuhuai Ren, Zhijie Lin, Yujin Han, Haoyuan Guo, Zhenheng Yang, Difan Zou, Jiashi Feng, and Xihui Liu. Parallelized autoregressive visual generation. arXiv preprint arXiv:2412.15119, 2024.", "Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin Bao, Zhuliang Yao, Qi Dai, and Han Hu. Simmim: A simple framework for masked image modeling. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9653\u20139663, 2022.", "Sihyun Yu, Sangkyung Kwak, Huiwon Jang, Jongheon Jeong, Jonathan Huang, Jinwoo Shin, and Saining Xie. Representation alignment for generation: Training diffusion transformers is easier than you think. arXiv preprint arXiv:2410.06940, 2024.", "Xiaoyu Yue, Lei Bai, Meng Wei, Jiangmiao Pang, Xihui Liu, Luping Zhou, and Wanli Ouyang. Understanding masked autoencoders from a local contrastive perspective. arXiv preprint arXiv:2310.01994, 2023.", "Jinghao Zhou, Chen Wei, Huiyu Wang, Wei Shen, Cihang Xie, Alan Yuille, and Tao Kong. ibot: Image bert pre-training with online tokenizer. arXiv preprint arXiv:2111.07832, 2021."], "sub_categories": ["Autoregressive Image Generation", "Self-Guided Training", "Visual Semantic Learning"]}
{"doc_id": "2509.15181v1", "title": "Maize Seedling Detection Dataset (MSDD): A Curated High-Resolution RGB   Dataset for Seedling Maize Detection and Benchmarking with YOLOv9, YOLO11,   YOLOv12 and Faster-RCNN", "abstract": "Accurate maize seedling detection is crucial for precision agriculture, yet curated datasets remain scarce. We introduce MSDD, a high-quality aerial image dataset for maize seedling stand counting, with applications in early-season crop monitoring, yield prediction, and in-field management. Stand counting determines how many plants germinated, guiding timely decisions such as replanting or adjusting inputs. Traditional methods are labor-intensive and error-prone, while computer vision enables efficient, accurate detection. MSDD contains three classes-single, double, and triple plants-capturing diverse growth stages, planting setups, soil types, lighting conditions, camera angles, and densities, ensuring robustness for real-world use. Benchmarking shows detection is most reliable during V4-V6 stages and under nadir views. Among tested models, YOLO11 is fastest, while YOLOv9 yields the highest accuracy for single plants. Single plant detection achieves precision up to 0.984 and recall up to 0.873, but detecting doubles and triples remains difficult due to rarity and irregular appearance, often from planting errors. Class imbalance further reduces accuracy in multi-plant detection. Despite these challenges, YOLO11 maintains efficient inference at 35 ms per image, with an additional 120 ms for saving outputs. MSDD establishes a strong foundation for developing models that enhance stand counting, optimize resource allocation, and support real-time decision-making. This dataset marks a step toward automating agricultural monitoring and advancing precision agriculture.", "authors": ["Dewi Endah Kharismawati", "Toni Kazic"], "published_date": "2025-09-18T17:41:59Z", "url": "http://arxiv.org/abs/2509.15181v1", "pdf_url": "http://arxiv.org/pdf/2509.15181v1", "primary_category": "cs.CV", "references": ["A. J. Dougill, Y. Y. Gong, and C. Orfila, \u201cConservation agriculture affects grain and nutrient yields of maize (Zea Mays L.) and can impact food and nutrition security in sub-saharan Africa,\u201d Fron. Nutr., vol. 8, p. 804663, 2022.", "S. A. Tanumihardjo, L. McCulley, R. Roh, S. Lopez-Ridaura, N. Palacios-Rojas, and N. S. Gunaratna, \u201cMaize agro-food systems to ensure food and nutrition security in reference to the sustainable development goals,\u201d Glob. Food Sec., vol. 25, p. 10037, 2020.", "M. van Dijk, T. Morley, M. L. Rau, and Y. Saghai, \u201cA meta-analysis of projected global food demand and population at risk of hunger for the period 2010\u20132050,\u201d Nat. Food, vol. 2, p. 494\u2013501, 2021.", "J. Schmidhuber and F. N. Tubiello, \u201cGlobal food security under climate change,\u201d Proc. Natl. Acad. Sci. USA, vol. 104, pp. 19 703\u201319 708, 2007.", "M. Kumar, \u201cImpact of climate change on crop yield and role of model for achieving food security,\u201d Environ. Monit. Assess., vol. 188, p. 465, 2016.", "D. Tilman, C. Balzer, J. Hill, and B. L. Befort, \u201cGlobal food demand and the sustainable intensification of agriculture,\u201d Proc. Natl. Acad. Sci. USA, vol. 108, pp. 20 260\u201320 264, 2011.", "B. L. Bodirsky, S. Rolinski, A. Biewald, I. Weindl, A. Popp, and H. Lotze-Campen, \u201cGlobal food demand scenarios for the 21st century,\u201d PLoS One, vol. 10, p. e0139201, 2015.", "OECD-FAO, \u201cOECD-FAO agricultural outlook 2010\u20132019,\u201d Organization for Economic Cooperation and Development and U. N. Food and Agriculture Organization, http://www.oecd-ilibrary.org/agriculture-and-food/oecd-fao-agricultural-outlook-2010 agroutlook-2010-en., Tech. Rep., 2010.", "Q. Xiao, X. Bai, C. Zhang, and Y. He, \u201cAdvanced high-throughput plant phenotyping techniques for genome-wide association studies: A review,\u201d J. Adv. Res., vol. 35, pp. 215\u2013230, 2022.", "A. Atefi, Y. Ge, S. Pitla, and J. Schnable, \u201cRobotic technologies for high-throughput plant phenotyping: Contemporary reviews and future perspectives,\u201d Fron. Pl. Sci., vol. 12, p. 611940, 2021.", "T. Gill, S. K. Gill, D. K. Saini, Y. Chopra, J. P. de Koff, and K. S. Sandhu, \u201cA comprehensive review of high throughput phenotyping and machine learning for plant stress phenotyping,\u201d Phenomics, vol. 2, pp. 156\u2013183, 2022.", "D. E. Kharismawati, H. A. Akbarpour, R. Aktar, F. Bunyak, K. Palaniappan, and T. Kazic, \u201cCorNet: unsupervised deep homography estimation for agricultural aerial imagery,\u201d in 16th European Conference on Computer Vision 2020 (ECCV2020), V. Ferrari, B. Fisher, C. Schmid, and E. Trucco, Eds., 2020, pp. 402\u2013419. [Online]. Available: https://eccv2020.eu/", "C. Costa, U. Schurr, F. Loreto, P. Menesatti, and S. Carpentier, \u201cPlant phenotyping research trends, a science mapping approach,\u201d Fron. Pl. Sci., vol. 9, p. 1933, 2019.", "R. Aktar, D. E. Kharismawati, K. Palaniappan, H. Aliakbarpour, F. Bunyak, A. E. Stapleton, and T. Kazic, \u201cRobust mosaicking of maize fields from aerial imagery,\u201d Appl. Plant Sci., vol. 8, p. e11387, 2020.", "D. E. Kharismawati and T. Kazic, \u201cDroneZaic: a robust end-to-end pipeline for mosaicking freely flown aerial video of agricultural fields,\u201d The Plant Phenom. J., vol. 8, p. e70033, 2025.", "O. A. Montesinos-L\u00b4opez, A. Montesinos-L\u00b4opez, J. Crossa, F. H. Toledo, O. P\u00b4erez-Hern\u00b4andez, K. M. Eskridge, and J. Rutkoski, \u201cA genomic Bayesian multi-trait and multi-environment model,\u201d G3: Genes Genom. Genet., vol. 6, pp. 2725\u20132744, 2016.", "H. Pathak, C. Igathinathane, Z. Zhang, D. Archer, and J. Hendrickson, \u201cA review of unmanned aerial vehicle-based methods for plant stand count evaluation in row crops,\u201d Comput. Electron. Ag., vol. 198, p. 107064, 2022.", "D. E. Kharismawati, F. Bunyak, and T. Kazic, \u201cDeepMaizeCounter: Smarter stand counts for seedling maize from drone imagery with YOLOv9,\u201d Plant Phenom., p. (in preparation), 2025.", "D. G. Bullock, D. S. Bullock, E. D. Nafziger, T. A. Doerge, S. R. Paszkiewicz, P. R. Carter, and T. A. Peterson, \u201cDoes variable rate seeding of corn pay?\u201d Agron. J., vol. 90, pp. 830\u2013836, 1998.", "N. Heider, L. Gunreben, S. Z\u00a8urner, and M. Schieck, \u201cA survey of datasets for computer vision in agriculture,\u201d in 45. GIL-Jahrestagung, Digitale Infrastrukturen f\u00a8ur eine nachhaltige Land-, Forst-und Ern\u00a8ahrungswirtschaft. Bonn, Germany: Gesellschaft f\u00a8ur Informatik e.V., 2025, pp. 35\u201346.", "E. David, S. Madec, P. Sadeghi-Tehran, H. Aasen, B. Zheng, S. Liu, N. Kirchgessner, G. Ishikawa, K. Nagasawa, M. A. Badhon, C. Pozniak, B. de Solan, A. Hund, S. C. Chapman, F. Baret, I. Stavness, and W. Guo, \u201cGlobal wheat head detection (gwhd) dataset: A large and diverse dataset of high-resolution rgb-labelled images to develop and benchmark wheat head detection methods,\u201d Plant Phe., vol. 2020, p. 3521852, 2020.", "J. Chamorro-Padial, R. Garc\u00b4\u0131a, and R. Gil, \u201cA systematic review of open data in agriculture,\u201d Comput. Electron. Ag., vol. 219, p. 108775, 2024.", "S. Oh, A. Chang, A. Ashapure, J. Jung, N. Dube, M. Maeda, D. Gonzalez, and J. Landivar, \u201cPlant counting of cotton from uas imagery using deep learning-based object detection framework,\u201d Remote Sens., vol. 12, p. 2981, 2020.", "M. Ullah, F. Islam, and A. Bais, \u201cQuantifying consistency of crop establishment using a lightweight u-net deep learning architecture and image processing techniques,\u201d Comput. Electron. Ag., vol. 217, p. 108617, 2020.", "K. Khun, N. Tremblay, B. Panneton, P. Vigneault, E. Lord, F. Cavayas, and C. Codjia, \u201cUse of oblique RGB imagery and apparent surface area of plants for early estimation of above-ground corn biomass,\u201d Remote Sens., vol. 13, p. 4032, 2021.", "S. Varela, P. Dhodda, W. Hsu, P. V. Prasad, Y. Assefa, N. Peralta, T. Griffin, A. Sharda, A. Ferguson, and I. Ciampitti, \u201cEarly-season stand count determination in corn via integration of imagery from unmanned aerial systems (UAS) and supervised learning techniques,\u201d Remote Sens., vol. 10, p. 343, 2018.", "J. Rodriguez-Vazquez, M. Fernandez-Cortizas, D. Perez-Saura, M. Molina, and P. Campoy, \u201cOvercoming domain shift in neural networks for accurate plant counting in aerial images,\u201d Remote Sens., p. 1700, 2023.", "X. Wu, X. Fan, P. Luo, S. D. Choudhury, T. Tjahjadi, and C. Hu, \u201cFrom laboratory to field: Unsupervised domain adaptation for plant disease recognition in the wild,\u201d Plant Phe., vol. 5, p. 0038, 2023.", "M. A. Arshad, T. Jubery, J. Afful, A. Jignasu, A. Balu, B. Ganapathysubramanian, S. Sarkar, and A. Krishnamurthy, \u201cEvaluating NeRFs for 3D plant geometry reconstruction in field conditions,\u201d arXiv, p. 2402.10344, 2024.", "X. Xu, L. Wang, X. Liang, L. Zhou, Y. Chen, P. Feng, H. Yu, and Y. Ma, \u201cMaize seedling leave counting based on semi-supervised learning and uav rgb images,\u201d Sustainability, vol. 15, p. 9583, 2023.", "S. Katari, S. Venkatesh, C. Stewart, and S. Khanal, \u201cIntegrating automated labeling framework for enhancing deep learning models to count corn plants using uas imagery,\u201d Sensors, vol. 24, p. 6467, 2024.", "A. Cravero, S. Pardo, S. Sepulveda, and L. M. noz, \u201cChallenges to use machine learning in agricultural big data: A systematic literature review,\u201d Agronomy, vol. 12, p. 748, 2022.", "L. Waltz, S. Katari, C. Hong, A. Anup, J. Colbert, A. Potlapally, T. Dill, C. Porter, J. Engle, C. Stewart, H. Subramoni, S. Shearer, R. Machiraju, O. Ortez, L. Lindsey, A. Nandi, and S. Khanal, \u201cCyberinfrastructure for machine learning applications in agriculture: experiences, analysis, and vision,\u201d Fron. Art. Intell, vol. 7, p. 1496066, 2025.", "R. L. Nielsen, \u201cDetermining corn leaf stages,\u201d Purdue University, Tech. Rep., 2004. [Online]. Available: https://www.agry.purdue.edu/ext/corn/news/articles.04/VStageMethods-0515.pdf", "J. Redmon and A. Farhadi, \u201cYOLOv3: An incremental improvement,\u201d 2018.", "A. Bochkovskiy, C. Wang, and H. M. Liao, \u201cYOLOv4: Optimal speed and accuracy of object detection,\u201d 2020.", "C. Wang, A. Bochkovskiy, and H. M. Liao, \u201cYOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors,\u201d arXiv, p. 2207.02696, 2022.", "C. Wang, I. Yeh, and H. M. Liao, \u201cYOLOv9: Learning what you want to learn using programmable gradient information,\u201d arXiv, 2024.", "S. Ren, K. He, R. Girshick, and J. Sun, \u201cFaster R-CNN: Towards real-time object detection with region proposal networks,\u201d 2016.", "G. Jocher, J. Qiu, and A. Chaurasia, \u201cUltralytics YOLO,\u201d 2023. [Online]. Available: https://github.com/ultralytics/ultralytics", "Y. Tian, Q. Ye, and D. Doermann, \u201cYOLOv12: Attention-centric real-time object detectors,\u201d University at Buffalo, Tech. Rep., 2025."], "sub_categories": ["Maize Seedling Detection", "High-Resolution RGB Dataset", "Object Detection Benchmarking"]}
{"doc_id": "2509.15178v1", "title": "Unleashing the Potential of Multimodal LLMs for Zero-Shot   Spatio-Temporal Video Grounding", "abstract": "Spatio-temporal video grounding (STVG) aims at localizing the spatio-temporal tube of a video, as specified by the input text query. In this paper, we utilize multimodal large language models (MLLMs) to explore a zero-shot solution in STVG. We reveal two key insights about MLLMs: (1) MLLMs tend to dynamically assign special tokens, referred to as \\textit{grounding tokens}, for grounding the text query; and (2) MLLMs often suffer from suboptimal grounding due to the inability to fully integrate the cues in the text query (\\textit{e.g.}, attributes, actions) for inference. Based on these insights, we propose a MLLM-based zero-shot framework for STVG, which includes novel decomposed spatio-temporal highlighting (DSTH) and temporal-augmented assembling (TAS) strategies to unleash the reasoning ability of MLLMs. The DSTH strategy first decouples the original query into attribute and action sub-queries for inquiring the existence of the target both spatially and temporally. It then uses a novel logit-guided re-attention (LRA) module to learn latent variables as spatial and temporal prompts, by regularizing token predictions for each sub-query. These prompts highlight attribute and action cues, respectively, directing the model's attention to reliable spatial and temporal related visual regions. In addition, as the spatial grounding by the attribute sub-query should be temporally consistent, we introduce the TAS strategy to assemble the predictions using the original video frames and the temporal-augmented frames as inputs to help improve temporal consistency. We evaluate our method on various MLLMs, and show that it outperforms SOTA methods on three common STVG benchmarks.   The code will be available at https://github.com/zaiquanyang/LLaVA_Next_STVG.", "authors": ["Zaiquan Yang", "Yuhao Liu", "Gerhard Hancke", "Rynson W. H. Lau"], "published_date": "2025-09-18T17:35:50Z", "url": "http://arxiv.org/abs/2509.15178v1", "pdf_url": "http://arxiv.org/pdf/2509.15178v1", "primary_category": "cs.CV", "references": ["Li, H., Chen, J., Wei, Z., Huang, S., Hui, T., Gao, J., Wei, X., Liu, S.: Llava-st: A multi-modal large language model for fine-grained spatial-temporal understanding. arXiv preprint arXiv:2501.08282 (2025)", "Li, M., Wang, H., Zhang, W., Miao, J., Zhao, Z., Zhang, S., Ji, W., Wu, F.: Winner: Weakly-supervised hierarchical decomposition and alignment for spatio-temporal video grounding. In: CVPR. pp. 23090\u201323099 (2023)", "Lin, B., Ye, Y., Zhu, B., Cui, J., Ning, M., Jin, P., Yuan, L.: Video-llava: Learning united visual representation by alignment before projection. In: EMNLP. pp. 5971\u20135984 (2024)", "Lin, Z., Tan, C., Hu, J.F., Jin, Z., Ye, T., Zheng, W.S.: Collaborative static and dynamic vision-language streams for spatio-temporal video grounding. In: CVPR. pp. 23100\u201323109 (2023)", "Liu, C., Ding, H., Jiang, X.: Gres: Generalized referring expression segmentation. In: CVPR. pp. 23592\u201323601 (2023)", "Liu, F., Liu, Y., Xu, K., Ye, S., Hancke, G.P., Lau, R.W.: Language-guided salient object ranking. In: Proceedings of the Computer Vision and Pattern Recognition Conference. pp. 29803\u201329813 (2025)", "Liu, H., Li, C., Wu, Q., Lee, Y.J.: Visual instruction tuning. In: Thirty-seventh Conference on Neural Information Processing Systems", "Liu, S., Zeng, Z., Ren, T., Li, F., Zhang, H., Yang, J., Jiang, Q., Li, C., Yang, J., Su, H., et al.: Grounding dino: Marrying dino with grounded pre-training for open-set object detection. In: ECCV. pp. 38\u201355. Springer (2024)", "Ma, C., Jiang, Y., Wu, J., Yuan, Z., Qi, X.: Groma: Localized visual tokenization for grounding multimodal large language models. In: ECCV. pp. 417\u2013435. Springer (2024)", "Munasinghe, S., Thushara, R., Maaz, M., Rasheed, H.A., Khan, S., Shah, M., Khan, F.: Pg-video-llava: Pixel grounding large video-language models. arXiv preprint arXiv:2311.13435 (2023)", "Prasad, A., Stengel-Eskin, E., Bansal, M.: Rephrase, augment, reason: Visual grounding of questions for vision-language models. arXiv preprint arXiv:2310.05861 (2023)", "Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from natural language supervision. In: ICML. pp. 8748\u20138763. PmLR (2021)", "Rasheed, H., Maaz, M., Shaji, S., Shaker, A., Khan, S., Cholakkal, H., Anwer, R.M., Xing, E., Yang, M.H., Khan, F.S.: Glamm: Pixel grounding large multimodal model. In: CVPR. pp. 13009\u201313018 (2024)", "Ravi, N., Gabeur, V., Hu, Y.T., Hu, R., Ryali, C., Ma, T., Khedr, H., R\u00e4dle, R., Rolland, C., Gustafson, L., et al.: Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714 (2024)", "Shtedritski, A., Rupprecht, C., Vedaldi, A.: What does clip know about a red circle? visual prompt engineering for vlms. In: ICCV. pp. 11987\u201311997 (2023)", "Shu, M., Nie, W., Huang, D.A., Yu, Z., Goldstein, T., Anandkumar, A., Xiao, C.: Test-time prompt tuning for zero-shot generalization in vision-language models. In: Advances in Neural Information Processing Systems (2022)", "Su, R., Yu, Q., Xu, D.: Stvgbert: A visual-linguistic transformer based framework for spatio-temporal video grounding. In: ICCV. pp. 1533\u20131542 (2021)", "Subramanian, S., Merrill, W., Darrell, T., Gardner, M., Singh, S., Rohrbach, A.: Reclip: A strong zero-shot baseline for referring expression comprehension. In: ACL. pp. 5198\u20135215 (2022)", "Tang, Z., Liao, Y., Liu, S., Li, G., Jin, X., Jiang, H., Yu, Q., Xu, D.: Human-centric spatio-temporal video grounding with visual transformers. TCSVT 32(12), 8238\u20138249 (2021)", "Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., J\u00e9gou, H.: Training data-efficient image transformers & distillation through attention. In: ICML. pp. 10347\u201310357. PMLR (2021)", "Wang, P., Bai, S., Tan, S., Wang, S., Fan, Z., Bai, J., Chen, K., Liu, X., Wang, J., Ge, W., et al.: Qwen2-vl: Enhancing vision-language model\u2019s perception of the world at any resolution. arXiv preprint arXiv:2409.12191 (2024)", "Wasim, S.T., Naseer, M., Khan, S., Yang, M.H., Khan, F.S.: Videogrounding-dino: Towards open-vocabulary spatio-temporal video grounding. In: CVPR. pp. 18909\u201318918 (2024)", "Wu, M., Cai, X., Ji, J., Li, J., Huang, O., Luo, G., Fei, H., JIANG, G., Sun, X., Ji, R.: Controlmllm: Training-free visual prompt learning for multimodal large language models. In: NeurIPS", "Xiao, J., Yao, A., Li, Y., Chua, T.S.: Can i trust your answer? visually grounded video question answering. In: CVPR. pp. 13204\u201313214 (2024)", "Yang, A., Miech, A., Sivic, J., Laptev, I., Schmid, C.: Tubedetr: Spatio-temporal video grounding with transformers. In: CVPR. pp. 16442\u201316453 (2022)", "Yang, Z., Liu, Y., Xu, W., Huang, C., Zhou, L., Tong, C.: Learning prototype via placeholder for zero-shot recognition. arXiv preprint arXiv:2207.14581 (2022)", "Yang, Z., Liu, Y., Lin, J., Hancke, G., Lau, R.W.: Boosting weakly-supervised referring image segmentation via progressive comprehension. arXiv preprint arXiv:2410.01544 (2024)", "Yang, Z., Wang, J., Tang, Y., Chen, K., Zhao, H., Torr, P.H.: Lavt: Language-aware vision transformer for referring image segmentation. In: CVPR. pp. 18155\u201318165 (2022)", "Yoon, H.S., Yoon, E., Tee, J.T.J., Hasegawa-Johnson, M.A., Li, Y., Yoo, C.D.: C-tpt: Calibrated test-time prompt tuning for vision-language models via text feature dispersion. In: ICLR", "Yuan, Y., Li, W., Liu, J., Tang, D., Luo, X., Qin, C., Zhang, L., Zhu, J.: Osprey: Pixel understanding with visual instruction tuning. In: CVPR. pp. 28202\u201328211 (2024)", "Zeng, S., Chang, X., Xie, M., Liu, X., Bai, Y., Pan, Z., Xu, M., Wei, X.: Futuresightdrive: Thinking visually with spatio-temporal cot for autonomous driving. arXiv preprint arXiv:2505.17685 (2025)", "Zhai, Y., Tong, S., Li, X., Cai, M., Qu, Q., Lee, Y.J., Ma, Y.: Investigating the catastrophic forgetting in multimodal large language models. arXiv preprint arXiv:2309.10313 (2023)", "Zhang, J., Khayatkhoei, M., Chhikara, P., Ilievski, F.: Mllms know where to look: Training-free perception of small visual details with multimodal llms. arXiv preprint arXiv:2502.17422 (2025)", "Zhang, J., Huang, J., Zhang, X., Shao, L., Lu, S.: Historical test-time prompt tuning for vision foundation models. In: NeurIPS", "Zhang, T., Li, X., Fei, H., Yuan, H., Wu, S., Ji, S., Loy, C.C., Shuicheng, Y.: Omg-llava: Bridging image-level, object-level, pixel-level reasoning and understanding. In: The Thirty-eighth Annual Conference on Neural Information Processing Systems", "Zhang, Y., Sun, P., Jiang, Y., Yu, D., Weng, F., Yuan, Z., Luo, P., Liu, W., Wang, X.: Bytetrack: Multi-object tracking by associating every detection box. In: ECCV. pp. 1\u201321. Springer (2022)", "Zhang, Z., Zhao, Z., Zhao, Y., Wang, Q., Liu, H., Gao, L.: Where does it exist: Spatio-temporal video grounding for multi-form sentences. In: CVPR. pp. 10668\u201310677 (2020)", "Zhou, B., Khosla, A., Lapedriza, A., Oliva, A., Torralba, A.: Learning deep features for discriminative localization. In: CVPR. pp. 2921\u20132929 (2016)", "Zhou, C., Loy, C.C., Dai, B.: Extract free dense labels from clip. In: ECCV. pp. 696\u2013712. Springer (2022)", "Zhou, K., Yang, J., Loy, C.C., Liu, Z.: Learning to prompt for vision-language models. IJCV 130(9), 2337\u20132348 (2022)"], "sub_categories": ["Zero-Shot Spatio-Temporal Video Grounding", "Multimodal Large Language Models", "Spatio-Temporal Highlighting"]}
{"doc_id": "2509.15177v1", "title": "A Race Bias Free Face Aging Model for Reliable Kinship Verification", "abstract": "The age gap in kinship verification addresses the time difference between the photos of the parent and the child. Moreover, their same-age photos are often unavailable, and face aging models are racially biased, which impacts the likeness of photos. Therefore, we propose a face aging GAN model, RA-GAN, consisting of two new modules, RACEpSp and a feature mixer, to produce racially unbiased images. The unbiased synthesized photos are used in kinship verification to investigate the results of verifying same-age parent-child images. The experiments demonstrate that our RA-GAN outperforms SAM-GAN on an average of 13.14\\% across all age groups, and CUSP-GAN in the 60+ age group by 9.1\\% in terms of racial accuracy. Moreover, RA-GAN can preserve subjects' identities better than SAM-GAN and CUSP-GAN across all age groups. Additionally, we demonstrate that transforming parent and child images from the KinFaceW-I and KinFaceW-II datasets to the same age can enhance the verification accuracy across all age groups. The accuracy increases with our RA-GAN for the kinship relationships of father-son and father-daughter, mother-son, and mother-daughter, which are 5.22, 5.12, 1.63, and 0.41, respectively, on KinFaceW-I. Additionally, the accuracy for the relationships of father-daughter, father-son, and mother-son is 2.9, 0.39, and 1.6 on KinFaceW-II, respectively. The code is available at~\\href{https://github.com/bardiya2254kariminia/An-Age-Transformation-whitout-racial-bias-for-Kinship-verification}{Github}", "authors": ["Ali Nazari", "Bardiya Kariminia", "Mohsen Ebrahimi Moghaddam"], "published_date": "2025-09-18T17:34:20Z", "url": "http://arxiv.org/abs/2509.15177v1", "pdf_url": "http://arxiv.org/pdf/2509.15177v1", "primary_category": "cs.CV", "references": ["Ghatas, F.S., Hemayed, E.E.: Gankin: generating kin faces using disentangled gan. SN Applied Sciences 2, 1\u201310 (2020)", "Pernu\u02c7s, M., Bhatnagar, M., Samad, B., Singh, D., Peer, P., Dobri\u02c7sek, S., et al.: Childnet: Structural kinship face synthesis model with appearance control mechanisms. IEEE Access 11, 49971\u201349991 (2023)", "Li, H., Hou, X., Huang, Z., Shen, L.: Stylegene: Crossover and mutation of region-level facial genes for kinship face synthesis. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 20960\u201320969 (2023)", "Chiu, P.-Y., Wu, D.-J., Chu, P.-H., Hsu, C.-H., Chiu, H.-C., Wang, C.-Y., Chen, J.-C.: Styledit: A unified framework for diverse child and partner faces synthesis with style latent diffusion transformer. arXiv preprint arXiv:2412.10785 (2024)", "Emara, M.M., Farouk, M., Fakhr, M.W.: Parent gan: image generation model for creating parent\u2019s images using children\u2019s images. Multimedia Tools and Applications, 1\u201323 (2024)", "Kim, H., Kim, H., Shim, J., Hwang, E.: A robust kinship verification scheme using face age transformation. Computer Vision and Image Understanding 231, 103662 (2023)", "Liu, F., Li, Z., Yang, W., Xu, F.: Age-invariant adversarial feature learning for kinship verification. Mathematics 10(3), 480 (2022)", "Chandaliya, P.K., Nain, N.: Childgan: Face aging and rejuvenation to find missing children. Pattern Recognition 129, 108761 (2022)", "Grimmer, M., Ramachandra, R., Busch, C.: Deep face age progression: A survey. IEEE Access 9, 83376\u201383393 (2021)", "Ramanathan, N., Chellappa, R.: Modeling shape and textural variations in aging faces. In: 2008 8th IEEE International Conference on Automatic Face & Gesture Recognition, pp. 1\u20138 (2008). IEEE", "Tang, J., Li, Z., Lai, H., Zhang, L., Yan, S., et al.: Personalized age progression with bi-level aging dictionary learning. IEEE transactions on pattern analysis and machine intelligence 40(4), 905\u2013917 (2017)", "Georgopoulos, M., Panagakis, Y., Pantic, M.: Modeling of facial aging and kinship: A survey. Image and Vision Computing 80, 58\u201379 (2018)", "Guo, Y., Su, X., Yan, G., Zhu, Y., Lv, X.: Age transformation based on deep learning: a survey. Neural Computing and Applications 36(9), 4537\u20134561 (2024)", "Wang, W., Cui, Z., Yan, Y., Feng, J., Yan, S., Shu, X., Sebe, N.: Recurrent face aging. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2378\u20132386 (2016)", "Suo, J., Chen, X., Shan, S., Gao, W., Dai, Q.: A concatenational graph evolution aging model. IEEE transactions on pattern analysis and machine intelligence 34(11), 2083\u20132096 (2012)", "Suo, J., Zhu, S.-C., Shan, S., Chen, X.: A compositional and dynamic model for face aging. IEEE Transactions on Pattern Analysis and Machine Intelligence 32(3), 385\u2013401 (2009)", "Nhan Duong, C., Luu, K., Gia Quach, K., Bui, T.D.: Longitudinal face modeling via temporal deep restricted boltzmann machines. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 5772\u20135780 (2016)", "Nhan Duong, C., Gia Quach, K., Luu, K., Le, N., Savvides, M.: Temporal non-volume preserving approach to facial age-progression and age-invariant face recognition. In: Proceedings of the IEEE International Conference on Computer Vision, pp. 3735\u20133743 (2017)", "Isola, P., Zhu, J.-Y., Zhou, T., Efros, A.A.: Image-to-image translation with conditional adversarial networks. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1125\u20131134 (2017)", "Zhu, J.-Y., Park, T., Isola, P., Efros, A.A.: Unpaired image-to-image translation using cycle-consistent adversarial networks. In: Proceedings of the IEEE International Conference on Computer Vision, pp. 2223\u20132232 (2017)", "Choi, Y., Choi, M., Kim, M., Ha, J.-W., Kim, S., Choo, J.: Stargan: Unified generative adversarial networks for multi-domain image-to-image translation. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 8789\u20138797 (2018)", "Li, S., Lee, H.J.: Gfam: A gender-preserving face aging model for age imbalance data. Electronics 12(11), 2369 (2023)", "Antipov, G., Baccouche, M., Dugelay, J.-L.: Face aging with conditional generative adversarial networks. In: 2017 IEEE International Conference on Image Processing (ICIP), pp. 2089\u20132093 (2017). IEEE", "Makhmudkhujaev, F., Hong, S., Park, I.K.: Re-aging gan: Toward personalized face age transformation. In: Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 3908\u20133917 (2021)", "Zhu, J.-Y., Kr\u00a8ahenb\u00a8uhl, P., Shechtman, E., Efros, A.A.: Generative visual manipulation on the natural image manifold. In: Computer vision\u2013ECCV 2016: 14th European Conference, Amsterdam, the Netherlands, October 11-14, 2016, Proceedings, Part V 14, pp. 597\u2013613 (2016). Springer", "Abdal, R., Zhu, P., Mitra, N.J., Wonka, P.: Styleflow: Attribute-conditioned exploration of stylegan-generated images using conditional continuous normalizing flows. ACM Transactions on Graphics (ToG) 40(3), 1\u201321 (2021)", "Richardson, E., Alaluf, Y., Patashnik, O., Nitzan, Y., Azar, Y., Shapiro, S., Cohen-Or, D.: Encoding in style: a stylegan encoder for image-to-image translation. In: IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2021)", "Katsumata, K., Vo, D.M., Liu, B., Nakayama, H.: Revisiting latent space of gan inversion for robust real image editing. In: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 5313\u20135322 (2024)", "Jiang, Y., Chang, S., Wang, Z.: Transgan: Two pure transformers can make one strong gan, and that can scale up. Advances in Neural Information Processing Systems 34, 14745\u201314758 (2021)", "Zhao, L., Zhang, Z., Chen, T., Metaxas, D., Zhang, H.: Improved transformer for high-resolution gans. Advances in Neural Information Processing Systems 34, 18367\u201318380 (2021)", "Zhang, B., Gu, S., Zhang, B., Bao, J., Chen, D., Wen, F., Wang, Y., Guo, B.: Styleswin: Transformer-based gan for high-resolution image generation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 11304\u201311314 (2022)", "Ho, J., Jain, A., Abbeel, P.: Denoising diffusion probabilistic models. Advances in neural information processing systems 33, 6840\u20136851 (2020)", "Saharia, C., Chan, W., Chang, H., Lee, C., Ho, J., Salimans, T., Fleet, D., Norouzi, M.: Palette: Image-to-image diffusion models. In: ACM SIGGRAPH 2022 Conference Proceedings, pp. 1\u201310 (2022)", "Chen, X., Lathuili`ere, S.: Face aging via diffusion-based editing. arXiv preprint arXiv:2309.11321 (2023)", "Yousif, M.J.: Enhancing the accuracy of image classification using deep learning and preprocessing methods. Artificial Intelligence & Robotics Development Journal (2023)", "Albert, S., Wichtmann, B.D., Zhao, W., Maurer, A., Hesser, J., Attenberger, U.I., Schad, L.R., Z\u00a8ollner, F.G.: Comparison of image normalization methods for multi-site deep learning. Applied Sciences 13(15), 8923 (2023)", "Rothe, R., Timofte, R., Van Gool, L.: Dex: Deep expectation of apparent age from a single image. In: Proceedings of the IEEE International Conference on Computer Vision Workshops, pp. 10\u201315 (2015)", "Zhang, Z., Song, Y., Qi, H.: Age progression/regression by conditional adversarial autoencoder. In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2017). IEEE", "Wang, X., Li, Y., Zhang, H., Shan, Y.: Towards real-world blind face restoration with generative facial prior. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9168\u20139178 (2021)", "Gomez-Trenado, G., Lathuili`ere, S., Mesejo, P., Cord\u00b4on, O.: Custom structure preservation in face aging. In: European Conference on Computer Vision, pp. 565\u2013580 (2022). Springer", "Karkkainen, K., Joo, J.: Fairface: Face attribute dataset for balanced race, gender, and age for bias measurement and mitigation. In: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 1548\u20131558 (2021)", "Lu, J., Zhou, X., Tan, Y.-P., Shang, Y., Zhou, J.: Neighborhood repulsed metric learning for kinship verification. IEEE transactions on pattern analysis and machine intelligence 36(2), 331\u2013345 (2013)", "Zhu, X., Li, C., Chen, X., Zhang, X., Jing, X.-Y.: Distance and direction based deep discriminant metric learning for kinship verification. ACM Transactions on Multimedia Computing, Communications and Applications 19(1s), 1\u201319 (2023)"], "sub_categories": ["Race Bias in Face Aging", "Kinship Verification", "Face Aging GAN"]}
{"doc_id": "2509.15167v1", "title": "Semi-Supervised 3D Medical Segmentation from 2D Natural Images   Pretrained Model", "abstract": "This paper explores the transfer of knowledge from general vision models pretrained on 2D natural images to improve 3D medical image segmentation. We focus on the semi-supervised setting, where only a few labeled 3D medical images are available, along with a large set of unlabeled images. To tackle this, we propose a model-agnostic framework that progressively distills knowledge from a 2D pretrained model to a 3D segmentation model trained from scratch. Our approach, M&N, involves iterative co-training of the two models using pseudo-masks generated by each other, along with our proposed learning rate guided sampling that adaptively adjusts the proportion of labeled and unlabeled data in each training batch to align with the models' prediction accuracy and stability, minimizing the adverse effect caused by inaccurate pseudo-masks. Extensive experiments on multiple publicly available datasets demonstrate that M&N achieves state-of-the-art performance, outperforming thirteen existing semi-supervised segmentation approaches under all different settings. Importantly, ablation studies show that M&N remains model-agnostic, allowing seamless integration with different architectures. This ensures its adaptability as more advanced models emerge. The code is available at https://github.com/pakheiyeung/M-N.", "authors": ["Pak-Hei Yeung", "Jayroop Ramesh", "Pengfei Lyu", "Ana Namburete", "Jagath Rajapakse"], "published_date": "2025-09-18T17:17:52Z", "url": "http://arxiv.org/abs/2509.15167v1", "pdf_url": "http://arxiv.org/pdf/2509.15167v1", "primary_category": "cs.CV", "references": ["Loshchilov, I., Hutter, F.: Decoupled weight decay regularization. In: International Conference on Learning Representations (ICLR) (2017), https://api.semanticscholar.org/CorpusID:53592270", "Lyu, P., Yeung, P.H., Yu, X., Xia, J., Chi, J., Wu, C., Rajapakse, J.C.: Bridging the inter-domain gap through low-level features for cross-modal medical image segmentation. arXiv preprint arXiv:2505.11909 (2025)", "Milletari, F., Navab, N., Ahmadi, S.A.: V-net: Fully convolutional neural networks for volumetric medical image segmentation. In: International Conference on 3D Vision (3DV). pp. 565\u2013571. IEEE (2016)", "Peiris, H., Hayat, M., Chen, Z., Egan, G., Harandi, M.: Uncertainty-guided dual-views for semi-supervised volumetric medical image segmentation. Nature Machine Intelligence 5(7), 724\u2013738 (2023)", "Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomedical image segmentation. In: International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI). pp. 234\u2013241. Springer (2015)", "Roth, H., Farag, A., Turkbey, E., Lu, L., Liu, J., Summers, R.: Data from pancreas-ct (version 2)[data set]. the cancer imaging archive (2016)", "Wang, M., houcheng su, Li, J., Li, C., Yin, N., Shen, L., Guo, J.: GraphCL: Graph-based clustering for semi-supervised medical image segmentation. In: International Conference on Machine Learning (ICML) (2025), https://openreview.net/forum?id=Q2av1PZmfT", "Wang, Y., Zhang, Y., Tian, J., Zhong, C., Shi, Z., Zhang, Y., He, Z.: Double-uncertainty weighted method for semi-supervised learning. In: International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI). pp. 542\u2013551. Springer (2020)", "Wu, R., Li, D., Zhang, C.: Semi-supervised medical image segmentation via query distribution consistency. In: IEEE International Symposium on Biomedical Imaging (ISBI). pp. 1\u20135. IEEE (2024)", "Wu, Y., Ge, Z., Zhang, D., Xu, M., Zhang, L., Xia, Y., Cai, J.: Mutual consistency learning for semi-supervised medical image segmentation. Medical Image Analysis 81, 102530 (2022)", "Wu, Y., Wu, Z., Wu, Q., Ge, Z., Cai, J.: Exploring smoothness and class-separation for semi-supervised medical image segmentation. In: International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI). pp. 34\u201343. Springer (2022)", "Wu, Y., Xu, M., Ge, Z., Cai, J., Zhang, L.: Semi-supervised left atrium segmentation with mutual consistency training. In: International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI). pp. 297\u2013306. Springer (2021)", "Xie, E., Wang, W., Yu, Z., Anandkumar, A., Alvarez, J.M., Luo, P.: Segformer: Simple and efficient design for semantic segmentation with transformers. Advances in Neural Information Processing Systems (NeurIPS) 34, 12077\u201312090 (2021)", "Xiong, Z., Xia, Q., Hu, Z., Huang, N., Bian, C., Zheng, Y., Vesal, S., Ravikumar, N., Maier, A., Yang, X., et al.: A global benchmark of algorithms for segmenting the left atrium from late gadolinium-enhanced cardiac magnetic resonance imaging. Medical Image Analysis 67, 101832 (2021)", "Yang, J., Huang, X., He, Y., Xu, J., Yang, C., Xu, G., Ni, B.: Reinventing 2d convolutions for 3d images. IEEE Journal of Biomedical and Health Informatics 25(8), 3009\u20133018 (2021)", "Yeung, P.H., Namburete, A.I., Xie, W.: Sli2vol: Annotate a 3d volume from a single slice with self-supervised learning. In: International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI). pp. 69\u201379. Springer (2021)", "Yu, L., Wang, S., Li, X., Fu, C.W., Heng, P.A.: Uncertainty-aware self-ensembling model for semi-supervised 3d left atrium segmentation. In: International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI). pp. 605\u2013613. Springer (2019)", "Zhao, Z., Wang, Z., Wang, L., Yu, D., Yuan, Y., Zhou, L.: Alternate diverse teaching for semi-supervised medical image segmentation. In: European Conference on Computer Vision (ECCV). pp. 227\u2013243. Springer (2024)", "Zhou, B., Zhao, H., Puig, X., Fidler, S., Barriuso, A., Torralba, A.: Scene parsing through ade20k dataset. In: Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR). pp. 633\u2013641 (2017)"], "sub_categories": ["Semi-Supervised 3D Medical Segmentation", "Knowledge Distillation", "Model-Agnostic Framework"]}
{"doc_id": "2509.15159v1", "title": "AIP: Subverting Retrieval-Augmented Generation via Adversarial   Instructional Prompt", "abstract": "Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by retrieving relevant documents from external sources to improve factual accuracy and verifiability. However, this reliance introduces new attack surfaces within the retrieval pipeline, beyond the LLM itself. While prior RAG attacks have exposed such vulnerabilities, they largely rely on manipulating user queries, which is often infeasible in practice due to fixed or protected user inputs. This narrow focus overlooks a more realistic and stealthy vector: instructional prompts, which are widely reused, publicly shared, and rarely audited. Their implicit trust makes them a compelling target for adversaries to manipulate RAG behavior covertly.   We introduce a novel attack for Adversarial Instructional Prompt (AIP) that exploits adversarial instructional prompts to manipulate RAG outputs by subtly altering retrieval behavior. By shifting the attack surface to the instructional prompts, AIP reveals how trusted yet seemingly benign interface components can be weaponized to degrade system integrity. The attack is crafted to achieve three goals: (1) naturalness, to evade user detection; (2) utility, to encourage use of prompts; and (3) robustness, to remain effective across diverse query variations. We propose a diverse query generation strategy that simulates realistic linguistic variation in user queries, enabling the discovery of prompts that generalize across paraphrases and rephrasings. Building on this, a genetic algorithm-based joint optimization is developed to evolve adversarial prompts by balancing attack success, clean-task utility, and stealthiness. Experimental results show that AIP achieves up to 95.23% ASR while preserving benign functionality. These findings uncover a critical and previously overlooked vulnerability in RAG systems, emphasizing the need to reassess the shared instructional prompts.", "authors": ["Saket S. Chaturvedi", "Gaurav Bagwe", "Lan Zhang", "Xiaoyong Yuan"], "published_date": "2025-09-18T17:06:53Z", "url": "http://arxiv.org/abs/2509.15159v1", "pdf_url": "http://arxiv.org/pdf/2509.15159v1", "primary_category": "cs.CV", "references": ["Chen et al., 2025", "Zhong et al., 2023", "Zhou and Pei, 2009", "Chen et al., 2025"], "sub_categories": ["Retrieval-Augmented Generation", "Adversarial Instructional Prompts", "RAG Attacks"]}
{"doc_id": "2509.15156v1", "title": "Leveraging Geometric Visual Illusions as Perceptual Inductive Biases for   Vision Models", "abstract": "Contemporary deep learning models have achieved impressive performance in image classification by primarily leveraging statistical regularities within large datasets, but they rarely incorporate structured insights drawn directly from perceptual psychology. To explore the potential of perceptually motivated inductive biases, we propose integrating classic geometric visual illusions well-studied phenomena from human perception into standard image-classification training pipelines. Specifically, we introduce a synthetic, parametric geometric-illusion dataset and evaluate three multi-source learning strategies that combine illusion recognition tasks with ImageNet classification objectives. Our experiments reveal two key conceptual insights: (i) incorporating geometric illusions as auxiliary supervision systematically improves generalization, especially in visually challenging cases involving intricate contours and fine textures; and (ii) perceptually driven inductive biases, even when derived from synthetic stimuli traditionally considered unrelated to natural image recognition, can enhance the structural sensitivity of both CNN and transformer-based architectures. These results demonstrate a novel integration of perceptual science and machine learning and suggest new directions for embedding perceptual priors into vision model design.", "authors": ["Haobo Yang", "Minghao Guo", "Dequan Yang", "Wenyu Wang"], "published_date": "2025-09-18T17:00:42Z", "url": "http://arxiv.org/abs/2509.15156v1", "pdf_url": "http://arxiv.org/pdf/2509.15156v1", "primary_category": "cs.CV", "references": ["Inception ResNet V2 [27]"], "sub_categories": ["Geometric Visual Illusions", "Perceptual Inductive Biases", "Multi-Source Learning"]}
