{"doc_id": "2509.15224v1", "title": "Depth AnyEvent: A Cross-Modal Distillation Paradigm for Event-Based   Monocular Depth Estimation", "abstract": "Event cameras capture sparse, high-temporal-resolution visual information, making them particularly suitable for challenging environments with high-speed motion and strongly varying lighting conditions. However, the lack of large datasets with dense ground-truth depth annotations hinders learning-based monocular depth estimation from event data. To address this limitation, we propose a cross-modal distillation paradigm to generate dense proxy labels leveraging a Vision Foundation Model (VFM). Our strategy requires an event stream spatially aligned with RGB frames, a simple setup even available off-the-shelf, and exploits the robustness of large-scale VFMs. Additionally, we propose to adapt VFMs, either a vanilla one like Depth Anything v2 (DAv2), or deriving from it a novel recurrent architecture to infer depth from monocular event cameras. We evaluate our approach with synthetic and real-world datasets, demonstrating that i) our cross-modal paradigm achieves competitive performance compared to fully supervised methods without requiring expensive depth annotations, and ii) our VFM-based models achieve state-of-the-art performance.", "authors": ["Luca Bartolomei", "Enrico Mannocci", "Fabio Tosi", "Matteo Poggi", "Stefano Mattoccia"], "published_date": "2025-09-18T17:59:51Z", "url": "http://arxiv.org/abs/2509.15224v1", "pdf_url": "http://arxiv.org/pdf/2509.15224v1", "primary_category": "cs.CV"}
{"doc_id": "2509.15220v1", "title": "Lightweight and Accurate Multi-View Stereo with Confidence-Aware   Diffusion Model", "abstract": "To reconstruct the 3D geometry from calibrated images, learning-based multi-view stereo (MVS) methods typically perform multi-view depth estimation and then fuse depth maps into a mesh or point cloud. To improve the computational efficiency, many methods initialize a coarse depth map and then gradually refine it in higher resolutions. Recently, diffusion models achieve great success in generation tasks. Starting from a random noise, diffusion models gradually recover the sample with an iterative denoising process. In this paper, we propose a novel MVS framework, which introduces diffusion models in MVS. Specifically, we formulate depth refinement as a conditional diffusion process. Considering the discriminative characteristic of depth estimation, we design a condition encoder to guide the diffusion process. To improve efficiency, we propose a novel diffusion network combining lightweight 2D U-Net and convolutional GRU. Moreover, we propose a novel confidence-based sampling strategy to adaptively sample depth hypotheses based on the confidence estimated by diffusion model. Based on our novel MVS framework, we propose two novel MVS methods, DiffMVS and CasDiffMVS. DiffMVS achieves competitive performance with state-of-the-art efficiency in run-time and GPU memory. CasDiffMVS achieves state-of-the-art performance on DTU, Tanks & Temples and ETH3D. Code is available at: https://github.com/cvg/diffmvs.", "authors": ["Fangjinhua Wang", "Qingshan Xu", "Yew-Soon Ong", "Marc Pollefeys"], "published_date": "2025-09-18T17:59:19Z", "url": "http://arxiv.org/abs/2509.15220v1", "pdf_url": "http://arxiv.org/pdf/2509.15220v1", "primary_category": "cs.CV"}
{"doc_id": "2509.14989v1", "title": "UCorr: Wire Detection and Depth Estimation for Autonomous Drones", "abstract": "In the realm of fully autonomous drones, the accurate detection of obstacles is paramount to ensure safe navigation and prevent collisions. Among these challenges, the detection of wires stands out due to their slender profile, which poses a unique and intricate problem. To address this issue, we present an innovative solution in the form of a monocular end-to-end model for wire segmentation and depth estimation. Our approach leverages a temporal correlation layer trained on synthetic data, providing the model with the ability to effectively tackle the complex joint task of wire detection and depth estimation. We demonstrate the superiority of our proposed method over existing competitive approaches in the joint task of wire detection and depth estimation. Our results underscore the potential of our model to enhance the safety and precision of autonomous drones, shedding light on its promising applications in real-world scenarios.", "authors": ["Benedikt Kolbeinsson", "Krystian Mikolajczyk"], "published_date": "2025-09-18T14:21:52Z", "url": "http://arxiv.org/abs/2509.14989v1", "pdf_url": "http://arxiv.org/pdf/2509.14989v1", "primary_category": "cs.CV"}
