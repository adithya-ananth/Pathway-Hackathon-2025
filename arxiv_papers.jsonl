{"doc_id": "2509.15225v1", "title": "Lost in Translation? Vocabulary Alignment for Source-Free Domain   Adaptation in Open-Vocabulary Semantic Segmentation", "abstract": "We introduce VocAlign, a novel source-free domain adaptation framework specifically designed for VLMs in open-vocabulary semantic segmentation. Our method adopts a student-teacher paradigm enhanced with a vocabulary alignment strategy, which improves pseudo-label generation by incorporating additional class concepts. To ensure efficiency, we use Low-Rank Adaptation (LoRA) to fine-tune the model, preserving its original capabilities while minimizing computational overhead. In addition, we propose a Top-K class selection mechanism for the student model, which significantly reduces memory requirements while further improving adaptation performance. Our approach achieves a notable 6.11 mIoU improvement on the CityScapes dataset and demonstrates superior performance on zero-shot segmentation benchmarks, setting a new standard for source-free adaptation in the open-vocabulary setting.", "authors": ["Silvio Mazzucco", "Carl Persson", "Mattia Segu", "Pier Luigi Dovesi", "Federico Tombari", "Luc Van Gool", "Matteo Poggi"], "published_date": "2025-09-18T17:59:58Z", "url": "http://arxiv.org/abs/2509.15225v1", "pdf_url": "http://arxiv.org/pdf/2509.15225v1", "primary_category": "cs.CV"}
{"doc_id": "2509.15226v1", "title": "Calibration-Aware Prompt Learning for Medical Vision-Language Models", "abstract": "Medical Vision-Language Models (Med-VLMs) have demonstrated remarkable performance across diverse medical imaging tasks by leveraging large-scale image-text pretraining. However, their confidence calibration is largely unexplored, and so remains a significant challenge. As such, miscalibrated predictions can lead to overconfident errors, undermining clinical trust and decision-making reliability. To address this, we introduce CalibPrompt, the first framework to calibrate Med-VLMs during prompt tuning. CalibPrompt optimizes a small set of learnable prompts with carefully designed calibration objectives under scarce labeled data regime. First, we study a regularizer that attempts to align the smoothed accuracy with the predicted model confidences. Second, we introduce an angular separation loss to maximize textual feature proximity toward improving the reliability in confidence estimates of multimodal Med-VLMs. Extensive experiments on four publicly available Med-VLMs and five diverse medical imaging datasets reveal that CalibPrompt consistently improves calibration without drastically affecting clean accuracy. Our code is available at https://github.com/iabh1shekbasu/CalibPrompt.", "authors": ["Abhishek Basu", "Fahad Shamshad", "Ashshak Sharifdeen", "Karthik Nandakumar", "Muhammad Haris Khan"], "published_date": "2025-09-18T17:59:58Z", "url": "http://arxiv.org/abs/2509.15226v1", "pdf_url": "http://arxiv.org/pdf/2509.15226v1", "primary_category": "cs.CV"}
{"doc_id": "2509.15224v1", "title": "Depth AnyEvent: A Cross-Modal Distillation Paradigm for Event-Based   Monocular Depth Estimation", "abstract": "Event cameras capture sparse, high-temporal-resolution visual information, making them particularly suitable for challenging environments with high-speed motion and strongly varying lighting conditions. However, the lack of large datasets with dense ground-truth depth annotations hinders learning-based monocular depth estimation from event data. To address this limitation, we propose a cross-modal distillation paradigm to generate dense proxy labels leveraging a Vision Foundation Model (VFM). Our strategy requires an event stream spatially aligned with RGB frames, a simple setup even available off-the-shelf, and exploits the robustness of large-scale VFMs. Additionally, we propose to adapt VFMs, either a vanilla one like Depth Anything v2 (DAv2), or deriving from it a novel recurrent architecture to infer depth from monocular event cameras. We evaluate our approach with synthetic and real-world datasets, demonstrating that i) our cross-modal paradigm achieves competitive performance compared to fully supervised methods without requiring expensive depth annotations, and ii) our VFM-based models achieve state-of-the-art performance.", "authors": ["Luca Bartolomei", "Enrico Mannocci", "Fabio Tosi", "Matteo Poggi", "Stefano Mattoccia"], "published_date": "2025-09-18T17:59:51Z", "url": "http://arxiv.org/abs/2509.15224v1", "pdf_url": "http://arxiv.org/pdf/2509.15224v1", "primary_category": "cs.CV"}
{"doc_id": "2509.15222v1", "title": "Two Web Toolkits for Multimodal Piano Performance Dataset Acquisition   and Fingering Annotation", "abstract": "Piano performance is a multimodal activity that intrinsically combines physical actions with the acoustic rendition. Despite growing research interest in analyzing the multimodal nature of piano performance, the laborious process of acquiring large-scale multimodal data remains a significant bottleneck, hindering further progress in this field. To overcome this barrier, we present an integrated web toolkit comprising two graphical user interfaces (GUIs): (i) PiaRec, which supports the synchronized acquisition of audio, video, MIDI, and performance metadata. (ii) ASDF, which enables the efficient annotation of performer fingering from the visual data. Collectively, this system can streamline the acquisition of multimodal piano performance datasets.", "authors": ["Junhyung Park", "Yonghyun Kim", "Joonhyung Bae", "Kirak Kim", "Taegyun Kwon", "Alexander Lerch", "Juhan Nam"], "published_date": "2025-09-18T17:59:24Z", "url": "http://arxiv.org/abs/2509.15222v1", "pdf_url": "http://arxiv.org/pdf/2509.15222v1", "primary_category": "cs.SD"}
{"doc_id": "2509.15221v1", "title": "ScaleCUA: Scaling Open-Source Computer Use Agents with Cross-Platform   Data", "abstract": "Vision-Language Models (VLMs) have enabled computer use agents (CUAs) that operate GUIs autonomously, showing great potential, yet progress is limited by the lack of large-scale, open-source computer use data and foundation models. In this work, we introduce ScaleCUA, a step toward scaling open-source CUAs. It offers a large-scale dataset spanning 6 operating systems and 3 task domains, built via a closed-loop pipeline uniting automated agents with human experts. Trained on this scaled-up data, ScaleCUA can operate seamlessly across platforms. Specifically, it delivers strong gains over baselines (+26.6 on WebArena-Lite-v2, +10.7 on ScreenSpot-Pro) and sets new state-of-the-art results (94.4% on MMBench-GUI L1-Hard, 60.6% on OSWorld-G, 47.4% on WebArena-Lite-v2). These findings underscore the power of data-driven scaling for general-purpose computer use agents. We will release data, models, and code to advance future research: https://github.com/OpenGVLab/ScaleCUA.", "authors": ["Zhaoyang Liu", "JingJing Xie", "Zichen Ding", "Zehao Li", "Bowen Yang", "Zhenyu Wu", "Xuehui Wang", "Qiushi Sun", "Shi Liu", "Weiyun Wang", "Shenglong Ye", "Qingyun Li", "Zeyue Tian", "Gen Luo", "Xiangyu Yue", "Biqing Qi", "Kai Chen", "Bowen Zhou", "Yu Qiao", "Qifeng Chen", "Wenhai Wang"], "published_date": "2025-09-18T17:59:22Z", "url": "http://arxiv.org/abs/2509.15221v1", "pdf_url": "http://arxiv.org/pdf/2509.15221v1", "primary_category": "cs.CV"}
{"doc_id": "2509.15220v1", "title": "Lightweight and Accurate Multi-View Stereo with Confidence-Aware   Diffusion Model", "abstract": "To reconstruct the 3D geometry from calibrated images, learning-based multi-view stereo (MVS) methods typically perform multi-view depth estimation and then fuse depth maps into a mesh or point cloud. To improve the computational efficiency, many methods initialize a coarse depth map and then gradually refine it in higher resolutions. Recently, diffusion models achieve great success in generation tasks. Starting from a random noise, diffusion models gradually recover the sample with an iterative denoising process. In this paper, we propose a novel MVS framework, which introduces diffusion models in MVS. Specifically, we formulate depth refinement as a conditional diffusion process. Considering the discriminative characteristic of depth estimation, we design a condition encoder to guide the diffusion process. To improve efficiency, we propose a novel diffusion network combining lightweight 2D U-Net and convolutional GRU. Moreover, we propose a novel confidence-based sampling strategy to adaptively sample depth hypotheses based on the confidence estimated by diffusion model. Based on our novel MVS framework, we propose two novel MVS methods, DiffMVS and CasDiffMVS. DiffMVS achieves competitive performance with state-of-the-art efficiency in run-time and GPU memory. CasDiffMVS achieves state-of-the-art performance on DTU, Tanks & Temples and ETH3D. Code is available at: https://github.com/cvg/diffmvs.", "authors": ["Fangjinhua Wang", "Qingshan Xu", "Yew-Soon Ong", "Marc Pollefeys"], "published_date": "2025-09-18T17:59:19Z", "url": "http://arxiv.org/abs/2509.15220v1", "pdf_url": "http://arxiv.org/pdf/2509.15220v1", "primary_category": "cs.CV"}
{"doc_id": "2509.15219v1", "title": "Out-of-Sight Trajectories: Tracking, Fusion, and Prediction", "abstract": "Trajectory prediction is a critical task in computer vision and autonomous systems, playing a key role in autonomous driving, robotics, surveillance, and virtual reality. Existing methods often rely on complete and noise-free observational data, overlooking the challenges associated with out-of-sight objects and the inherent noise in sensor data caused by limited camera coverage, obstructions, and the absence of ground truth for denoised trajectories. These limitations pose safety risks and hinder reliable prediction in real-world scenarios. In this extended work, we present advancements in Out-of-Sight Trajectory (OST), a novel task that predicts the noise-free visual trajectories of out-of-sight objects using noisy sensor data. Building on our previous research, we broaden the scope of Out-of-Sight Trajectory Prediction (OOSTraj) to include pedestrians and vehicles, extending its applicability to autonomous driving, robotics, surveillance, and virtual reality. Our enhanced Vision-Positioning Denoising Module leverages camera calibration to establish a vision-positioning mapping, addressing the lack of visual references, while effectively denoising noisy sensor data in an unsupervised manner. Through extensive evaluations on the Vi-Fi and JRDB datasets, our approach achieves state-of-the-art performance in both trajectory denoising and prediction, significantly surpassing previous baselines. Additionally, we introduce comparisons with traditional denoising methods, such as Kalman filtering, and adapt recent trajectory prediction models to our task, providing a comprehensive benchmark. This work represents the first initiative to integrate vision-positioning projection for denoising noisy sensor trajectories of out-of-sight agents, paving the way for future advances. The code and preprocessed datasets are available at github.com/Hai-chao-Zhang/OST", "authors": ["Haichao Zhang", "Yi Xu", "Yun Fu"], "published_date": "2025-09-18T17:59:16Z", "url": "http://arxiv.org/abs/2509.15219v1", "pdf_url": "http://arxiv.org/pdf/2509.15219v1", "primary_category": "cs.CV"}
{"doc_id": "2509.15217v1", "title": "Generalizable Geometric Image Caption Synthesis", "abstract": "Multimodal large language models have various practical applications that demand strong reasoning abilities. Despite recent advancements, these models still struggle to solve complex geometric problems. A key challenge stems from the lack of high-quality image-text pair datasets for understanding geometric images. Furthermore, most template-based data synthesis pipelines typically fail to generalize to questions beyond their predefined templates. In this paper, we bridge this gap by introducing a complementary process of Reinforcement Learning with Verifiable Rewards (RLVR) into the data generation pipeline. By adopting RLVR to refine captions for geometric images synthesized from 50 basic geometric relations and using reward signals derived from mathematical problem-solving tasks, our pipeline successfully captures the key features of geometry problem-solving. This enables better task generalization and yields non-trivial improvements. Furthermore, even in out-of-distribution scenarios, the generated dataset enhances the general reasoning capabilities of multimodal large language models, yielding accuracy improvements of $2.8\\%\\text{-}4.8\\%$ in statistics, arithmetic, algebraic, and numerical tasks with non-geometric input images of MathVista and MathVerse, along with $2.4\\%\\text{-}3.9\\%$ improvements in Art, Design, Tech, and Engineering tasks in MMMU.", "authors": ["Yue Xin", "Wenyuan Wang", "Rui Pan", "Ruida Wang", "Howard Meng", "Renjie Pi", "Shizhe Diao", "Tong Zhang"], "published_date": "2025-09-18T17:59:11Z", "url": "http://arxiv.org/abs/2509.15217v1", "pdf_url": "http://arxiv.org/pdf/2509.15217v1", "primary_category": "cs.AI"}
{"doc_id": "2509.15212v1", "title": "RynnVLA-001: Using Human Demonstrations to Improve Robot Manipulation", "abstract": "This paper presents RynnVLA-001, a vision-language-action(VLA) model built upon large-scale video generative pretraining from human demonstrations. We propose a novel two-stage pretraining methodology. The first stage, Ego-Centric Video Generative Pretraining, trains an Image-to-Video model on 12M ego-centric manipulation videos to predict future frames conditioned on an initial frame and a language instruction. The second stage, Human-Centric Trajectory-Aware Modeling, extends this by jointly predicting future keypoint trajectories, thereby effectively bridging visual frame prediction with action prediction. Furthermore, to enhance action representation, we propose ActionVAE, a variational autoencoder that compresses sequences of actions into compact latent embeddings, reducing the complexity of the VLA output space. When finetuned on the same downstream robotics datasets, RynnVLA-001 achieves superior performance over state-of-the-art baselines, demonstrating that the proposed pretraining strategy provides a more effective initialization for VLA models.", "authors": ["Yuming Jiang", "Siteng Huang", "Shengke Xue", "Yaxi Zhao", "Jun Cen", "Sicong Leng", "Kehan Li", "Jiayan Guo", "Kexiang Wang", "Mingxiu Chen", "Fan Wang", "Deli Zhao", "Xin Li"], "published_date": "2025-09-18T17:58:02Z", "url": "http://arxiv.org/abs/2509.15212v1", "pdf_url": "http://arxiv.org/pdf/2509.15212v1", "primary_category": "cs.CV"}
{"doc_id": "2509.15208v1", "title": "Geometric Image Synchronization with Deep Watermarking", "abstract": "Synchronization is the task of estimating and inverting geometric transformations (e.g., crop, rotation) applied to an image. This work introduces SyncSeal, a bespoke watermarking method for robust image synchronization, which can be applied on top of existing watermarking methods to enhance their robustness against geometric transformations. It relies on an embedder network that imperceptibly alters images and an extractor network that predicts the geometric transformation to which the image was subjected. Both networks are end-to-end trained to minimize the error between the predicted and ground-truth parameters of the transformation, combined with a discriminator to maintain high perceptual quality. We experimentally validate our method on a wide variety of geometric and valuemetric transformations, demonstrating its effectiveness in accurately synchronizing images. We further show that our synchronization can effectively upgrade existing watermarking methods to withstand geometric transformations to which they were previously vulnerable.", "authors": ["Pierre Fernandez", "Tom\u00e1\u0161 Sou\u010dek", "Nikola Jovanovi\u0107", "Hady Elsahar", "Sylvestre-Alvise Rebuffi", "Valeriu Lacatusu", "Tuan Tran", "Alexandre Mourachko"], "published_date": "2025-09-18T17:56:54Z", "url": "http://arxiv.org/abs/2509.15208v1", "pdf_url": "http://arxiv.org/pdf/2509.15208v1", "primary_category": "cs.CV"}
{"doc_id": "2509.15207v1", "title": "FlowRL: Matching Reward Distributions for LLM Reasoning", "abstract": "We propose FlowRL: matching the full reward distribution via flow balancing instead of maximizing rewards in large language model (LLM) reinforcement learning (RL). Recent advanced reasoning models adopt reward-maximizing methods (\\eg, PPO and GRPO), which tend to over-optimize dominant reward signals while neglecting less frequent but valid reasoning paths, thus reducing diversity. In contrast, we transform scalar rewards into a normalized target distribution using a learnable partition function, and then minimize the reverse KL divergence between the policy and the target distribution. We implement this idea as a flow-balanced optimization method that promotes diverse exploration and generalizable reasoning trajectories. We conduct experiments on math and code reasoning tasks: FlowRL achieves a significant average improvement of $10.0\\%$ over GRPO and $5.1\\%$ over PPO on math benchmarks, and performs consistently better on code reasoning tasks. These results highlight reward distribution-matching as a key step toward efficient exploration and diverse reasoning in LLM reinforcement learning.", "authors": ["Xuekai Zhu", "Daixuan Cheng", "Dinghuai Zhang", "Hengli Li", "Kaiyan Zhang", "Che Jiang", "Youbang Sun", "Ermo Hua", "Yuxin Zuo", "Xingtai Lv", "Qizheng Zhang", "Lin Chen", "Fanghao Shao", "Bo Xue", "Yunchong Song", "Zhenjie Yang", "Ganqu Cui", "Ning Ding", "Jianfeng Gao", "Xiaodong Liu", "Bowen Zhou", "Hongyuan Mei", "Zhouhan Lin"], "published_date": "2025-09-18T17:56:36Z", "url": "http://arxiv.org/abs/2509.15207v1", "pdf_url": "http://arxiv.org/pdf/2509.15207v1", "primary_category": "cs.LG"}
{"doc_id": "2509.15205v1", "title": "Voyager: An End-to-End Framework for Design-Space Exploration and   Generation of DNN Accelerators", "abstract": "While deep neural networks (DNNs) have achieved state-of-the-art performance in fields from computer vision to natural language processing, efficiently running these computationally demanding models requires hardware accelerators. However, designing these accelerators is a time-consuming, labor-intensive process that does not scale well. While prior efforts have sought to automate DNN accelerator generation, they offer limited parameterization, cannot produce high-performance, tapeout-ready designs, provide limited support for datatypes and quantization schemes, and lack an integrated, end-to-end software compiler. This work proposes Voyager, a high-level synthesis (HLS)-based framework for design space exploration (DSE) and generation of DNN accelerators. Voyager overcomes the limitations of prior work by offering extensive configurability across technology nodes, clock frequencies, and scales, with customizable parameters such as number of processing elements, on-chip buffer sizes, and external memory bandwidth. Voyager supports a wider variety of datatypes and quantization schemes versus prior work, including both built-in floating-point, posit and integer formats, as well as user-defined formats with both per-tensor scaling and microscaling quantization. Voyager's PyTorch-based compiler efficiently maps networks end-to-end on the generated hardware, with support for quantization, fusion, and tiling. We evaluate Voyager on state-of-the-art vision and language models. Voyager enables fast DSE with full-dataset accuracy evaluation for datatypes and quantization schemes. Generated designs achieve a high utilization across models and scales, up to 99.8%, and outperform prior generators with up to 61% lower latency and 56% lower area. Compared to hand-optimized accelerators, Voyager achieves comparable performance, while offering much greater automation in design and workload mapping.", "authors": ["Kartik Prabhu", "Jeffrey Yu", "Xinyuan Allen Pan", "Zhouhua Xie", "Abigail Aleshire", "Zihan Chen", "Ammar Ali Ratnani", "Priyanka Raina"], "published_date": "2025-09-18T17:56:15Z", "url": "http://arxiv.org/abs/2509.15205v1", "pdf_url": "http://arxiv.org/pdf/2509.15205v1", "primary_category": "cs.AR"}
{"doc_id": "2509.15204v1", "title": "Circuit-based chatacterization of finite-temperature quantum phases and   self-correcting quantum memory", "abstract": "Quantum phases at zero temperature can be characterized as equivalence classes under local unitary transformations: two ground states within a gapped phase can be transformed into each other via a local unitary circuit. We generalize this circuit-based characterization of phases to systems at finite-temperature thermal equilibrium described by Gibbs states. We construct a channel circuit that approximately transforms one Gibbs state into another provided the two are connected by a path in parameter space along which a certain correlation-decay condition holds. For finite-dimensional systems of linear size $L$ and approximation error $\\epsilon$, the locality of the circuit is ${\\rm polylog}({\\rm poly}(L)/\\epsilon)$. The correlation-decay condition, which we specify, is expected to be satisfied in the interior of many noncritical thermal phases, including those displaying discrete symmetry breaking and topological order. As an application, we show that any system in the same thermal phase as a zero-temperature topological code coherently preserves quantum information for a macroscopically long time, establishing self-correction as a universal property of thermal phases. As part of the proof, we provide explicit encoding and decoding channel circuits to encode information into, and decode it from, a system in thermal equilibrium.", "authors": ["Ruochen Ma", "Vedika Khemani", "Shengqi Sang"], "published_date": "2025-09-18T17:55:15Z", "url": "http://arxiv.org/abs/2509.15204v1", "pdf_url": "http://arxiv.org/pdf/2509.15204v1", "primary_category": "quant-ph"}
{"doc_id": "2509.15185v1", "title": "Understand Before You Generate: Self-Guided Training for Autoregressive   Image Generation", "abstract": "Recent studies have demonstrated the importance of high-quality visual representations in image generation and have highlighted the limitations of generative models in image understanding. As a generative paradigm originally designed for natural language, autoregressive models face similar challenges. In this work, we present the first systematic investigation into the mechanisms of applying the next-token prediction paradigm to the visual domain. We identify three key properties that hinder the learning of high-level visual semantics: local and conditional dependence, inter-step semantic inconsistency, and spatial invariance deficiency. We show that these issues can be effectively addressed by introducing self-supervised objectives during training, leading to a novel training framework, Self-guided Training for AutoRegressive models (ST-AR). Without relying on pre-trained representation models, ST-AR significantly enhances the image understanding ability of autoregressive models and leads to improved generation quality. Specifically, ST-AR brings approximately 42% FID improvement for LlamaGen-L and 49% FID improvement for LlamaGen-XL, while maintaining the same sampling strategy.", "authors": ["Xiaoyu Yue", "Zidong Wang", "Yuqing Wang", "Wenlong Zhang", "Xihui Liu", "Wanli Ouyang", "Lei Bai", "Luping Zhou"], "published_date": "2025-09-18T17:47:40Z", "url": "http://arxiv.org/abs/2509.15185v1", "pdf_url": "http://arxiv.org/pdf/2509.15185v1", "primary_category": "cs.CV"}
{"doc_id": "2509.15181v1", "title": "Maize Seedling Detection Dataset (MSDD): A Curated High-Resolution RGB   Dataset for Seedling Maize Detection and Benchmarking with YOLOv9, YOLO11,   YOLOv12 and Faster-RCNN", "abstract": "Accurate maize seedling detection is crucial for precision agriculture, yet curated datasets remain scarce. We introduce MSDD, a high-quality aerial image dataset for maize seedling stand counting, with applications in early-season crop monitoring, yield prediction, and in-field management. Stand counting determines how many plants germinated, guiding timely decisions such as replanting or adjusting inputs. Traditional methods are labor-intensive and error-prone, while computer vision enables efficient, accurate detection. MSDD contains three classes-single, double, and triple plants-capturing diverse growth stages, planting setups, soil types, lighting conditions, camera angles, and densities, ensuring robustness for real-world use. Benchmarking shows detection is most reliable during V4-V6 stages and under nadir views. Among tested models, YOLO11 is fastest, while YOLOv9 yields the highest accuracy for single plants. Single plant detection achieves precision up to 0.984 and recall up to 0.873, but detecting doubles and triples remains difficult due to rarity and irregular appearance, often from planting errors. Class imbalance further reduces accuracy in multi-plant detection. Despite these challenges, YOLO11 maintains efficient inference at 35 ms per image, with an additional 120 ms for saving outputs. MSDD establishes a strong foundation for developing models that enhance stand counting, optimize resource allocation, and support real-time decision-making. This dataset marks a step toward automating agricultural monitoring and advancing precision agriculture.", "authors": ["Dewi Endah Kharismawati", "Toni Kazic"], "published_date": "2025-09-18T17:41:59Z", "url": "http://arxiv.org/abs/2509.15181v1", "pdf_url": "http://arxiv.org/pdf/2509.15181v1", "primary_category": "cs.CV"}
{"doc_id": "2509.15178v1", "title": "Unleashing the Potential of Multimodal LLMs for Zero-Shot   Spatio-Temporal Video Grounding", "abstract": "Spatio-temporal video grounding (STVG) aims at localizing the spatio-temporal tube of a video, as specified by the input text query. In this paper, we utilize multimodal large language models (MLLMs) to explore a zero-shot solution in STVG. We reveal two key insights about MLLMs: (1) MLLMs tend to dynamically assign special tokens, referred to as \\textit{grounding tokens}, for grounding the text query; and (2) MLLMs often suffer from suboptimal grounding due to the inability to fully integrate the cues in the text query (\\textit{e.g.}, attributes, actions) for inference. Based on these insights, we propose a MLLM-based zero-shot framework for STVG, which includes novel decomposed spatio-temporal highlighting (DSTH) and temporal-augmented assembling (TAS) strategies to unleash the reasoning ability of MLLMs. The DSTH strategy first decouples the original query into attribute and action sub-queries for inquiring the existence of the target both spatially and temporally. It then uses a novel logit-guided re-attention (LRA) module to learn latent variables as spatial and temporal prompts, by regularizing token predictions for each sub-query. These prompts highlight attribute and action cues, respectively, directing the model's attention to reliable spatial and temporal related visual regions. In addition, as the spatial grounding by the attribute sub-query should be temporally consistent, we introduce the TAS strategy to assemble the predictions using the original video frames and the temporal-augmented frames as inputs to help improve temporal consistency. We evaluate our method on various MLLMs, and show that it outperforms SOTA methods on three common STVG benchmarks.   The code will be available at https://github.com/zaiquanyang/LLaVA_Next_STVG.", "authors": ["Zaiquan Yang", "Yuhao Liu", "Gerhard Hancke", "Rynson W. H. Lau"], "published_date": "2025-09-18T17:35:50Z", "url": "http://arxiv.org/abs/2509.15178v1", "pdf_url": "http://arxiv.org/pdf/2509.15178v1", "primary_category": "cs.CV"}
{"doc_id": "2509.15177v1", "title": "A Race Bias Free Face Aging Model for Reliable Kinship Verification", "abstract": "The age gap in kinship verification addresses the time difference between the photos of the parent and the child. Moreover, their same-age photos are often unavailable, and face aging models are racially biased, which impacts the likeness of photos. Therefore, we propose a face aging GAN model, RA-GAN, consisting of two new modules, RACEpSp and a feature mixer, to produce racially unbiased images. The unbiased synthesized photos are used in kinship verification to investigate the results of verifying same-age parent-child images. The experiments demonstrate that our RA-GAN outperforms SAM-GAN on an average of 13.14\\% across all age groups, and CUSP-GAN in the 60+ age group by 9.1\\% in terms of racial accuracy. Moreover, RA-GAN can preserve subjects' identities better than SAM-GAN and CUSP-GAN across all age groups. Additionally, we demonstrate that transforming parent and child images from the KinFaceW-I and KinFaceW-II datasets to the same age can enhance the verification accuracy across all age groups. The accuracy increases with our RA-GAN for the kinship relationships of father-son and father-daughter, mother-son, and mother-daughter, which are 5.22, 5.12, 1.63, and 0.41, respectively, on KinFaceW-I. Additionally, the accuracy for the relationships of father-daughter, father-son, and mother-son is 2.9, 0.39, and 1.6 on KinFaceW-II, respectively. The code is available at~\\href{https://github.com/bardiya2254kariminia/An-Age-Transformation-whitout-racial-bias-for-Kinship-verification}{Github}", "authors": ["Ali Nazari", "Bardiya Kariminia", "Mohsen Ebrahimi Moghaddam"], "published_date": "2025-09-18T17:34:20Z", "url": "http://arxiv.org/abs/2509.15177v1", "pdf_url": "http://arxiv.org/pdf/2509.15177v1", "primary_category": "cs.CV"}
{"doc_id": "2509.15167v1", "title": "Semi-Supervised 3D Medical Segmentation from 2D Natural Images   Pretrained Model", "abstract": "This paper explores the transfer of knowledge from general vision models pretrained on 2D natural images to improve 3D medical image segmentation. We focus on the semi-supervised setting, where only a few labeled 3D medical images are available, along with a large set of unlabeled images. To tackle this, we propose a model-agnostic framework that progressively distills knowledge from a 2D pretrained model to a 3D segmentation model trained from scratch. Our approach, M&N, involves iterative co-training of the two models using pseudo-masks generated by each other, along with our proposed learning rate guided sampling that adaptively adjusts the proportion of labeled and unlabeled data in each training batch to align with the models' prediction accuracy and stability, minimizing the adverse effect caused by inaccurate pseudo-masks. Extensive experiments on multiple publicly available datasets demonstrate that M&N achieves state-of-the-art performance, outperforming thirteen existing semi-supervised segmentation approaches under all different settings. Importantly, ablation studies show that M&N remains model-agnostic, allowing seamless integration with different architectures. This ensures its adaptability as more advanced models emerge. The code is available at https://github.com/pakheiyeung/M-N.", "authors": ["Pak-Hei Yeung", "Jayroop Ramesh", "Pengfei Lyu", "Ana Namburete", "Jagath Rajapakse"], "published_date": "2025-09-18T17:17:52Z", "url": "http://arxiv.org/abs/2509.15167v1", "pdf_url": "http://arxiv.org/pdf/2509.15167v1", "primary_category": "cs.CV"}
{"doc_id": "2509.15159v1", "title": "AIP: Subverting Retrieval-Augmented Generation via Adversarial   Instructional Prompt", "abstract": "Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by retrieving relevant documents from external sources to improve factual accuracy and verifiability. However, this reliance introduces new attack surfaces within the retrieval pipeline, beyond the LLM itself. While prior RAG attacks have exposed such vulnerabilities, they largely rely on manipulating user queries, which is often infeasible in practice due to fixed or protected user inputs. This narrow focus overlooks a more realistic and stealthy vector: instructional prompts, which are widely reused, publicly shared, and rarely audited. Their implicit trust makes them a compelling target for adversaries to manipulate RAG behavior covertly.   We introduce a novel attack for Adversarial Instructional Prompt (AIP) that exploits adversarial instructional prompts to manipulate RAG outputs by subtly altering retrieval behavior. By shifting the attack surface to the instructional prompts, AIP reveals how trusted yet seemingly benign interface components can be weaponized to degrade system integrity. The attack is crafted to achieve three goals: (1) naturalness, to evade user detection; (2) utility, to encourage use of prompts; and (3) robustness, to remain effective across diverse query variations. We propose a diverse query generation strategy that simulates realistic linguistic variation in user queries, enabling the discovery of prompts that generalize across paraphrases and rephrasings. Building on this, a genetic algorithm-based joint optimization is developed to evolve adversarial prompts by balancing attack success, clean-task utility, and stealthiness. Experimental results show that AIP achieves up to 95.23% ASR while preserving benign functionality. These findings uncover a critical and previously overlooked vulnerability in RAG systems, emphasizing the need to reassess the shared instructional prompts.", "authors": ["Saket S. Chaturvedi", "Gaurav Bagwe", "Lan Zhang", "Xiaoyong Yuan"], "published_date": "2025-09-18T17:06:53Z", "url": "http://arxiv.org/abs/2509.15159v1", "pdf_url": "http://arxiv.org/pdf/2509.15159v1", "primary_category": "cs.CV"}
{"doc_id": "2509.15156v1", "title": "Leveraging Geometric Visual Illusions as Perceptual Inductive Biases for   Vision Models", "abstract": "Contemporary deep learning models have achieved impressive performance in image classification by primarily leveraging statistical regularities within large datasets, but they rarely incorporate structured insights drawn directly from perceptual psychology. To explore the potential of perceptually motivated inductive biases, we propose integrating classic geometric visual illusions well-studied phenomena from human perception into standard image-classification training pipelines. Specifically, we introduce a synthetic, parametric geometric-illusion dataset and evaluate three multi-source learning strategies that combine illusion recognition tasks with ImageNet classification objectives. Our experiments reveal two key conceptual insights: (i) incorporating geometric illusions as auxiliary supervision systematically improves generalization, especially in visually challenging cases involving intricate contours and fine textures; and (ii) perceptually driven inductive biases, even when derived from synthetic stimuli traditionally considered unrelated to natural image recognition, can enhance the structural sensitivity of both CNN and transformer-based architectures. These results demonstrate a novel integration of perceptual science and machine learning and suggest new directions for embedding perceptual priors into vision model design.", "authors": ["Haobo Yang", "Minghao Guo", "Dequan Yang", "Wenyu Wang"], "published_date": "2025-09-18T17:00:42Z", "url": "http://arxiv.org/abs/2509.15156v1", "pdf_url": "http://arxiv.org/pdf/2509.15156v1", "primary_category": "cs.CV"}
