{"doc_id": "2509.15224v1", "title": "Depth AnyEvent: A Cross-Modal Distillation Paradigm for Event-Based   Monocular Depth Estimation", "abstract": "Event cameras capture sparse, high-temporal-resolution visual information, making them particularly suitable for challenging environments with high-speed motion and strongly varying lighting conditions. However, the lack of large datasets with dense ground-truth depth annotations hinders learning-based monocular depth estimation from event data. To address this limitation, we propose a cross-modal distillation paradigm to generate dense proxy labels leveraging a Vision Foundation Model (VFM). Our strategy requires an event stream spatially aligned with RGB frames, a simple setup even available off-the-shelf, and exploits the robustness of large-scale VFMs. Additionally, we propose to adapt VFMs, either a vanilla one like Depth Anything v2 (DAv2), or deriving from it a novel recurrent architecture to infer depth from monocular event cameras. We evaluate our approach with synthetic and real-world datasets, demonstrating that i) our cross-modal paradigm achieves competitive performance compared to fully supervised methods without requiring expensive depth annotations, and ii) our VFM-based models achieve state-of-the-art performance.", "authors": ["Luca Bartolomei", "Enrico Mannocci", "Fabio Tosi", "Matteo Poggi", "Stefano Mattoccia"], "published_date": "2025-09-18T17:59:51Z", "url": "http://arxiv.org/abs/2509.15224v1", "pdf_url": "http://arxiv.org/pdf/2509.15224v1", "primary_category": "cs.CV", "journal_ref": null, "doi": null}
{"doc_id": "2509.13713v1", "title": "UM-Depth : Uncertainty Masked Self-Supervised Monocular Depth Estimation   with Visual Odometry", "abstract": "Monocular depth estimation has been increasingly adopted in robotics and autonomous driving for its ability to infer scene geometry from a single camera. In self-supervised monocular depth estimation frameworks, the network jointly generates and exploits depth and pose estimates during training, thereby eliminating the need for depth labels. However, these methods remain challenged by uncertainty in the input data, such as low-texture or dynamic regions, which can cause reduced depth accuracy. To address this, we introduce UM-Depth, a framework that combines motion- and uncertainty-aware refinement to enhance depth accuracy at dynamic object boundaries and in textureless regions. Specifically, we develop a teacherstudent training strategy that embeds uncertainty estimation into both the training pipeline and network architecture, thereby strengthening supervision where photometric signals are weak. Unlike prior motion-aware approaches that incur inference-time overhead and rely on additional labels or auxiliary networks for real-time generation, our method uses optical flow exclusively within the teacher network during training, which eliminating extra labeling demands and any runtime cost. Extensive experiments on the KITTI and Cityscapes datasets demonstrate the effectiveness of our uncertainty-aware refinement. Overall, UM-Depth achieves state-of-the-art results in both self-supervised depth and pose estimation on the KITTI datasets.", "authors": ["Tae-Wook Um", "Ki-Hyeon Kim", "Hyun-Duck Choi", "Hyo-Sung Ahn"], "published_date": "2025-09-17T05:51:07Z", "url": "http://arxiv.org/abs/2509.13713v1", "pdf_url": "http://arxiv.org/pdf/2509.13713v1", "primary_category": "cs.CV", "journal_ref": null, "doi": null}
{"doc_id": "2509.13686v1", "title": "RF-LSCM: Pushing Radiance Fields to Multi-Domain Localized Statistical   Channel Modeling for Cellular Network Optimization", "abstract": "Accurate localized wireless channel modeling is a cornerstone of cellular network optimization, enabling reliable prediction of network performance during parameter tuning. Localized statistical channel modeling (LSCM) is the state-of-the-art channel modeling framework tailored for cellular network optimization. However, traditional LSCM methods, which infer the channel's Angular Power Spectrum (APS) from Reference Signal Received Power (RSRP) measurements, suffer from critical limitations: they are typically confined to single-cell, single-grid and single-carrier frequency analysis and fail to capture complex cross-domain interactions. To overcome these challenges, we propose RF-LSCM, a novel framework that models the channel APS by jointly representing large-scale signal attenuation and multipath components within a radiance field. RF-LSCM introduces a multi-domain LSCM formulation with a physics-informed frequency-dependent Attenuation Model (FDAM) to facilitate the cross frequency generalization as well as a point-cloud-aided environment enhanced method to enable multi-cell and multi-grid channel modeling. Furthermore, to address the computational inefficiency of typical neural radiance fields, RF-LSCM leverages a low-rank tensor representation, complemented by a novel Hierarchical Tensor Angular Modeling (HiTAM) algorithm. This efficient design significantly reduces GPU memory requirements and training time while preserving fine-grained accuracy. Extensive experiments on real-world multi-cell datasets demonstrate that RF-LSCM significantly outperforms state-of-the-art methods, achieving up to a 30% reduction in mean absolute error (MAE) for coverage prediction and a 22% MAE improvement by effectively fusing multi-frequency data.", "authors": ["Bingsheng Peng", "Shutao Zhang", "Xi Zheng", "Ye Xue", "Xinyu Qin", "Tsung-Hui Chang"], "published_date": "2025-09-17T04:31:18Z", "url": "http://arxiv.org/abs/2509.13686v1", "pdf_url": "http://arxiv.org/pdf/2509.13686v1", "primary_category": "cs.LG", "journal_ref": null, "doi": null}
{"doc_id": "2509.13652v1", "title": "Gaussian Alignment for Relative Camera Pose Estimation via Single-View   Reconstruction", "abstract": "Estimating metric relative camera pose from a pair of images is of great importance for 3D reconstruction and localisation. However, conventional two-view pose estimation methods are not metric, with camera translation known only up to a scale, and struggle with wide baselines and textureless or reflective surfaces. This paper introduces GARPS, a training-free framework that casts this problem as the direct alignment of two independently reconstructed 3D scenes. GARPS leverages a metric monocular depth estimator and a Gaussian scene reconstructor to obtain a metric 3D Gaussian Mixture Model (GMM) for each image. It then refines an initial pose from a feed-forward two-view pose estimator by optimising a differentiable GMM alignment objective. This objective jointly considers geometric structure, view-independent colour, anisotropic covariance, and semantic feature consistency, and is robust to occlusions and texture-poor regions without requiring explicit 2D correspondences. Extensive experiments on the Real\\-Estate10K dataset demonstrate that GARPS outperforms both classical and state-of-the-art learning-based methods, including MASt3R. These results highlight the potential of bridging single-view perception with multi-view geometry to achieve robust and metric relative pose estimation.", "authors": ["Yumin Li", "Dylan Campbell"], "published_date": "2025-09-17T02:57:34Z", "url": "http://arxiv.org/abs/2509.13652v1", "pdf_url": "http://arxiv.org/pdf/2509.13652v1", "primary_category": "cs.CV", "journal_ref": null, "doi": null}
{"doc_id": "2509.13571v1", "title": "SuNeRF-CME: Physics-Informed Neural Radiance Fields for Tomographic   Reconstruction of Coronal Mass Ejections", "abstract": "Coronagraphic observations enable direct monitoring of coronal mass ejections (CMEs) through scattered light from free electrons, but determining the 3D plasma distribution from 2D imaging data is challenging due to the optically-thin plasma and the complex image formation processes. We introduce SuNeRF-CME, a framework for 3D tomographic reconstructions of the heliosphere using multi-viewpoint coronagraphic observations. The method leverages Neural Radiance Fields (NeRFs) to estimate the electron density in the heliosphere through a ray-tracing approach, while accounting for the underlying Thomson scattering of image formation. The model is optimized by iteratively fitting the time-dependent observational data. In addition, we apply physical constraints in terms of continuity, propagation direction, and speed of the heliospheric plasma to overcome limitations imposed by the sparse number of viewpoints. We utilize synthetic observations of a CME simulation to fully quantify the model's performance for different viewpoint configurations. The results demonstrate that our method can reliably estimate the CME parameters from only two viewpoints, with a mean velocity error of $3.01\\pm1.94\\%$ and propagation direction errors of $3.39\\pm1.94^\\circ$ in latitude and $1.76\\pm0.79^\\circ$ in longitude. We further show that our approach can achieve a full 3D reconstruction of the simulated CME from two viewpoints, where we correctly model the three-part structure, deformed CME front, and internal plasma variations. Additional viewpoints can be seamlessly integrated, directly enhancing the reconstruction of the plasma distribution in the heliosphere. This study underscores the value of physics-informed methods for reconstructing the heliospheric plasma distribution, paving the way for unraveling the dynamic 3D structure of CMEs and enabling advanced space weather monitoring.", "authors": ["Robert Jarolim", "Martin Sanner", "Chia-Man Hung", "Emma Stevenson", "Hala Lamdouar", "Josh Veitch-Michaelis", "Ioanna Bouri", "Anna Malanushenko", "Elena Provornikova", "V\u00edt R\u016f\u017ei\u010dka", "Carlos Urbina-Ortega"], "published_date": "2025-09-16T22:26:08Z", "url": "http://arxiv.org/abs/2509.13571v1", "pdf_url": "http://arxiv.org/pdf/2509.13571v1", "primary_category": "astro-ph.SR", "journal_ref": null, "doi": null}
{"doc_id": "2509.13414v1", "title": "MapAnything: Universal Feed-Forward Metric 3D Reconstruction", "abstract": "We introduce MapAnything, a unified transformer-based feed-forward model that ingests one or more images along with optional geometric inputs such as camera intrinsics, poses, depth, or partial reconstructions, and then directly regresses the metric 3D scene geometry and cameras. MapAnything leverages a factored representation of multi-view scene geometry, i.e., a collection of depth maps, local ray maps, camera poses, and a metric scale factor that effectively upgrades local reconstructions into a globally consistent metric frame. Standardizing the supervision and training across diverse datasets, along with flexible input augmentation, enables MapAnything to address a broad range of 3D vision tasks in a single feed-forward pass, including uncalibrated structure-from-motion, calibrated multi-view stereo, monocular depth estimation, camera localization, depth completion, and more. We provide extensive experimental analyses and model ablations demonstrating that MapAnything outperforms or matches specialist feed-forward models while offering more efficient joint training behavior, thus paving the way toward a universal 3D reconstruction backbone.", "authors": ["Nikhil Keetha", "Norman M\u00fcller", "Johannes Sch\u00f6nberger", "Lorenzo Porzi", "Yuchen Zhang", "Tobias Fischer", "Arno Knapitsch", "Duncan Zauss", "Ethan Weber", "Nelson Antunes", "Jonathon Luiten", "Manuel Lopez-Antequera", "Samuel Rota Bul\u00f2", "Christian Richardt", "Deva Ramanan", "Sebastian Scherer", "Peter Kontschieder"], "published_date": "2025-09-16T18:00:14Z", "url": "http://arxiv.org/abs/2509.13414v1", "pdf_url": "http://arxiv.org/pdf/2509.13414v1", "primary_category": "cs.CV", "journal_ref": null, "doi": null}
{"doc_id": "2509.13177v1", "title": "ROOM: A Physics-Based Continuum Robot Simulator for Photorealistic   Medical Datasets Generation", "abstract": "Continuum robots are advancing bronchoscopy procedures by accessing complex lung airways and enabling targeted interventions. However, their development is limited by the lack of realistic training and test environments: Real data is difficult to collect due to ethical constraints and patient safety concerns, and developing autonomy algorithms requires realistic imaging and physical feedback. We present ROOM (Realistic Optical Observation in Medicine), a comprehensive simulation framework designed for generating photorealistic bronchoscopy training data. By leveraging patient CT scans, our pipeline renders multi-modal sensor data including RGB images with realistic noise and light specularities, metric depth maps, surface normals, optical flow and point clouds at medically relevant scales. We validate the data generated by ROOM in two canonical tasks for medical robotics -- multi-view pose estimation and monocular depth estimation, demonstrating diverse challenges that state-of-the-art methods must overcome to transfer to these medical settings. Furthermore, we show that the data produced by ROOM can be used to fine-tune existing depth estimation models to overcome these challenges, also enabling other downstream applications such as navigation. We expect that ROOM will enable large-scale data generation across diverse patient anatomies and procedural scenarios that are challenging to capture in clinical settings. Code and data: https://github.com/iamsalvatore/room.", "authors": ["Salvatore Esposito", "Mat\u00edas Mattamala", "Daniel Rebain", "Francis Xiatian Zhang", "Kevin Dhaliwal", "Mohsen Khadem", "Subramanian Ramamoorthy"], "published_date": "2025-09-16T15:30:02Z", "url": "http://arxiv.org/abs/2509.13177v1", "pdf_url": "http://arxiv.org/pdf/2509.13177v1", "primary_category": "cs.RO", "journal_ref": null, "doi": null}
{"doc_id": "2509.12836v1", "title": "Exploring Metric Fusion for Evaluation of NeRFs", "abstract": "Neural Radiance Fields (NeRFs) have demonstrated significant potential in synthesizing novel viewpoints. Evaluating the NeRF-generated outputs, however, remains a challenge due to the unique artifacts they exhibit, and no individual metric performs well across all datasets. We hypothesize that combining two successful metrics, Deep Image Structure and Texture Similarity (DISTS) and Video Multi-Method Assessment Fusion (VMAF), based on different perceptual methods, can overcome the limitations of individual metrics and achieve improved correlation with subjective quality scores. We experiment with two normalization strategies for the individual metrics and two fusion strategies to evaluate their impact on the resulting correlation with the subjective scores. The proposed pipeline is tested on two distinct datasets, Synthetic and Outdoor, and its performance is evaluated across three different configurations. We present a detailed analysis comparing the correlation coefficients of fusion methods and individual scores with subjective scores to demonstrate the robustness and generalizability of the fusion metrics.", "authors": ["Shreyas Shivakumara", "Gabriel Eilertsen", "Karljohan Lundin Palmerius"], "published_date": "2025-09-16T08:59:33Z", "url": "http://arxiv.org/abs/2509.12836v1", "pdf_url": "http://arxiv.org/pdf/2509.12836v1", "primary_category": "cs.CV", "journal_ref": null, "doi": null}
{"doc_id": "2509.12458v1", "title": "Neural 3D Object Reconstruction with Small-Scale Unmanned Aerial   Vehicles", "abstract": "Small Unmanned Aerial Vehicles (UAVs) exhibit immense potential for navigating indoor and hard-to-reach areas, yet their significant constraints in payload and autonomy have largely prevented their use for complex tasks like high-quality 3-Dimensional (3D) reconstruction. To overcome this challenge, we introduce a novel system architecture that enables fully autonomous, high-fidelity 3D scanning of static objects using UAVs weighing under 100 grams. Our core innovation lies in a dual-reconstruction pipeline that creates a real-time feedback loop between data capture and flight control. A near-real-time (near-RT) process uses Structure from Motion (SfM) to generate an instantaneous pointcloud of the object. The system analyzes the model quality on the fly and dynamically adapts the UAV's trajectory to intelligently capture new images of poorly covered areas. This ensures comprehensive data acquisition. For the final, detailed output, a non-real-time (non-RT) pipeline employs a Neural Radiance Fields (NeRF)-based Neural 3D Reconstruction (N3DR) approach, fusing SfM-derived camera poses with precise Ultra Wide-Band (UWB) location data to achieve superior accuracy. We implemented and validated this architecture using Crazyflie 2.1 UAVs. Our experiments, conducted in both single- and multi-UAV configurations, conclusively show that dynamic trajectory adaptation consistently improves reconstruction quality over static flight paths. This work demonstrates a scalable and autonomous solution that unlocks the potential of miniaturized UAVs for fine-grained 3D reconstruction in constrained environments, a capability previously limited to much larger platforms.", "authors": ["\u00c0lmos Veres-Vit\u00e0lyos", "Genis Castillo Gomez-Raya", "Filip Lemic", "Daniel Johannes Bugelnig", "Bernhard Rinner", "Sergi Abadal", "Xavier Costa-P\u00e9rez"], "published_date": "2025-09-15T21:08:32Z", "url": "http://arxiv.org/abs/2509.12458v1", "pdf_url": "http://arxiv.org/pdf/2509.12458v1", "primary_category": "cs.RO", "journal_ref": null, "doi": null}
{"doc_id": "2509.11885v1", "title": "BREA-Depth: Bronchoscopy Realistic Airway-geometric Depth Estimation", "abstract": "Monocular depth estimation in bronchoscopy can significantly improve real-time navigation accuracy and enhance the safety of interventions in complex, branching airways. Recent advances in depth foundation models have shown promise for endoscopic scenarios, yet these models often lack anatomical awareness in bronchoscopy, overfitting to local textures rather than capturing the global airway structure, particularly under ambiguous depth cues and poor lighting. To address this, we propose Brea-Depth, a novel framework that integrates airway-specific geometric priors into foundation model adaptation for bronchoscopic depth estimation. Our method introduces a depth-aware CycleGAN, refining the translation between real bronchoscopic images and airway geometries from anatomical data, effectively bridging the domain gap. In addition, we introduce an airway structure awareness loss to enforce depth consistency within the airway lumen while preserving smooth transitions and structural integrity. By incorporating anatomical priors, Brea-Depth enhances model generalization and yields more robust, accurate 3D airway reconstructions. To assess anatomical realism, we introduce Airway Depth Structure Evaluation, a new metric for structural consistency. We validate BREA-Depth on a collected ex vivo human lung dataset and an open bronchoscopic dataset, where it outperforms existing methods in anatomical depth preservation.", "authors": ["Francis Xiatian Zhang", "Emile Mackute", "Mohammadreza Kasaei", "Kevin Dhaliwal", "Robert Thomson", "Mohsen Khadem"], "published_date": "2025-09-15T13:02:42Z", "url": "http://arxiv.org/abs/2509.11885v1", "pdf_url": "http://arxiv.org/pdf/2509.11885v1", "primary_category": "cs.CV", "journal_ref": null, "doi": null}
{"doc_id": "2509.11411v1", "title": "On the Skinning of Gaussian Avatars", "abstract": "Radiance field-based methods have recently been used to reconstruct human avatars, showing that we can significantly downscale the systems needed for creating animated human avatars. Although this progress has been initiated by neural radiance fields, their slow rendering and backward mapping from the observation space to the canonical space have been the main challenges. With Gaussian splatting overcoming both challenges, a new family of approaches has emerged that are faster to train and render, while also straightforward to implement using forward skinning from the canonical to the observation space. However, the linear blend skinning required for the deformation of the Gaussians does not provide valid results for their non-linear rotation properties. To address such artifacts, recent works use mesh properties to rotate the non-linear Gaussian properties or train models to predict corrective offsets. Instead, we propose a weighted rotation blending approach that leverages quaternion averaging. This leads to simpler vertex-based Gaussians that can be efficiently animated and integrated in any engine by only modifying the linear blend skinning technique, and using any Gaussian rasterizer.", "authors": ["Nikolaos Zioulis", "Nikolaos Kotarelas", "Georgios Albanis", "Spyridon Thermos", "Anargyros Chatzitofis"], "published_date": "2025-09-14T19:58:48Z", "url": "http://arxiv.org/abs/2509.11411v1", "pdf_url": "http://arxiv.org/pdf/2509.11411v1", "primary_category": "cs.CV", "journal_ref": null, "doi": null}
{"doc_id": "2509.11275v1", "title": "ROSGS: Relightable Outdoor Scenes With Gaussian Splatting", "abstract": "Image data captured outdoors often exhibit unbounded scenes and unconstrained, varying lighting conditions, making it challenging to decompose them into geometry, reflectance, and illumination. Recent works have focused on achieving this decomposition using Neural Radiance Fields (NeRF) or the 3D Gaussian Splatting (3DGS) representation but remain hindered by two key limitations: the high computational overhead associated with neural networks of NeRF and the use of low-frequency lighting representations, which often result in inefficient rendering and suboptimal relighting accuracy. We propose ROSGS, a two-stage pipeline designed to efficiently reconstruct relightable outdoor scenes using the Gaussian Splatting representation. By leveraging monocular normal priors, ROSGS first reconstructs the scene's geometry with the compact 2D Gaussian Splatting (2DGS) representation, providing an efficient and accurate geometric foundation. Building upon this reconstructed geometry, ROSGS then decomposes the scene's texture and lighting through a hybrid lighting model. This model effectively represents typical outdoor lighting by employing a spherical Gaussian function to capture the directional, high-frequency components of sunlight, while learning a radiance transfer function via Spherical Harmonic coefficients to model the remaining low-frequency skylight comprehensively. Both quantitative metrics and qualitative comparisons demonstrate that ROSGS achieves state-of-the-art performance in relighting outdoor scenes and highlight its ability to deliver superior relighting accuracy and rendering efficiency.", "authors": ["Lianjun Liao", "Chunhui Zhang", "Tong Wu", "Henglei Lv", "Bailin Deng", "Lin Gao"], "published_date": "2025-09-14T13:58:58Z", "url": "http://arxiv.org/abs/2509.11275v1", "pdf_url": "http://arxiv.org/pdf/2509.11275v1", "primary_category": "cs.CV", "journal_ref": null, "doi": null}
{"doc_id": "2509.11169v1", "title": "Multispectral-NeRF:a multispectral modeling approach based on neural   radiance fields", "abstract": "3D reconstruction technology generates three-dimensional representations of real-world objects, scenes, or environments using sensor data such as 2D images, with extensive applications in robotics, autonomous vehicles, and virtual reality systems. Traditional 3D reconstruction techniques based on 2D images typically relies on RGB spectral information. With advances in sensor technology, additional spectral bands beyond RGB have been increasingly incorporated into 3D reconstruction workflows. Existing methods that integrate these expanded spectral data often suffer from expensive scheme prices, low accuracy and poor geometric features. Three - dimensional reconstruction based on NeRF can effectively address the various issues in current multispectral 3D reconstruction methods, producing high - precision and high - quality reconstruction results. However, currently, NeRF and some improved models such as NeRFacto are trained on three - band data and cannot take into account the multi - band information. To address this problem, we propose Multispectral-NeRF, an enhanced neural architecture derived from NeRF that can effectively integrates multispectral information. Our technical contributions comprise threefold modifications: Expanding hidden layer dimensionality to accommodate 6-band spectral inputs; Redesigning residual functions to optimize spectral discrepancy calculations between reconstructed and reference images; Adapting data compression modules to address the increased bit-depth requirements of multispectral imagery. Experimental results confirm that Multispectral-NeRF successfully processes multi-band spectral features while accurately preserving the original scenes' spectral characteristics.", "authors": ["Hong Zhang", "Fei Guo", "Zihan Xie", "Dizhao Yao"], "published_date": "2025-09-14T09:04:35Z", "url": "http://arxiv.org/abs/2509.11169v1", "pdf_url": "http://arxiv.org/pdf/2509.11169v1", "primary_category": "cs.CV", "journal_ref": null, "doi": null}
{"doc_id": "2509.10405v1", "title": "Self-supervised Learning Of Visual Pose Estimation Without Pose Labels   By Classifying LED States", "abstract": "We introduce a model for monocular RGB relative pose estimation of a ground robot that trains from scratch without pose labels nor prior knowledge about the robot's shape or appearance. At training time, we assume: (i) a robot fitted with multiple LEDs, whose states are independent and known at each frame; (ii) knowledge of the approximate viewing direction of each LED; and (iii) availability of a calibration image with a known target distance, to address the ambiguity of monocular depth estimation. Training data is collected by a pair of robots moving randomly without needing external infrastructure or human supervision. Our model trains on the task of predicting from an image the state of each LED on the robot. In doing so, it learns to predict the position of the robot in the image, its distance, and its relative bearing. At inference time, the state of the LEDs is unknown, can be arbitrary, and does not affect the pose estimation performance. Quantitative experiments indicate that our approach: is competitive with SoA approaches that require supervision from pose labels or a CAD model of the robot; generalizes to different domains; and handles multi-robot pose estimation.", "authors": ["Nicholas Carlotti", "Mirko Nava", "Alessandro Giusti"], "published_date": "2025-09-12T16:54:56Z", "url": "http://arxiv.org/abs/2509.10405v1", "pdf_url": "http://arxiv.org/pdf/2509.10405v1", "primary_category": "cs.RO", "journal_ref": null, "doi": null}
{"doc_id": "2509.08027v1", "title": "MCTED: A Machine-Learning-Ready Dataset for Digital Elevation Model   Generation From Mars Imagery", "abstract": "This work presents a new dataset for the Martian digital elevation model prediction task, ready for machine learning applications called MCTED. The dataset has been generated using a comprehensive pipeline designed to process high-resolution Mars orthoimage and DEM pairs from Day et al., yielding a dataset consisting of 80,898 data samples. The source images are data gathered by the Mars Reconnaissance Orbiter using the CTX instrument, providing a very diverse and comprehensive coverage of the Martian surface. Given the complexity of the processing pipelines used in large-scale DEMs, there are often artefacts and missing data points in the original data, for which we developed tools to solve or mitigate their impact. We divide the processed samples into training and validation splits, ensuring samples in both splits cover no mutual areas to avoid data leakage. Every sample in the dataset is represented by the optical image patch, DEM patch, and two mask patches, indicating values that were originally missing or were altered by us. This allows future users of the dataset to handle altered elevation regions as they please. We provide statistical insights of the generated dataset, including the spatial distribution of samples, the distributions of elevation values, slopes and more. Finally, we train a small U-Net architecture on the MCTED dataset and compare its performance to a monocular depth estimation foundation model, DepthAnythingV2, on the task of elevation prediction. We find that even a very small architecture trained on this dataset specifically, beats a zero-shot performance of a depth estimation foundation model like DepthAnythingV2. We make the dataset and code used for its generation completely open source in public repositories.", "authors": ["Rafa\u0142 Osadnik", "Pablo G\u00f3mez", "Eleni Bohacek", "Rickbir Bahia"], "published_date": "2025-09-09T13:14:49Z", "url": "http://arxiv.org/abs/2509.08027v1", "pdf_url": "http://arxiv.org/pdf/2509.08027v1", "primary_category": "cs.CV", "journal_ref": null, "doi": null}
{"doc_id": "2509.06685v3", "title": "VIM-GS: Visual-Inertial Monocular Gaussian Splatting via Object-level   Guidance in Large Scenes", "abstract": "VIM-GS is a Gaussian Splatting (GS) framework using monocular images for novel-view synthesis (NVS) in large scenes. GS typically requires accurate depth to initiate Gaussian ellipsoids using RGB-D/stereo cameras. Their limited depth sensing range makes it difficult for GS to work in large scenes. Monocular images, however, lack depth to guide the learning and lead to inferior NVS results. Although large foundation models (LFMs) for monocular depth estimation are available, they suffer from cross-frame inconsistency, inaccuracy for distant scenes, and ambiguity in deceptive texture cues. This paper aims to generate dense, accurate depth images from monocular RGB inputs for high-definite GS rendering. The key idea is to leverage the accurate but sparse depth from visual-inertial Structure-from-Motion (SfM) to refine the dense but coarse depth from LFMs. To bridge the sparse input and dense output, we propose an object-segmented depth propagation algorithm that renders the depth of pixels of structured objects. Then we develop a dynamic depth refinement module to handle the crippled SfM depth of dynamic objects and refine the coarse LFM depth. Experiments using public and customized datasets demonstrate the superior rendering quality of VIM-GS in large scenes.", "authors": ["Shengkai Zhang", "Yuhe Liu", "Guanjun Wu", "Jianhua He", "Xinggang Wang", "Mozi Chen", "Kezhong Liu"], "published_date": "2025-09-08T13:41:10Z", "url": "http://arxiv.org/abs/2509.06685v3", "pdf_url": "http://arxiv.org/pdf/2509.06685v3", "primary_category": "cs.CV", "journal_ref": null, "doi": null}
{"doc_id": "2509.04379v1", "title": "SSGaussian: Semantic-Aware and Structure-Preserving 3D Style Transfer", "abstract": "Recent advancements in neural representations, such as Neural Radiance Fields and 3D Gaussian Splatting, have increased interest in applying style transfer to 3D scenes. While existing methods can transfer style patterns onto 3D-consistent neural representations, they struggle to effectively extract and transfer high-level style semantics from the reference style image. Additionally, the stylized results often lack structural clarity and separation, making it difficult to distinguish between different instances or objects within the 3D scene. To address these limitations, we propose a novel 3D style transfer pipeline that effectively integrates prior knowledge from pretrained 2D diffusion models. Our pipeline consists of two key stages: First, we leverage diffusion priors to generate stylized renderings of key viewpoints. Then, we transfer the stylized key views onto the 3D representation. This process incorporates two innovative designs. The first is cross-view style alignment, which inserts cross-view attention into the last upsampling block of the UNet, allowing feature interactions across multiple key views. This ensures that the diffusion model generates stylized key views that maintain both style fidelity and instance-level consistency. The second is instance-level style transfer, which effectively leverages instance-level consistency across stylized key views and transfers it onto the 3D representation. This results in a more structured, visually coherent, and artistically enriched stylization. Extensive qualitative and quantitative experiments demonstrate that our 3D style transfer pipeline significantly outperforms state-of-the-art methods across a wide range of scenes, from forward-facing to challenging 360-degree environments. Visit our project page https://jm-xu.github.io/SSGaussian for immersive visualization.", "authors": ["Jimin Xu", "Bosheng Qin", "Tao Jin", "Zhou Zhao", "Zhenhui Ye", "Jun Yu", "Fei Wu"], "published_date": "2025-09-04T16:40:44Z", "url": "http://arxiv.org/abs/2509.04379v1", "pdf_url": "http://arxiv.org/pdf/2509.04379v1", "primary_category": "cs.CV", "journal_ref": null, "doi": null}
{"doc_id": "2509.03012v1", "title": "Uncertainty-aware Test-Time Training (UT$^3$) for Efficient On-the-fly   Domain Adaptive Dense Regression", "abstract": "Deep neural networks (DNNs) are increasingly being used in autonomous systems. However, DNNs do not generalize well to domain shift. Adapting to a continuously evolving environment is a safety-critical challenge inevitably faced by all autonomous systems deployed to the real world. Recent work on test-time training proposes methods that adapt to a new test distribution on the fly by optimizing the DNN model for each test input using self-supervision. However, these techniques result in a sharp increase in inference time as multiple forward and backward passes are required for a single test sample (for test-time training) before finally making the prediction based on the fine-tuned features. This is undesirable for real-world robotics applications where these models may be deployed to resource constraint hardware with strong latency requirements. In this work, we propose a new framework (called UT$^3$) that leverages test-time training for improved performance in the presence of continuous domain shift while also decreasing the inference time, making it suitable for real-world applications. Our method proposes an uncertainty-aware self-supervision task for efficient test-time training that leverages the quantified uncertainty to selectively apply the training leading to sharp improvements in the inference time while performing comparably to standard test-time training protocol. Our proposed protocol offers a continuous setting to identify the selected keyframes, allowing the end-user to control how often to apply test-time training. We demonstrate the efficacy of our method on a dense regression task - monocular depth estimation.", "authors": ["Uddeshya Upadhyay"], "published_date": "2025-09-03T04:41:43Z", "url": "http://arxiv.org/abs/2509.03012v1", "pdf_url": "http://arxiv.org/pdf/2509.03012v1", "primary_category": "cs.RO", "journal_ref": null, "doi": null}
{"doc_id": "2509.02474v1", "title": "Unifi3D: A Study on 3D Representations for Generation and Reconstruction   in a Common Framework", "abstract": "Following rapid advancements in text and image generation, research has increasingly shifted towards 3D generation. Unlike the well-established pixel-based representation in images, 3D representations remain diverse and fragmented, encompassing a wide variety of approaches such as voxel grids, neural radiance fields, signed distance functions, point clouds, or octrees, each offering distinct advantages and limitations. In this work, we present a unified evaluation framework designed to assess the performance of 3D representations in reconstruction and generation. We compare these representations based on multiple criteria: quality, computational efficiency, and generalization performance. Beyond standard model benchmarking, our experiments aim to derive best practices over all steps involved in the 3D generation pipeline, including preprocessing, mesh reconstruction, compression with autoencoders, and generation. Our findings highlight that reconstruction errors significantly impact overall performance, underscoring the need to evaluate generation and reconstruction jointly. We provide insights that can inform the selection of suitable 3D models for various applications, facilitating the development of more robust and application-specific solutions in 3D generation. The code for our framework is available at https://github.com/isl-org/unifi3d.", "authors": ["Nina Wiedemann", "Sainan Liu", "Quentin Leboutet", "Katelyn Gao", "Benjamin Ummenhofer", "Michael Paulitsch", "Kai Yuan"], "published_date": "2025-09-02T16:25:12Z", "url": "http://arxiv.org/abs/2509.02474v1", "pdf_url": "http://arxiv.org/pdf/2509.02474v1", "primary_category": "cs.GR", "journal_ref": null, "doi": null}
{"doc_id": "2509.01206v2", "title": "EndoGeDE: Generalizable Monocular Depth Estimation with Mixture of   Low-Rank Experts for Diverse Endoscopic Scenes", "abstract": "Self-supervised monocular depth estimation is a significant task for low-cost and efficient 3D scene perception in endoscopy. In recent years, a series of methods are proposed to address the illumination inconsistency, while certain works also focus on the generalization of the model by efficiently finetuning the foundation models. However, the variety of illumination conditions and scene features is still the primary challenges for depth estimation in endoscopic scenes. In this work, a self-supervised framework is proposed for monocular depth estimation in diverse endoscopy. Firstly, considering the diverse features in endoscopic scenes with different tissues, a novel block-wise mixture of dynamic low-rank experts is proposed to efficiently finetune the foundation model for endoscopic depth estimation. In the proposed module, based on the input feature, different experts with a small amount of trainable parameters are adaptively selected for weighted inference, from low-rank experts which are allocated based on the generalization of each block. Moreover, a novel self-supervised training framework is proposed to jointly cope with brightness inconsistency and reflectance interference. The proposed method outperforms state-of-the-art works on SCARED dataset and SimCol dataset. Furthermore, the proposed network also achieves the best generalization based on zero-shot depth estimation on C3VD, Hamlyn and SERV-CT dataset. The outstanding performance of our model is further demonstrated with 3D reconstruction and ego-motion estimation. The proposed method could contribute to accurate endoscopy for minimally invasive measurement and surgery. The evaluation codes will be released upon acceptance, while the demo videos can be found on: https://endo-gede.netlify.app/.", "authors": ["Liangjing Shao", "Benshuang Chen", "Chenkang Du", "Xueli Liu", "Xinrong Chen"], "published_date": "2025-09-01T07:45:12Z", "url": "http://arxiv.org/abs/2509.01206v2", "pdf_url": "http://arxiv.org/pdf/2509.01206v2", "primary_category": "cs.CV", "journal_ref": null, "doi": null}
