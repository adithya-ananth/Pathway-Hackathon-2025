{"doc_id": "2007.11898v2", "title": "ORB-SLAM3: An Accurate Open-Source Library for Visual, Visual-Inertial   and Multi-Map SLAM", "abstract": "This paper presents ORB-SLAM3, the first system able to perform visual, visual-inertial and multi-map SLAM with monocular, stereo and RGB-D cameras, using pin-hole and fisheye lens models. The first main novelty is a feature-based tightly-integrated visual-inertial SLAM system that fully relies on Maximum-a-Posteriori (MAP) estimation, even during the IMU initialization phase. The result is a system that operates robustly in real-time, in small and large, indoor and outdoor environments, and is 2 to 5 times more accurate than previous approaches. The second main novelty is a multiple map system that relies on a new place recognition method with improved recall. Thanks to it, ORB-SLAM3 is able to survive to long periods of poor visual information: when it gets lost, it starts a new map that will be seamlessly merged with previous maps when revisiting mapped areas. Compared with visual odometry systems that only use information from the last few seconds, ORB-SLAM3 is the first system able to reuse in all the algorithm stages all previous information. This allows to include in bundle adjustment co-visible keyframes, that provide high parallax observations boosting accuracy, even if they are widely separated in time or if they come from a previous mapping session. Our experiments show that, in all sensor configurations, ORB-SLAM3 is as robust as the best systems available in the literature, and significantly more accurate. Notably, our stereo-inertial SLAM achieves an average accuracy of 3.6 cm on the EuRoC drone and 9 mm under quick hand-held motions in the room of TUM-VI dataset, a setting representative of AR/VR scenarios. For the benefit of the community we make public the source code.", "authors": ["Carlos Campos", "Richard Elvira", "Juan J. G\u00f3mez Rodr\u00edguez", "Jos\u00e9 M. M. Montiel", "Juan D. Tard\u00f3s"], "published_date": "2020-07-23T10:09:54Z", "url": "http://arxiv.org/abs/2007.11898v2", "pdf_url": "http://arxiv.org/pdf/2007.11898v2", "primary_category": "cs.RO"}
{"doc_id": "1912.05405v1", "title": "Training Deep SLAM on Single Frames", "abstract": "Learning-based visual odometry and SLAM methods demonstrate a steady improvement over past years. However, collecting ground truth poses to train these methods is difficult and expensive. This could be resolved by training in an unsupervised mode, but there is still a large gap between performance of unsupervised and supervised methods. In this work, we focus on generating synthetic data for deep learning-based visual odometry and SLAM methods that take optical flow as an input. We produce training data in a form of optical flow that corresponds to arbitrary camera movement between a real frame and a virtual frame. For synthesizing data we use depth maps either produced by a depth sensor or estimated from stereo pair. We train visual odometry model on synthetic data and do not use ground truth poses hence this model can be considered unsupervised. Also it can be classified as monocular as we do not use depth maps on inference. We also propose a simple way to convert any visual odometry model into a SLAM method based on frame matching and graph optimization. We demonstrate that both the synthetically-trained visual odometry model and the proposed SLAM method build upon this model yields state-of-the-art results among unsupervised methods on KITTI dataset and shows promising results on a challenging EuRoC dataset.", "authors": ["Igor Slinko", "Anna Vorontsova", "Dmitry Zhukov", "Olga Barinova", "Anton Konushin"], "published_date": "2019-12-11T16:02:20Z", "url": "http://arxiv.org/abs/1912.05405v1", "pdf_url": "http://arxiv.org/pdf/1912.05405v1", "primary_category": "cs.CV"}
{"doc_id": "1912.01064v1", "title": "A Keyframe-based Continuous Visual SLAM for RGB-D Cameras via   Nonparametric Joint Geometric and Appearance Representation", "abstract": "This paper reports on a robust RGB-D SLAM system that performs well in scarcely textured and structured environments. We present a novel keyframe-based continuous visual odometry that builds on the recently developed continuous sensor registration framework. A joint geometric and appearance representation is the result of transforming the RGB-D images into functions that live in a Reproducing Kernel Hilbert Space (RKHS). We solve both registration and keyframe selection problems via the inner product structure available in the RKHS. We also extend the proposed keyframe-based odometry method to a SLAM system using indirect ORB loop-closure constraints. The experimental evaluations using publicly available RGB-D benchmarks show that the developed keyframe selection technique using continuous visual odometry outperforms its robust dense (and direct) visual odometry equivalent. In addition, the developed SLAM system has better generalization across different training and validation sequences; it is robust to the lack of texture and structure in the scene; and shows comparable performance with the state-of-the-art SLAM systems.", "authors": ["Xi Lin", "Dingyi Sun", "Tzu-Yuan Lin", "Ryan M. Eustice", "Maani Ghaffari"], "published_date": "2019-12-02T20:10:21Z", "url": "http://arxiv.org/abs/1912.01064v1", "pdf_url": "http://arxiv.org/pdf/1912.01064v1", "primary_category": "cs.RO"}
{"doc_id": "2406.00929v1", "title": "Self-Supervised Geometry-Guided Initialization for Robust Monocular   Visual Odometry", "abstract": "Monocular visual odometry is a key technology in a wide variety of autonomous systems. Relative to traditional feature-based methods, that suffer from failures due to poor lighting, insufficient texture, large motions, etc., recent learning-based SLAM methods exploit iterative dense bundle adjustment to address such failure cases and achieve robust accurate localization in a wide variety of real environments, without depending on domain-specific training data. However, despite its potential, learning-based SLAM still struggles with scenarios involving large motion and object dynamics. In this paper, we diagnose key weaknesses in a popular learning-based SLAM model (DROID-SLAM) by analyzing major failure cases on outdoor benchmarks and exposing various shortcomings of its optimization process. We then propose the use of self-supervised priors leveraging a frozen large-scale pre-trained monocular depth estimation to initialize the dense bundle adjustment process, leading to robust visual odometry without the need to fine-tune the SLAM backbone. Despite its simplicity, our proposed method demonstrates significant improvements on KITTI odometry, as well as the challenging DDAD benchmark. Code and pre-trained models will be released upon publication.", "authors": ["Takayuki Kanai", "Igor Vasiljevic", "Vitor Guizilini", "Kazuhiro Shintani"], "published_date": "2024-06-03T01:59:29Z", "url": "http://arxiv.org/abs/2406.00929v1", "pdf_url": "http://arxiv.org/pdf/2406.00929v1", "primary_category": "cs.CV"}
{"doc_id": "1710.02081v1", "title": "Online Photometric Calibration for Auto Exposure Video for Realtime   Visual Odometry and SLAM", "abstract": "Recent direct visual odometry and SLAM algorithms have demonstrated impressive levels of precision. However, they require a photometric camera calibration in order to achieve competitive results. Hence, the respective algorithm cannot be directly applied to an off-the-shelf-camera or to a video sequence acquired with an unknown camera. In this work we propose a method for online photometric calibration which enables to process auto exposure videos with visual odometry precisions that are on par with those of photometrically calibrated videos. Our algorithm recovers the exposure times of consecutive frames, the camera response function, and the attenuation factors of the sensor irradiance due to vignetting. Gain robust KLT feature tracks are used to obtain scene point correspondences as input to a nonlinear optimization framework. We show that our approach can reliably calibrate arbitrary video sequences by evaluating it on datasets for which full photometric ground truth is available. We further show that our calibration can improve the performance of a state-of-the-art direct visual odometry method that works solely on pixel intensities, calibrating for photometric parameters in an online fashion in realtime.", "authors": ["Paul Bergmann", "Rui Wang", "Daniel Cremers"], "published_date": "2017-10-05T15:49:13Z", "url": "http://arxiv.org/abs/1710.02081v1", "pdf_url": "http://arxiv.org/pdf/1710.02081v1", "primary_category": "cs.CV"}
{"doc_id": "1804.03558v2", "title": "Evaluation of the visual odometry methods for semi-dense real-time", "abstract": "Recent decades have witnessed a significant increase in the use of visual odometry(VO) in the computer vision area. It has also been used in varieties of robotic applications, for example on the Mars Exploration Rovers. This paper, firstly, discusses two popular existing visual odometry approaches, namely LSD-SLAM and ORB-SLAM2 to improve the performance metrics of visual SLAM systems using Umeyama Method. We carefully evaluate the methods referred to above on three different well-known KITTI datasets, EuRoC MAV dataset, and TUM RGB-D dataset to obtain the best results and graphically compare the results to evaluation metrics from different visual odometry approaches. Secondly, we propose an approach running in real-time with a stereo camera, which combines an existing feature-based (indirect) method and an existing feature-less (direct) method matching with accurate semidense direct image alignment and reconstructing an accurate 3D environment directly on pixels that have image gradient. Keywords VO, performance metrics, Umeyama Method, feature-based method, feature-less method & semi-dense real-time.", "authors": ["Haidara Gaoussou", "Peng Dewei"], "published_date": "2018-04-10T14:28:08Z", "url": "http://arxiv.org/abs/1804.03558v2", "pdf_url": "http://arxiv.org/pdf/1804.03558v2", "primary_category": "cs.CV"}
{"doc_id": "1909.07267v4", "title": "A Fast and Robust Place Recognition Approach for Stereo Visual Odometry   Using LiDAR Descriptors", "abstract": "Place recognition is a core component of Simultaneous Localization and Mapping (SLAM) algorithms. Particularly in visual SLAM systems, previously-visited places are recognized by measuring the appearance similarity between images representing these locations. However, such approaches are sensitive to visual appearance change and also can be computationally expensive. In this paper, we propose an alternative approach adapting LiDAR descriptors for 3D points obtained from stereo-visual odometry for place recognition. 3D points are potentially more reliable than 2D visual cues (e.g., 2D features) against environmental changes (e.g., variable illumination) and this may benefit visual SLAM systems in long-term deployment scenarios. Stereo-visual odometry generates 3D points with an absolute scale, which enables us to use LiDAR descriptors for place recognition with high computational efficiency. Through extensive evaluations on standard benchmark datasets, we demonstrate the accuracy, efficiency, and robustness of using 3D points for place recognition over 2D methods.", "authors": ["Jiawei Mo", "Junaed Sattar"], "published_date": "2019-09-16T15:14:55Z", "url": "http://arxiv.org/abs/1909.07267v4", "pdf_url": "http://arxiv.org/pdf/1909.07267v4", "primary_category": "cs.CV"}
{"doc_id": "2205.05916v2", "title": "Dynamic Dense RGB-D SLAM using Learning-based Visual Odometry", "abstract": "We propose a dense dynamic RGB-D SLAM pipeline based on a learning-based visual odometry, TartanVO. TartanVO, like other direct methods rather than feature-based, estimates camera pose through dense optical flow, which only applies to static scenes and disregards dynamic objects. Due to the color constancy assumption, optical flow is not able to differentiate between dynamic and static pixels. Therefore, to reconstruct a static map through such direct methods, our pipeline resolves dynamic/static segmentation by leveraging the optical flow output, and only fuse static points into the map. Moreover, we rerender the input frames such that the dynamic pixels are removed and iteratively pass them back into the visual odometry to refine the pose estimate.", "authors": ["Shihao Shen", "Yilin Cai", "Jiayi Qiu", "Guangzhao Li"], "published_date": "2022-05-12T07:11:41Z", "url": "http://arxiv.org/abs/2205.05916v2", "pdf_url": "http://arxiv.org/pdf/2205.05916v2", "primary_category": "cs.RO"}
{"doc_id": "2309.05249v3", "title": "Evaluating Visual Odometry Methods for Autonomous Driving in Rain", "abstract": "The increasing demand for autonomous vehicles has created a need for robust navigation systems that can also operate effectively in adverse weather conditions. Visual odometry is a technique used in these navigation systems, enabling the estimation of vehicle position and motion using input from onboard cameras. However, visual odometry accuracy can be significantly impacted in challenging weather conditions, such as heavy rain, snow, or fog. In this paper, we evaluate a range of visual odometry methods, including our DROID-SLAM based heuristic approach. Specifically, these algorithms are tested on both clear and rainy weather urban driving data to evaluate their robustness. We compiled a dataset comprising of a range of rainy weather conditions from different cities. This includes, the Oxford Robotcar dataset from Oxford, the 4Seasons dataset from Munich and an internal dataset collected in Singapore. We evaluated different visual odometry algorithms for both monocular and stereo camera setups using the Absolute Trajectory Error (ATE). From the range of approaches evaluated, our findings suggest that the Depth and Flow for Visual Odometry (DF-VO) algorithm with monocular setup performed the best for short range distances (< 500m) and our proposed DROID-SLAM based heuristic approach for the stereo setup performed relatively well for long-term localization. Both VO algorithms suggested a need for a more robust sensor fusion based approach for localization in rain.", "authors": ["Yu Xiang Tan", "Marcel Bartholomeus Prasetyo", "Mohammad Alif Daffa", "Deshpande Sunny Nitin", "Malika Meghjani"], "published_date": "2023-09-11T05:55:01Z", "url": "http://arxiv.org/abs/2309.05249v3", "pdf_url": "http://arxiv.org/pdf/2309.05249v3", "primary_category": "cs.RO"}
{"doc_id": "1806.05842v3", "title": "Real-time Monocular Visual Odometry for Turbid and Dynamic Underwater   Environments", "abstract": "In the context of robotic underwater operations, the visual degradations induced by the medium properties make difficult the exclusive use of cameras for localization purpose. Hence, most localization methods are based on expensive navigational sensors associated with acoustic positioning. On the other hand, visual odometry and visual SLAM have been exhaustively studied for aerial or terrestrial applications, but state-of-the-art algorithms fail underwater. In this paper we tackle the problem of using a simple low-cost camera for underwater localization and propose a new monocular visual odometry method dedicated to the underwater environment. We evaluate different tracking methods and show that optical flow based tracking is more suited to underwater images than classical approaches based on descriptors. We also propose a keyframe-based visual odometry approach highly relying on nonlinear optimization. The proposed algorithm has been assessed on both simulated and real underwater datasets and outperforms state-of-the-art visual SLAM methods under many of the most challenging conditions. The main application of this work is the localization of Remotely Operated Vehicles (ROVs) used for underwater archaeological missions but the developed system can be used in any other applications as long as visual information is available.", "authors": ["Maxime Ferrera", "Julien Moras", "Pauline Trouv\u00e9-Peloux", "Vincent Creuze"], "published_date": "2018-06-15T07:47:00Z", "url": "http://arxiv.org/abs/1806.05842v3", "pdf_url": "http://arxiv.org/pdf/1806.05842v3", "primary_category": "cs.RO"}
{"doc_id": "2311.12580v1", "title": "CoVOR-SLAM: Cooperative SLAM using Visual Odometry and Ranges for   Multi-Robot Systems", "abstract": "A swarm of robots has advantages over a single robot, since it can explore larger areas much faster and is more robust to single-point failures. Accurate relative positioning is necessary to successfully carry out a collaborative mission without collisions. When Visual Simultaneous Localization and Mapping (VSLAM) is used to estimate the poses of each robot, inter-agent loop closing is widely applied to reduce the relative positioning errors. This technique can mitigate errors using the feature points commonly observed by different robots. However, it requires significant computing and communication capabilities to detect inter-agent loops, and to process the data transmitted by multiple agents. In this paper, we propose Collaborative SLAM using Visual Odometry and Range measurements (CoVOR-SLAM) to overcome this challenge. In the framework of CoVOR-SLAM, robots only need to exchange pose estimates, covariances (uncertainty) of the estimates, and range measurements between robots. Since CoVOR-SLAM does not require to associate visual features and map points observed by different agents, the computational and communication loads are significantly reduced. The required range measurements can be obtained using pilot signals of the communication system, without requiring complex additional infrastructure. We tested CoVOR-SLAM using real images as well as real ultra-wideband-based ranges obtained with two rovers. In addition, CoVOR-SLAM is evaluated with a larger scale multi-agent setup exploiting public image datasets and ranges generated using a realistic simulation. The results show that CoVOR-SLAM can accurately estimate the robots' poses, requiring much less computational power and communication capabilities than the inter-agent loop closing technique.", "authors": ["Young-Hee Lee", "Chen Zhu", "Thomas Wiedemann", "Emanuel Staudinger", "Siwei Zhang", "Christoph G\u00fcnther"], "published_date": "2023-11-21T12:56:23Z", "url": "http://arxiv.org/abs/2311.12580v1", "pdf_url": "http://arxiv.org/pdf/2311.12580v1", "primary_category": "cs.RO"}
{"doc_id": "2312.16800v1", "title": "SR-LIVO: LiDAR-Inertial-Visual Odometry and Mapping with Sweep   Reconstruction", "abstract": "Existing LiDAR-inertial-visual odometry and mapping (LIV-SLAM) systems mainly utilize the LiDAR-inertial odometry (LIO) module for structure reconstruction and the visual-inertial odometry (VIO) module for color rendering. However, the accuracy of VIO is often compromised by photometric changes, weak textures and motion blur, unlike the more robust LIO. This paper introduces SR-LIVO, an advanced and novel LIV-SLAM system employing sweep reconstruction to align reconstructed sweeps with image timestamps. This allows the LIO module to accurately determine states at all imaging moments, enhancing pose accuracy and processing efficiency. Experimental results on two public datasets demonstrate that: 1) our SRLIVO outperforms existing state-of-the-art LIV-SLAM systems in both pose accuracy and time efficiency; 2) our LIO-based pose estimation prove more accurate than VIO-based ones in several mainstream LIV-SLAM systems (including ours). We have released our source code to contribute to the community development in this field.", "authors": ["Zikang Yuan", "Jie Deng", "Ruiye Ming", "Fengtian Lang", "Xin Yang"], "published_date": "2023-12-28T03:06:49Z", "url": "http://arxiv.org/abs/2312.16800v1", "pdf_url": "http://arxiv.org/pdf/2312.16800v1", "primary_category": "cs.CV"}
{"doc_id": "2408.01654v1", "title": "Deep Patch Visual SLAM", "abstract": "Recent work in visual SLAM has shown the effectiveness of using deep network backbones. Despite excellent accuracy, however, such approaches are often expensive to run or do not generalize well zero-shot. Their runtime can also fluctuate wildly while their frontend and backend fight for access to GPU resources. To address these problems, we introduce Deep Patch Visual (DPV) SLAM, a method for monocular visual SLAM on a single GPU. DPV-SLAM maintains a high minimum framerate and small memory overhead (5-7G) compared to existing deep SLAM systems. On real-world datasets, DPV-SLAM runs at 1x-4x real-time framerates. We achieve comparable accuracy to DROID-SLAM on EuRoC and TartanAir while running 2.5x faster using a fraction of the memory. DPV-SLAM is an extension to the DPVO visual odometry system; its code can be found in the same repository: https://github.com/princeton-vl/DPVO", "authors": ["Lahav Lipson", "Zachary Teed", "Jia Deng"], "published_date": "2024-08-03T03:51:47Z", "url": "http://arxiv.org/abs/2408.01654v1", "pdf_url": "http://arxiv.org/pdf/2408.01654v1", "primary_category": "cs.CV"}
{"doc_id": "1611.06069v1", "title": "DeepVO: A Deep Learning approach for Monocular Visual Odometry", "abstract": "Deep Learning based techniques have been adopted with precision to solve a lot of standard computer vision problems, some of which are image classification, object detection and segmentation. Despite the widespread success of these approaches, they have not yet been exploited largely for solving the standard perception related problems encountered in autonomous navigation such as Visual Odometry (VO), Structure from Motion (SfM) and Simultaneous Localization and Mapping (SLAM). This paper analyzes the problem of Monocular Visual Odometry using a Deep Learning-based framework, instead of the regular 'feature detection and tracking' pipeline approaches. Several experiments were performed to understand the influence of a known/unknown environment, a conventional trackable feature and pre-trained activations tuned for object classification on the network's ability to accurately estimate the motion trajectory of the camera (or the vehicle). Based on these observations, we propose a Convolutional Neural Network architecture, best suited for estimating the object's pose under known environment conditions, and displays promising results when it comes to inferring the actual scale using just a single camera in real-time.", "authors": ["Vikram Mohanty", "Shubh Agrawal", "Shaswat Datta", "Arna Ghosh", "Vishnu Dutt Sharma", "Debashish Chakravarty"], "published_date": "2016-11-18T13:41:22Z", "url": "http://arxiv.org/abs/1611.06069v1", "pdf_url": "http://arxiv.org/pdf/1611.06069v1", "primary_category": "cs.CV"}
{"doc_id": "1912.08263v3", "title": "ViPR: Visual-Odometry-aided Pose Regression for 6DoF Camera Localization", "abstract": "Visual Odometry (VO) accumulates a positional drift in long-term robot navigation tasks. Although Convolutional Neural Networks (CNNs) improve VO in various aspects, VO still suffers from moving obstacles, discontinuous observation of features, and poor textures or visual information. While recent approaches estimate a 6DoF pose either directly from (a series of) images or by merging depth maps with optical flow (OF), research that combines absolute pose regression with OF is limited. We propose ViPR, a novel modular architecture for long-term 6DoF VO that leverages temporal information and synergies between absolute pose estimates (from PoseNet-like modules) and relative pose estimates (from FlowNet-based modules) by combining both through recurrent layers. Experiments on known datasets and on our own Industry dataset show that our modular design outperforms state of the art in long-term navigation tasks.", "authors": ["Felix Ott", "Tobias Feigl", "Christoffer L\u00f6ffler", "Christopher Mutschler"], "published_date": "2019-12-17T20:29:15Z", "url": "http://arxiv.org/abs/1912.08263v3", "pdf_url": "http://arxiv.org/pdf/1912.08263v3", "primary_category": "cs.CV"}
{"doc_id": "2001.02090v1", "title": "AD-VO: Scale-Resilient Visual Odometry Using Attentive Disparity Map", "abstract": "Visual odometry is an essential key for a localization module in SLAM systems. However, previous methods require tuning the system to adapt environment changes. In this paper, we propose a learning-based approach for frame-to-frame monocular visual odometry estimation. The proposed network is only learned by disparity maps for not only covering the environment changes but also solving the scale problem. Furthermore, attention block and skip-ordering scheme are introduced to achieve robust performance in various driving environment. Our network is compared with the conventional methods which use common domain such as color or optical flow. Experimental results confirm that the proposed network shows better performance than other approaches with higher and more stable results.", "authors": ["Joosung Lee", "Sangwon Hwang", "Kyungjae Lee", "Woo Jin Kim", "Junhyeop Lee", "Tae-young Chung", "Sangyoun Lee"], "published_date": "2020-01-07T15:01:57Z", "url": "http://arxiv.org/abs/2001.02090v1", "pdf_url": "http://arxiv.org/pdf/2001.02090v1", "primary_category": "cs.CV"}
{"doc_id": "2004.04090v1", "title": "Beyond Photometric Consistency: Gradient-based Dissimilarity for   Improving Visual Odometry and Stereo Matching", "abstract": "Pose estimation and map building are central ingredients of autonomous robots and typically rely on the registration of sensor data. In this paper, we investigate a new metric for registering images that builds upon on the idea of the photometric error. Our approach combines a gradient orientation-based metric with a magnitude-dependent scaling term. We integrate both into stereo estimation as well as visual odometry systems and show clear benefits for typical disparity and direct image registration tasks when using our proposed metric. Our experimental evaluation indicats that our metric leads to more robust and more accurate estimates of the scene depth as well as camera trajectory. Thus, the metric improves camera pose estimation and in turn the mapping capabilities of mobile robots. We believe that a series of existing visual odometry and visual SLAM systems can benefit from the findings reported in this paper.", "authors": ["Jan Quenzel", "Radu Alexandru Rosu", "Thomas L\u00e4be", "Cyrill Stachniss", "Sven Behnke"], "published_date": "2020-04-08T16:13:25Z", "url": "http://arxiv.org/abs/2004.04090v1", "pdf_url": "http://arxiv.org/pdf/2004.04090v1", "primary_category": "cs.CV"}
{"doc_id": "2405.06241v2", "title": "MGS-SLAM: Monocular Sparse Tracking and Gaussian Mapping with Depth   Smooth Regularization", "abstract": "This letter introduces a novel framework for dense Visual Simultaneous Localization and Mapping (VSLAM) based on Gaussian Splatting. Recently, SLAM based on Gaussian Splatting has shown promising results. However, in monocular scenarios, the Gaussian maps reconstructed lack geometric accuracy and exhibit weaker tracking capability. To address these limitations, we jointly optimize sparse visual odometry tracking and 3D Gaussian Splatting scene representation for the first time. We obtain depth maps on visual odometry keyframe windows using a fast Multi-View Stereo (MVS) network for the geometric supervision of Gaussian maps. Furthermore, we propose a depth smooth loss and Sparse-Dense Adjustment Ring (SDAR) to reduce the negative effect of estimated depth maps and preserve the consistency in scale between the visual odometry and Gaussian maps. We have evaluated our system across various synthetic and real-world datasets. The accuracy of our pose estimation surpasses existing methods and achieves state-of-the-art. Additionally, it outperforms previous monocular methods in terms of novel view synthesis and geometric reconstruction fidelities.", "authors": ["Pengcheng Zhu", "Yaoming Zhuang", "Baoquan Chen", "Li Li", "Chengdong Wu", "Zhanlin Liu"], "published_date": "2024-05-10T04:42:21Z", "url": "http://arxiv.org/abs/2405.06241v2", "pdf_url": "http://arxiv.org/pdf/2405.06241v2", "primary_category": "cs.CV"}
{"doc_id": "2504.11698v1", "title": "An Online Adaptation Method for Robust Depth Estimation and Visual   Odometry in the Open World", "abstract": "Recently, learning-based robotic navigation systems have gained extensive research attention and made significant progress. However, the diversity of open-world scenarios poses a major challenge for the generalization of such systems to practical scenarios. Specifically, learned systems for scene measurement and state estimation tend to degrade when the application scenarios deviate from the training data, resulting to unreliable depth and pose estimation. Toward addressing this problem, this work aims to develop a visual odometry system that can fast adapt to diverse novel environments in an online manner. To this end, we construct a self-supervised online adaptation framework for monocular visual odometry aided by an online-updated depth estimation module. Firstly, we design a monocular depth estimation network with lightweight refiner modules, which enables efficient online adaptation. Then, we construct an objective for self-supervised learning of the depth estimation module based on the output of the visual odometry system and the contextual semantic information of the scene. Specifically, a sparse depth densification module and a dynamic consistency enhancement module are proposed to leverage camera poses and contextual semantics to generate pseudo-depths and valid masks for the online adaptation. Finally, we demonstrate the robustness and generalization capability of the proposed method in comparison with state-of-the-art learning-based approaches on urban, in-house datasets and a robot platform. Code is publicly available at: https://github.com/jixingwu/SOL-SLAM.", "authors": ["Xingwu Ji", "Haochen Niu", "Dexin Duan", "Rendong Ying", "Fei Wen", "Peilin Liu"], "published_date": "2025-04-16T01:48:10Z", "url": "http://arxiv.org/abs/2504.11698v1", "pdf_url": "http://arxiv.org/pdf/2504.11698v1", "primary_category": "cs.RO"}
{"doc_id": "2209.13274v2", "title": "Orbeez-SLAM: A Real-time Monocular Visual SLAM with ORB Features and   NeRF-realized Mapping", "abstract": "A spatial AI that can perform complex tasks through visual signals and cooperate with humans is highly anticipated. To achieve this, we need a visual SLAM that easily adapts to new scenes without pre-training and generates dense maps for downstream tasks in real-time. None of the previous learning-based and non-learning-based visual SLAMs satisfy all needs due to the intrinsic limitations of their components. In this work, we develop a visual SLAM named Orbeez-SLAM, which successfully collaborates with implicit neural representation and visual odometry to achieve our goals. Moreover, Orbeez-SLAM can work with the monocular camera since it only needs RGB inputs, making it widely applicable to the real world. Results show that our SLAM is up to 800x faster than the strong baseline with superior rendering outcomes. Code link: https://github.com/MarvinChung/Orbeez-SLAM.", "authors": ["Chi-Ming Chung", "Yang-Che Tseng", "Ya-Ching Hsu", "Xiang-Qian Shi", "Yun-Hung Hua", "Jia-Fong Yeh", "Wen-Chin Chen", "Yi-Ting Chen", "Winston H. Hsu"], "published_date": "2022-09-27T09:37:57Z", "url": "http://arxiv.org/abs/2209.13274v2", "pdf_url": "http://arxiv.org/pdf/2209.13274v2", "primary_category": "cs.RO"}
